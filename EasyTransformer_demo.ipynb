{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aVariengien/Easy-Transformer/blob/main/EasyTransformer_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPzDKLDOIa-H"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGh1b5pU6ROA",
        "outputId": "5d1a4731-c51c-4b59-9dc6-0b1c8442ab35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 15 00:53:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 15.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 12.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 82.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip install einops\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIIxTGsK6hgx",
        "outputId": "1a12c973-f64f-4e31-84dd-269a51c249e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n"
          ]
        }
      ],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from functools import *\n",
        "import pandas as pd\n",
        "import gc\n",
        "import collections\n",
        "import copy\n",
        "\n",
        "# import comet_ml\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "dJPrRnol_9-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pV6fExYuZMm"
      },
      "outputs": [],
      "source": [
        "def get_corner(tensor, n=2):\n",
        "    # Prints the top left corner of the tensor\n",
        "    if len(tensor.shape)==0:\n",
        "        return tensor\n",
        "    elif len(tensor.shape)==1:\n",
        "        return tensor[:n]\n",
        "    elif len(tensor.shape)==2:\n",
        "        return tensor[:n, :n]\n",
        "    elif len(tensor.shape)==3:\n",
        "        return tensor[:n, :n, :n]\n",
        "    elif len(tensor.shape)==4:\n",
        "        return tensor[:n, :n, :n, :n]\n",
        "    elif len(tensor.shape)==5:\n",
        "        return tensor[:n, :n, :n, :n, :n]\n",
        "    elif len(tensor.shape)==6:\n",
        "        return tensor[:n, :n, :n, :n, :n, :n]\n",
        "    else:\n",
        "        # I never need tensors of rank > 6\n",
        "        raise ValueError(f'Tensor of shape {tensor.shape} is too big')\n",
        "\n",
        "def to_numpy(tensor, flat=False):\n",
        "    if (type(tensor)!=torch.Tensor) and (type(tensor)!=torch.nn.parameter.Parameter):\n",
        "        return tensor\n",
        "    if flat:\n",
        "        return tensor.flatten().detach().cpu().numpy()\n",
        "    else:\n",
        "        return tensor.detach().cpu().numpy()\n",
        "\n",
        "def gelu_new(input):\n",
        "    # Implementation of GeLU used by GPT2 - subtly different from PyTorch's\n",
        "    return 0.5 * input * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hook Points"
      ],
      "metadata": {
        "id": "Ruj-tYAiAMc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Garcon-style interface - the key thing is a HookPoint class. This is a layer to wrap any activation within the model in. The HookPoint acts as an identity function, but allows us to put PyTorch hooks in to edit and access the relevant activation. This allows us to take any model and insert in access points to all interesting activations by wrapping them in HookPoints\n",
        "\n",
        "There is also a `HookedRootModule` class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well. \n",
        "\n",
        "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass. \n",
        "\n",
        "The syntax for a hook is `function(activation, hook)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.\n",
        "\n"
      ],
      "metadata": {
        "id": "p6as1AqeAOIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsdZ2huPoOUp"
      },
      "outputs": [],
      "source": [
        "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
        "# It's a dummy module that is the identity function by default\n",
        "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
        "# way to add PyTorch hooks\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "        self.ctx = {}\n",
        "        \n",
        "        # A variable giving the hook's name (from the perspective of the root \n",
        "        # module) - this is set by the root module at setup.\n",
        "        self.name = None\n",
        "    \n",
        "    def add_hook(self, hook, dir='fwd'):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output, \n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, hook=self)\n",
        "        if dir=='fwd':\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir=='bwd':\n",
        "            handle = self.register_full_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "    \n",
        "    def remove_hooks(self, dir='fwd'):\n",
        "        if (dir=='fwd') or (dir=='both'):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir=='bwd') or (dir=='both'):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in ['fwd', 'bwd', 'both']:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "    \n",
        "    def clear_context(self):\n",
        "        del self.ctx\n",
        "        self.ctx = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def layer(self):\n",
        "        # Returns the layer index if the name has the form 'blocks.{layer}.{...}'\n",
        "        # Helper function that's mainly useful on EasyTransformer\n",
        "        # If it doesn't have this form, raises an error - \n",
        "        split_name = self.name.split('.')\n",
        "        return int(split_name[1])\n",
        "\n",
        "class HookedRootModule(nn.Module):\n",
        "    # A class building on nn.Module to interface nicely with HookPoints\n",
        "    # Allows you to name each hook, remove hooks, cache every activation/gradient, etc\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "    \n",
        "    def setup(self):\n",
        "        # Setup function - this needs to be run in __init__ AFTER defining all \n",
        "        # layers\n",
        "        # Add a parameter to each module giving its name\n",
        "        # Build a dictionary mapping a module name to the module\n",
        "        self.mod_dict = {}\n",
        "        self.hook_dict = {}\n",
        "        for name, module in self.named_modules():\n",
        "            module.name = name\n",
        "            self.mod_dict[name] = module\n",
        "            if type(module)==HookPoint:\n",
        "                self.hook_dict[name] = module\n",
        "        \n",
        "    def hook_points(self):\n",
        "        return (self.hook_dict.values())\n",
        "\n",
        "    def remove_all_hook_fns(self, direction='both'):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(direction)\n",
        "    \n",
        "    def clear_contexts(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.clear_context()\n",
        "    \n",
        "    def reset_hooks(self, clear_contexts=True, direction='both'):\n",
        "        if clear_contexts: self.clear_contexts()\n",
        "        self.remove_all_hook_fns(direction)\n",
        "    \n",
        "    def cache_all(self, cache, incl_bwd=False, device='cuda'):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, hook):\n",
        "            cache[hook.name] = tensor.detach().to(device)\n",
        "        def save_hook_back(tensor, hook):\n",
        "            cache[hook.name+'_grad'] = tensor[0].detach().to(device)\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, 'fwd')\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, 'bwd')\n",
        "    \n",
        "    def run_with_hooks(self, \n",
        "                       *args, \n",
        "                       fwd_hooks=[], \n",
        "                       bwd_hooks=[], \n",
        "                       reset_hooks_start=True, \n",
        "                       reset_hooks_end=True, \n",
        "                       clear_contexts=False):\n",
        "        '''\n",
        "        fwd_hooks: A list of (name, hook), where name is either the name of \n",
        "        a hook point or a Boolean function on hook names and hook is the \n",
        "        function to add to that hook point, or the hook whose names evaluate \n",
        "        to True respectively. Ditto bwd_hooks\n",
        "        reset_hooks_start (bool): If True, all prior hooks are removed at the start\n",
        "        reset_hooks_end (bool): If True, all hooks are removed at the end (ie, \n",
        "        including those added in this run)\n",
        "        clear_contexts (bool): If True, clears hook contexts whenever hooks are reset\n",
        "        \n",
        "        Note that if we want to use backward hooks, we need to set \n",
        "        reset_hooks_end to be False, so the backward hooks are still there - this function only runs a forward pass.\n",
        "        '''\n",
        "        if reset_hooks_start:\n",
        "            self.reset_hooks(clear_contexts)\n",
        "        for name, hook in fwd_hooks:\n",
        "            if type(name)==str:\n",
        "                self.mod_dict[name].add_hook(hook, dir='fwd')\n",
        "            else:\n",
        "                # Otherwise, name is a Boolean function on names\n",
        "                for hook_name, hp in self.hook_dict.items():\n",
        "                    if name(hook_name):\n",
        "                        hp.add_hook(hook, dir='fwd')\n",
        "        for name, hook in bwd_hooks:\n",
        "            if type(name)==str:\n",
        "                self.mod_dict[name].add_hook(hook, dir='fwd')\n",
        "            else:\n",
        "                # Otherwise, name is a Boolean function on names\n",
        "                for hook_name, hp in self.hook_dict:\n",
        "                    if name(hook_name):\n",
        "                        hp.add_hook(hook, dir='bwd')\n",
        "        out = self.forward(*args)\n",
        "        if reset_hooks_end:\n",
        "            if len(bwd_hooks)>0:\n",
        "                print(\"WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set.\")\n",
        "                print(\"This removes the backward hooks before a backward pass can occur\")\n",
        "            self.reset_hooks(clear_contexts)\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example"
      ],
      "metadata": {
        "id": "T5jev5MWAhvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simple example of how to use the classes:\n",
        "\n",
        "We define a basic network with two layers that each take a scalar input $x$, square it, and add a constant:\n",
        "$x_0=x$, $x_1=x_0^2+3$, $x_2=x_1^2-4$.\n",
        "\n",
        "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point."
      ],
      "metadata": {
        "id": "cV4xf75lCZIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SquareThenAdd(nn.Module):\n",
        "    def __init__(self, offset):\n",
        "        super().__init__()\n",
        "        self.offset = nn.Parameter(torch.tensor(offset))\n",
        "        self.hook_square = HookPoint()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # The hook_square doesn't change the value, but lets us access it\n",
        "        square = self.hook_square(x * x)\n",
        "        return self.offset + square\n",
        "    \n",
        "class TwoLayerModel(HookedRootModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = SquareThenAdd(3.)\n",
        "        self.layer2 = SquareThenAdd(-4.)\n",
        "        self.hook_in = HookPoint()\n",
        "        self.hook_mid = HookPoint()\n",
        "        self.hook_out = HookPoint()\n",
        "\n",
        "        # We need to call the setup function of HookedRootModule to build an \n",
        "        # internal dictionary of modules and hooks, and to give each hook a name\n",
        "        super().setup()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # We wrap the input and each layer's output in a hook - they leave the \n",
        "        # value unchanged (unless there's a hook added to explicitly change it), \n",
        "        # but allow us to access it.\n",
        "        x_in = self.hook_in(x)\n",
        "        x_mid = self.hook_mid(self.layer1(x_in))\n",
        "        x_out = self.hook_out(self.layer2(x_mid))\n",
        "        return x_out\n",
        "model = TwoLayerModel()\n",
        "\n"
      ],
      "metadata": {
        "id": "dv5Eu4FrAjHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add a cache, to save the activation at each hook point\n",
        "\n",
        "(There's a custom `cache_all` function on the root module as a convenience, which will add hooks to cache every activation at a hook point - we could also manually add hooks with `run_with_hooks`)"
      ],
      "metadata": {
        "id": "nb60DIyvEdn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "model.cache_all(cache)\n",
        "print('Model output:', model(torch.tensor(5.)).item())\n",
        "for key in cache:\n",
        "    print(f\"Value cached at hook {key}\", cache[key].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqTlLwjzEdEC",
        "outputId": "df2628ba-bf90-453a-f25d-7f6b41f7e181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: 780.0\n",
            "Value cached at hook hook_in 5.0\n",
            "Value cached at hook layer1.hook_square 25.0\n",
            "Value cached at hook hook_mid 28.0\n",
            "Value cached at hook layer2.hook_square 784.0\n",
            "Value cached at hook hook_out 780.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5"
      ],
      "metadata": {
        "id": "_vE4yuDFDgaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_to_zero_hook(tensor, hook):\n",
        "    print(hook.name)\n",
        "    return torch.tensor(0.)\n",
        "print('Output after intervening on layer2.hook_scaled', \n",
        "      model.run_with_hooks(torch.tensor(5.),\n",
        "                           fwd_hooks = [('layer2.hook_square', set_to_zero_hook)]).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GPjSo_RDfi_",
        "outputId": "9aa57075-ac18-489a-8f36-439daa71fcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer2.hook_square\n",
            "Output after intervening on layer2.hook_scaled -4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65A88S5P6sWa"
      },
      "source": [
        "#Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a stripped down transformer. There are helper functions to load in the weights of several families of open source LLMs - OpenAI's GPT-2, Facebook's OPT and Eleuther's GPT-Neo.\n",
        "\n",
        "Note: OPT-350M is not supported - it applies the LayerNorms to the *outputs* of each layer, which means we cannot fold the weights and biases into other layers, and would require notably different architecture.\n",
        "\n",
        "**TODO:** Add in GPT-J and GPT-NeoX functionality"
      ],
      "metadata": {
        "id": "1LIWhwJ7IVCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYkw63iRi6gg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7cf79301-91d3-4992-8c15-7ba0f1853b3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbloom-350m\\nbloom-760m\\nbloom-1b3\\nbloom-2b5\\nbloom-6b3\\nbloom (176B parameters)\\nhttps://huggingface.co/docs/transformers/model_doc/bloom\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "VALID_MODEL_NAMES = ['gpt2', \n",
        "                     'gpt2-medium', \n",
        "                     'gpt2-large', \n",
        "                     'gpt2-xl', \n",
        "                     'facebook/opt-125m', \n",
        "                     'facebook/opt-1.3b', \n",
        "                     'facebook/opt-2.7b', \n",
        "                     'facebook/opt-6.7b', \n",
        "                     'facebook/opt-13b', \n",
        "                     'facebook/opt-30b', \n",
        "                     'facebook/opt-66b', \n",
        "                     'EleutherAI/gpt-neo-125M', \n",
        "                     'EleutherAI/gpt-neo-1.3B', \n",
        "                     'EleutherAI/gpt-neo-2.7B', \n",
        "                     'EleutherAI/gpt-j-6B', \n",
        "                     'EleutherAI/gpt-neox-20b']\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Add Bloom\n",
        "'''\n",
        "bloom-350m\n",
        "bloom-760m\n",
        "bloom-1b3\n",
        "bloom-2b5\n",
        "bloom-6b3\n",
        "bloom (176B parameters)\n",
        "https://huggingface.co/docs/transformers/model_doc/bloom\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the components of our simple transformer. Notable deviations from the standard framing:\n",
        "* Each attention head has weight matrices $W_Q,W_K,W_V$ of size [head_index, d_head, d_model] and $W_O$ of size [head_index, d_model, d_head] (ie, rather than concatenating the vectors from each head before the linear map)\n",
        "* The LayerNorms purely center and normalise, they do not have weights or biases. Instead these are folded into the weights and biases of the layers that come immediately after (attention, MLP and unembed)\n",
        "** This means that every query, key, value, MLP input, and the unembed calculations all have biases, even if they don't in the unfolded model\n"
      ],
      "metadata": {
        "id": "B17fPjE2JGeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network architecture\n",
        "\n",
        "# Embed & Unembed\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "        # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "        return einops.rearrange(self.W_E[:, tokens], 'd_model batch pos -> batch pos d_model')\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))\n",
        "        self.b_U = nn.Parameter(torch.empty(self.cfg['d_vocab']))\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        return torch.einsum('vm,bpm->bpv', self.W_U, tokens)+self.b_U # [batch, pos, d_vocab]\n",
        "\n",
        "# Positional Embeddings\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['n_ctx'])) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Output shape [pos, d_model] - will be broadcast along batch dim\n",
        "        return self.W_pos[:, :x.size(-1)].T # [pos, d_model]\n",
        "\n",
        "# LayerNormPre\n",
        "# I fold the LayerNorm weights and biases into later weights and biases. \n",
        "# This is just the 'center and normalise' part of LayerNorm\n",
        "# Centering is equivalent to just deleting one direction of residual space, \n",
        "# and is equivalent to centering the weight matrices of everything writing to the residual stream\n",
        "# Normalising is a funkier non-linear operation, that projects the residual stream onto the unit hypersphere\n",
        "class LayerNormPre(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.eps = self.cfg['eps']\n",
        "\n",
        "        # Adds a hook point for the normalisation scale factor\n",
        "        self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n",
        "        scale = self.hook_scale((einops.reduce(x.pow(2), \n",
        "                                               'batch pos embed -> batch pos 1', \n",
        "                                               'mean') + \n",
        "                                 self.eps).sqrt()) # [batch, pos, 1]\n",
        "        return x / scale\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, cfg, attn_type='global'):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "        self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "        self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "        self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n",
        "        self.b_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        self.b_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        self.b_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        self.b_O = nn.Parameter(torch.empty(self.cfg['d_model']))\n",
        "        \n",
        "        self.attn_type = attn_type\n",
        "        # Create a query_pos x key_pos mask, with True iff that query position \n",
        "        # can attend to that key position\n",
        "        causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())\n",
        "        if self.attn_type == 'global':\n",
        "            # For global attention, this is a lower triangular matrix - key <= query\n",
        "            self.register_buffer('mask', causal_mask)\n",
        "        elif self.attn_type == 'local':\n",
        "            # For local, this is banded, query - window_size < key <= query\n",
        "            self.register_buffer('mask', torch.triu(causal_mask, 1-self.cfg['window_size']))\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid attention type: {self.attn_type}\")\n",
        "        \n",
        "        self.register_buffer('IGNORE', torch.tensor(-1e5))\n",
        "        \n",
        "        if self.cfg['use_attn_scale']:\n",
        "            self.attn_scale = np.sqrt(self.cfg['d_head'])\n",
        "        else:\n",
        "            self.attn_scale = 1.\n",
        "\n",
        "        self.hook_k = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_q = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_v = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_z = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "        self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "        self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.hook_q(torch.einsum('ihm,bpm->bpih', self.W_Q, x) + self.b_Q) # [batch, pos, head_index, d_head]\n",
        "        k = self.hook_k(torch.einsum('ihm,bpm->bpih', self.W_K, x) + self.b_K) # [batch, pos, head_index, d_head]\n",
        "        v = self.hook_v(torch.einsum('ihm,bpm->bpih', self.W_V, x) + self.b_V) # [batch, pos, head_index, d_head]\n",
        "        attn_scores = torch.einsum('bpih,bqih->bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]\n",
        "        attn_scores = self.hook_attn_scores(self.causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]\n",
        "        attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]\n",
        "        z = self.hook_z(torch.einsum('bpih,biqp->bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]\n",
        "        if self.cfg['use_attn_result']:\n",
        "            result = self.hook_result(torch.einsum('imh,bqih->bqim', self.W_O, z)) # [batch, pos, head_index, d_model]\n",
        "            out = einops.reduce(result, \n",
        "                                'batch position index model->batch position model', \n",
        "                                'sum')+self.b_O  # [batch, pos, d_model]\n",
        "        else:\n",
        "            out = torch.einsum('idh,bqih->bqd', self.W_O, z)+self.b_O # [batch, pos, d_model]\n",
        "        return out\n",
        "    \n",
        "    def causal_mask(self, attn_scores):\n",
        "        return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)\n",
        "\n",
        "# MLP Layers\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))\n",
        "        self.b_in = nn.Parameter(torch.empty(self.cfg['d_mlp']))\n",
        "        self.W_out = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))\n",
        "        self.b_out = nn.Parameter(torch.empty(self.cfg['d_model']))\n",
        "\n",
        "        self.hook_pre = HookPoint() # [batch, pos, d_mlp]\n",
        "        self.hook_post = HookPoint() # [batch, pos, d_mlp]\n",
        "\n",
        "        if self.cfg['act_fn']=='relu':\n",
        "            self.act_fn = F.relu\n",
        "        elif self.cfg['act_fn']=='gelu_new':\n",
        "            self.act_fn = gelu_new\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid activation function name: {self.cfg['act_fn']}\")\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in) # [batch, pos, d_mlp]\n",
        "        x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]\n",
        "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out # [batch, pos, d_model]\n",
        "        return x\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg, block_index):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = LayerNormPre(cfg)\n",
        "        if not self.cfg['use_local_attn']:\n",
        "            self.attn = Attention(cfg, 'global')\n",
        "        else:\n",
        "            attn_type = self.cfg['attn_types'][block_index]\n",
        "            self.attn = Attention(cfg, attn_type)\n",
        "        self.ln2 = LayerNormPre(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "        self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_mlp_out = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_mid = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n",
        "        attn_out = self.hook_attn_out(self.attn(self.ln1(resid_pre))) # [batch, pos, d_model]\n",
        "        resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]\n",
        "        mlp_out = self.hook_mlp_out(self.mlp(self.ln2(resid_mid))) # [batch, pos, d_model]\n",
        "        resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]\n",
        "        return resid_post\n"
      ],
      "metadata": {
        "id": "aZRDetgm5EU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Full transformer\n",
        "class EasyTransformer(HookedRootModule):\n",
        "    \"\"\"\n",
        "    This class implements a full Transformer using the above components, with \n",
        "    HookPoints on every interesting activation. It inherits from HookedRootModule. \n",
        "    It is initialised with a model_name, and automatically loads the model weights \n",
        "    for that model, loads them into this model, folds in LayerNorm and centers \n",
        "    the weights\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 model_name, \n",
        "                 use_attn_result=False, \n",
        "                 model=None, \n",
        "                 keep_original_model=False, \n",
        "                 center_weights=True):\n",
        "        \"\"\"\n",
        "        model_name (str): The name of the model to load, via HuggingFace\n",
        "        use_attn_result (bool): Says whether to explicitly calculate the amount \n",
        "            each head adds to the residual stream (with a hook) and THEN add it \n",
        "            up, vs just calculating the sum. This can be very memory intensive \n",
        "            for large models, so defaults to False\n",
        "        model: The loaded model from HuggingFace. If None, it is automatically \n",
        "            loaded from HuggingFace - this just saves memory if the model was \n",
        "            already loaded into RAM\n",
        "        keep_original_model (bool): If False, the original HuggingFace model is \n",
        "            deleted, otherwise it's kept as a self.model attribute\n",
        "        \"\"\"\n",
        "        assert model_name in VALID_MODEL_NAMES\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.model_type = self.get_model_type(model_name)\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "        else:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        \n",
        "        self.cfg = self.convert_config(self.model.config, model_type=self.model_type)\n",
        "        self.cfg['use_attn_result']=use_attn_result\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        self.embed = Embed(self.cfg)\n",
        "        self.hook_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "        self.pos_embed = PosEmbed(self.cfg)\n",
        "        self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "        self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n",
        "        self.ln_final = LayerNormPre(self.cfg)\n",
        "        self.unembed = Unembed(self.cfg)\n",
        "\n",
        "        # Gives each module a parameter with its name (relative to this root module)\n",
        "        # Needed for HookPoints to work\n",
        "        self.setup()\n",
        "\n",
        "        # Load model weights, and fold in layer norm weights\n",
        "        if self.model_type=='gpt2':\n",
        "            self.load_gpt2_weights(self.model)\n",
        "        elif self.model_type=='neo':\n",
        "            self.load_neo_weights(self.model)\n",
        "        elif self.model_type=='gptj':\n",
        "            self.load_gptj_weights(self.model)\n",
        "        elif self.model_type=='neox':\n",
        "            self.load_neox_weights(self.model)\n",
        "        elif self.model_type=='opt':\n",
        "            self.load_opt_weights(self.model)\n",
        "        \n",
        "        # Set the average of each weight matrix writing to the residual stream to zero\n",
        "        # (Layer Norm removes the mean anyway, so this simplifies the weights \n",
        "        # without changing the computation)\n",
        "        if center_weights:\n",
        "            self.center_weights()\n",
        "        \n",
        "        if not keep_original_model:\n",
        "            # Delete the original model to save memory\n",
        "            del self.model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Input x is either a batch of tokens ([batch, pos]) or a text string\n",
        "        if type(x)==str:\n",
        "            # If text, convert to tokens (batch_size=1)\n",
        "            x = self.to_tokens(x)\n",
        "        embed = self.hook_embed(self.embed(x)) # [batch, pos, d_model]\n",
        "        pos_embed = self.hook_pos_embed(self.pos_embed(x)) # [batch, pos, d_model]\n",
        "        residual = embed + pos_embed # [batch, pos, d_model]\n",
        "        for block in self.blocks:\n",
        "            # Note that each block includes skip connections, so we don't need\n",
        "            # residual + block(residual)\n",
        "            residual = block(residual) # [batch, pos, d_model]\n",
        "        x = self.unembed(self.ln_final(residual)) # [batch, pos, d_vocab]\n",
        "        return x\n",
        "    \n",
        "    def to_tokens(self, text):\n",
        "        return self.tokenizer(text, return_tensors='pt')['input_ids']\n",
        "    \n",
        "    def get_model_type(self, model_name):\n",
        "        if 'gpt2' in model_name:\n",
        "            return 'gpt2'\n",
        "        elif 'opt' in model_name:\n",
        "            return 'opt'\n",
        "        elif model_name=='EleutherAI/gpt-neox-20b':\n",
        "            return 'neox'\n",
        "        elif model_name=='EleutherAI/gpt-j-6B':\n",
        "            return 'gptj'\n",
        "        elif 'neo' in model_name:\n",
        "            return 'neo'\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "    \n",
        "    def convert_config(self, config, model_type):\n",
        "        if model_type=='neo':\n",
        "            cfg = {\n",
        "                'd_model':config.hidden_size,\n",
        "                'd_head':config.hidden_size//config.num_heads,\n",
        "                'n_heads':config.num_heads,\n",
        "                'd_mlp':config.hidden_size*4,\n",
        "                'n_layers':config.num_layers,\n",
        "                'n_ctx':config.max_position_embeddings,\n",
        "                'eps':config.layer_norm_epsilon,\n",
        "                'd_vocab':config.vocab_size,\n",
        "                'attn_types':config.attention_layers,\n",
        "                'act_fn':config.activation_function,\n",
        "                'use_attn_scale':False,\n",
        "                'use_local_attn':True,\n",
        "                'window_size':config.window_size,\n",
        "            }\n",
        "        elif model_type=='gpt2':\n",
        "            cfg = {\n",
        "                'd_model':config.n_embd,\n",
        "                'd_head':config.n_embd//config.n_head,\n",
        "                'n_heads':config.n_head,\n",
        "                'd_mlp':config.n_embd*4,\n",
        "                'n_layers':config.n_layer,\n",
        "                'n_ctx':config.n_ctx,\n",
        "                'eps':config.layer_norm_epsilon,\n",
        "                'd_vocab':config.vocab_size,\n",
        "                'act_fn':config.activation_function,\n",
        "                'use_attn_scale':True,\n",
        "                'use_local_attn':False,\n",
        "            }\n",
        "        elif model_type=='opt':\n",
        "            cfg = {\n",
        "                'd_model':config.hidden_size,\n",
        "                'd_head':config.hidden_size//config.num_attention_heads,\n",
        "                'n_heads':config.num_attention_heads,\n",
        "                'd_mlp':config.ffn_dim,\n",
        "                'n_layers':config.num_hidden_layers,\n",
        "                'n_ctx':config.max_position_embeddings,\n",
        "                'eps':1e-5,\n",
        "                'd_vocab':config.vocab_size,\n",
        "                'act_fn':config.activation_function,\n",
        "                'use_attn_scale':True,\n",
        "                'use_local_attn':False,\n",
        "            }\n",
        "        elif model_type=='gptj':\n",
        "            raise NotImplementedError\n",
        "        elif model_type=='neox':\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        cfg['model_name']=self.model_name\n",
        "        cfg['model_type']=model_type\n",
        "        return cfg\n",
        "    \n",
        "    def center_weights(self):\n",
        "        # Sets the average of each row of each weight matrix writing to the \n",
        "        # residual stream to zero\n",
        "        # LayerNorm subtracts the mean of the residual stream, and it's always \n",
        "        # applied when reading from the residual stream, so this dimension is \n",
        "        # purely noise\n",
        "        # Also does the same for W_U, since translating the logits doesn't affect\n",
        "        # the log_probs or loss\n",
        "        self.embed.W_E.data -= self.embed.W_E.mean(0, keepdim=True)\n",
        "        self.pos_embed.W_pos.data -= self.pos_embed.W_pos.mean(0, keepdim=True)\n",
        "        self.unembed.W_U.data -= self.unembed.W_U.mean(0, keepdim=True)\n",
        "        for block in self.blocks:\n",
        "            block.attn.W_O.data -= einops.reduce(block.attn.W_O, \n",
        "                                                            'index d_model d_head -> index 1 d_head',\n",
        "                                                            'mean')\n",
        "            block.mlp.W_out.data -= block.mlp.W_out.mean(0, keepdim=True)\n",
        "        \n",
        "    def load_gpt2_weights(self, gpt2):\n",
        "        sd = self.state_dict()\n",
        "\n",
        "        sd['embed.W_E'] = gpt2.transformer.wte.weight.T\n",
        "        sd['pos_embed.W_pos'] = gpt2.transformer.wpe.weight.T\n",
        "\n",
        "        for l in range(self.cfg['n_layers']):\n",
        "            # In GPT-2, q,k,v are produced by one big linear map, whose output is \n",
        "            # concat([q, k, v])\n",
        "            W = gpt2.transformer.h[l].attn.c_attn.weight\n",
        "            w_ln_attn = gpt2.transformer.h[l].ln_1.weight\n",
        "            W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
        "            W_Q = einops.rearrange(W_Q, 'm (i h)->i h m', i=self.cfg['n_heads'])\n",
        "            W_K = einops.rearrange(W_K, 'm (i h)->i h m', i=self.cfg['n_heads'])\n",
        "            W_V = einops.rearrange(W_V, 'm (i h)->i h m', i=self.cfg['n_heads'])\n",
        "            \n",
        "            # Fold in layer norm weights\n",
        "            sd[f\"blocks.{l}.attn.W_Q\"] = W_Q * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_K\"] = W_K * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_V\"] = W_V * w_ln_attn\n",
        "\n",
        "            b_ln = gpt2.transformer.h[l].ln_1.bias\n",
        "            qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias\n",
        "            qkv_bias = einops.rearrange(qkv_bias, \n",
        "                                        '(qkv index head)->qkv index head', \n",
        "                                        qkv=3, \n",
        "                                        index=self.cfg['n_heads'], \n",
        "                                        head=self.cfg['d_head'])\n",
        "            # Fold in layer norm biases\n",
        "            sd[f'blocks.{l}.attn.b_Q'] = W_Q @ b_ln + qkv_bias[0]\n",
        "            sd[f'blocks.{l}.attn.b_K'] = W_K @ b_ln + qkv_bias[1]\n",
        "            sd[f'blocks.{l}.attn.b_V'] = W_V @ b_ln + qkv_bias[2]\n",
        "\n",
        "\n",
        "            W_O = gpt2.transformer.h[l].attn.c_proj.weight\n",
        "            W_O = einops.rearrange(W_O, '(i h) m->i m h', i=self.cfg['n_heads'])\n",
        "            sd[f\"blocks.{l}.attn.W_O\"] = W_O\n",
        "            sd[f'blocks.{l}.attn.b_O'] = gpt2.transformer.h[l].attn.c_proj.bias\n",
        "\n",
        "            W_in = gpt2.transformer.h[l].mlp.c_fc.weight.T\n",
        "            W_out = gpt2.transformer.h[l].mlp.c_proj.weight.T\n",
        "            # Fold in layer norm weights\n",
        "            W_in_adj = gpt2.transformer.h[l].ln_2.weight[None, :] * W_in\n",
        "            sd[f\"blocks.{l}.mlp.W_in\"] = W_in_adj\n",
        "            # Fold in layer norm biases\n",
        "            sd[f\"blocks.{l}.mlp.b_in\"] = gpt2.transformer.h[l].mlp.c_fc.bias+(W_in @ gpt2.transformer.h[l].ln_2.bias)\n",
        "            sd[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
        "            sd[f\"blocks.{l}.mlp.b_out\"] = gpt2.transformer.h[l].mlp.c_proj.bias\n",
        "        W_U = gpt2.lm_head.weight\n",
        "        # Fold in layer norm weights\n",
        "        sd['unembed.W_U'] = gpt2.transformer.ln_f.weight[None, :] * W_U\n",
        "        # Fold in layer norm biases\n",
        "        sd['unembed.b_U'] = gpt2.lm_head.weight @ gpt2.transformer.ln_f.bias\n",
        "        self.load_state_dict(sd)\n",
        "    \n",
        "    def load_neo_weights(self, neo):\n",
        "        sd = self.state_dict()\n",
        "\n",
        "        sd['embed.W_E'] = neo.transformer.wte.weight.T\n",
        "        sd['pos_embed.W_pos'] = neo.transformer.wpe.weight.T\n",
        "\n",
        "        for l in range(self.cfg['n_layers']):\n",
        "            w_ln_attn = neo.transformer.h[l].ln_1.weight\n",
        "            W_Q = neo.transformer.h[l].attn.attention.q_proj.weight\n",
        "            W_K = neo.transformer.h[l].attn.attention.k_proj.weight\n",
        "            W_V = neo.transformer.h[l].attn.attention.v_proj.weight\n",
        "            W_Q = einops.rearrange(W_Q, '(i h) m->i h m', i=self.cfg['n_heads'])\n",
        "            W_K = einops.rearrange(W_K, '(i h) m->i h m', i=self.cfg['n_heads'])\n",
        "            W_V = einops.rearrange(W_V, '(i h) m->i h m', i=self.cfg['n_heads'])\n",
        "            \n",
        "            sd[f\"blocks.{l}.attn.W_Q\"] = W_Q * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_K\"] = W_K * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_V\"] = W_V * w_ln_attn\n",
        "\n",
        "            b_ln = neo.transformer.h[l].ln_1.bias\n",
        "            sd[f'blocks.{l}.attn.b_Q'] = W_Q @ b_ln\n",
        "            sd[f'blocks.{l}.attn.b_K'] = W_K @ b_ln\n",
        "            sd[f'blocks.{l}.attn.b_V'] = W_V @ b_ln\n",
        "\n",
        "\n",
        "            W_O = neo.transformer.h[l].attn.attention.out_proj.weight\n",
        "            W_O = einops.rearrange(W_O, 'm (i h)->i m h', i=self.cfg['n_heads'])\n",
        "            sd[f\"blocks.{l}.attn.W_O\"] = W_O\n",
        "            sd[f'blocks.{l}.attn.b_O'] = neo.transformer.h[l].attn.attention.out_proj.bias\n",
        "\n",
        "            W_in = neo.transformer.h[l].mlp.c_fc.weight\n",
        "            W_out = neo.transformer.h[l].mlp.c_proj.weight\n",
        "            W_in_adj = neo.transformer.h[l].ln_2.weight[None, :] * W_in\n",
        "            sd[f\"blocks.{l}.mlp.W_in\"] = W_in_adj\n",
        "            sd[f\"blocks.{l}.mlp.b_in\"] = neo.transformer.h[l].mlp.c_fc.bias+(W_in @ neo.transformer.h[l].ln_2.bias)\n",
        "            sd[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
        "            sd[f\"blocks.{l}.mlp.b_out\"] = neo.transformer.h[l].mlp.c_proj.bias\n",
        "        W_U = neo.lm_head.weight\n",
        "        sd['unembed.W_U'] = neo.transformer.ln_f.weight[None, :] * W_U\n",
        "        sd['unembed.b_U'] = neo.lm_head.weight @ neo.transformer.ln_f.bias\n",
        "        self.load_state_dict(sd)\n",
        "    \n",
        "    def load_neox_weights(self, neox):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def load_gptj_weights(self, gptj):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def load_opt_weights(self, opt):\n",
        "        sd = self.state_dict()\n",
        "\n",
        "        sd['embed.W_E'] = opt.model.decoder.embed_tokens.weight.T\n",
        "        sd['pos_embed.W_pos'] = opt.model.decoder.embed_positions.weight.T[:, 2:]\n",
        "\n",
        "        for l in range(self.cfg['n_layers']):\n",
        "            w_ln_attn = opt.model.decoder.layers[l].self_attn_layer_norm.weight\n",
        "            W_Q = opt.model.decoder.layers[l].self_attn.q_proj.weight\n",
        "            W_K = opt.model.decoder.layers[l].self_attn.k_proj.weight\n",
        "            W_V = opt.model.decoder.layers[l].self_attn.v_proj.weight\n",
        "            W_Q = einops.rearrange(W_Q, '(index d_head) d_model->index d_head d_model', i=self.cfg['n_heads'])\n",
        "            W_K = einops.rearrange(W_K, '(index d_head) d_model->index d_head d_model', i=self.cfg['n_heads'])\n",
        "            W_V = einops.rearrange(W_V, '(index d_head) d_model->index d_head d_model', i=self.cfg['n_heads'])\n",
        "            \n",
        "            sd[f\"blocks.{l}.attn.W_Q\"] = W_Q * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_K\"] = W_K * w_ln_attn\n",
        "            sd[f\"blocks.{l}.attn.W_V\"] = W_V * w_ln_attn\n",
        "\n",
        "            b_ln = opt.model.decoder.layers[l].self_attn_layer_norm.bias\n",
        "            q_bias = einops.rearrange(opt.model.decoder.layers[l].self_attn.q_proj.bias, '(head_index d_head)->head_index d_head', head_index=self.cfg['n_heads'], d_head=self.cfg['d_head'])\n",
        "            k_bias = einops.rearrange(opt.model.decoder.layers[l].self_attn.k_proj.bias, '(head_index d_head)->head_index d_head', head_index=self.cfg['n_heads'], d_head=self.cfg['d_head'])\n",
        "            v_bias = einops.rearrange(opt.model.decoder.layers[l].self_attn.v_proj.bias, '(head_index d_head)->head_index d_head', head_index=self.cfg['n_heads'], d_head=self.cfg['d_head'])\n",
        "\n",
        "            sd[f'blocks.{l}.attn.b_Q'] = W_Q @ b_ln + q_bias \n",
        "            sd[f'blocks.{l}.attn.b_K'] = W_K @ b_ln + k_bias\n",
        "            sd[f'blocks.{l}.attn.b_V'] = W_V @ b_ln + v_bias\n",
        "\n",
        "            W_O = opt.model.decoder.layers[l].self_attn.out_proj.weight\n",
        "            W_O = einops.rearrange(W_O, 'd_model (index d_head)->index d_model d_head', i=self.cfg['n_heads'])\n",
        "            sd[f\"blocks.{l}.attn.W_O\"] = W_O\n",
        "            sd[f'blocks.{l}.attn.b_O'] = opt.model.decoder.layers[l].self_attn.out_proj.bias\n",
        "\n",
        "            W_in = opt.model.decoder.layers[l].fc1.weight\n",
        "            W_out = opt.model.decoder.layers[l].fc2.weight\n",
        "            W_in_adj = opt.model.decoder.layers[l].final_layer_norm.weight[None, :] * W_in\n",
        "            sd[f\"blocks.{l}.mlp.W_in\"] = W_in_adj\n",
        "            sd[f\"blocks.{l}.mlp.b_in\"] = opt.model.decoder.layers[l].fc1.bias+(W_in @ opt.model.decoder.layers[l].final_layer_norm.bias)\n",
        "            sd[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
        "            sd[f\"blocks.{l}.mlp.b_out\"] = opt.model.decoder.layers[l].fc2.bias\n",
        "        W_U = opt.lm_head.weight\n",
        "        sd['unembed.W_U'] = opt.model.decoder.final_layer_norm.weight[None, :] * W_U\n",
        "        sd['unembed.b_U'] = W_U @ opt.model.decoder.final_layer_norm.bias\n",
        "        self.load_state_dict(sd)\n",
        "\n",
        "    def load_bloom_weights(self, bloom):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "HxWlPvJ1CiLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discussion"
      ],
      "metadata": {
        "id": "s3MWaY_fuN-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Simplifications\n"
      ],
      "metadata": {
        "id": "QT8mwRcnLqop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Centering $W_U$"
      ],
      "metadata": {
        "id": "lDra6BP9wa-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of $W_U$ is a $d_{vocab}$ vector (or tensor with that as the final dimension) which is fed into a softmax"
      ],
      "metadata": {
        "id": "3rb99WLnwdm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LayerNorm Folding"
      ],
      "metadata": {
        "id": "gOFcfk-6wZdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "LayerNorm is only applied at the start of a linear layer reading from the residual stream (eg query, key, value, mlp_in or unembed calculations)\n",
        "\n",
        "Each LayerNorm has the functional form $LN:\\mathbb{R}^n\\to\\mathbb{R}^n$, \n",
        "$LN(x)=s(x) * w_{ln} + b_{ln}$, where $*$ is element-wise multiply and $s(x)=\\frac{x-\\bar{x}}{|x-\\bar{x}|}$, and $w_{ln},b_{ln}$ are both vectors in $\\mathbb{R}^n$\n",
        "\n",
        "The linear layer has form $l:\\mathbb{R}^n\\to\\mathbb{R}^m$, $l(y)=Wy+b$ where $W\\in \\mathbb{R}^{m\\times n},b\\in \\mathbb{R}^m,y\\in\\mathbb{R}^n$\n",
        "\n",
        "So $f(LN(x))=W(w_{ln} * s(x)+b_{ln})+b=(W * w_{ln})s(x)+(Wb_{ln}+b)=W_{eff}s(x)+b_{eff}$, where $W_{eff}$ is the elementwise product of $W$ and $w_{ln}$ (showing that elementwise multiplication commutes like this is left as an exercise) and $b_{eff}=Wb_{ln}+b\\in \\mathbb{R}^m$.\n",
        "\n",
        "From the perspective of interpretability, it's much nicer to interpret the folded layer $W_{eff},b_{eff}$ - fundamentally, this is the computation being done, and there's no reason to expect $W$ or $w_{ln}$ to be meaningful on their own. \n"
      ],
      "metadata": {
        "id": "tUVCa0s7Ls1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tips for Running Large Models"
      ],
      "metadata": {
        "id": "Re8efoNTMDjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current library does not support models on multiple GPUs, which makes running the models too large for one GPU much harder - if you're interested in adding this functionality, I'd love to chat!\n",
        "\n",
        "Tips for getting the most out of your GPU:\n",
        "* Use the `torch.no_grad` context manager or `torch.set_grad_enabled(False)` global to turn off AutoGrad - AutoGrad automatically caches all activations to use when computing gradients, which can consume a lot of GPU memory\n",
        "* Cast your weights to bfloat16 or FP16 before moving to the GPU - this halves the memory footprint\n",
        "** I expect there are a bunch of ways to use mixed precision to achieve this without sacrificing accuracy - though for the purposes of interpretability I expect it to not matter a ton.\n",
        "* When cacheing activations, copy them to the CPU - this is *much* slower, but the CPU should have a lot more memory\n",
        "* General rule of thumb - your GPU is more memory limited than runtime limited, and can fill up very quickly - it's often worth deleting activations that can be easily recomputed.\n",
        "* It's fairly easy to get 16GB RAM GPUs (eg Colab Pro+). If you want to get more GPU RAM than this, you can rent A100 40GB RAM GPUs in a bunch of places (AWS, GCP, Paperspace, etc). After a cursory search, the only place I've found A100 80GB RAM GPUs for rent is [runpod.io](https://runpod.io)"
      ],
      "metadata": {
        "id": "sZ7vNLhLMKwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Details"
      ],
      "metadata": {
        "id": "6cKDQnt0MFv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* GPT-2 and GPT-Neo scale attention scores ($q\\cdot k$) down by $\\sqrt{d_{head}}$ before softmax, OPT does not\n",
        "* OPT has ReLU activations, GPT-2 and GPT-Neo have GeLU\n",
        "* In GPT-Neo, every other attention layer uses local attention (heads can only attend back a fixed number of tokens, compared to dense attention where heads can attend to any prior token)\n",
        "* All models use learned positional embeddings"
      ],
      "metadata": {
        "id": "4q8aenIFMH-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Examples"
      ],
      "metadata": {
        "id": "zs8juArnyuyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "OqxKiHT9AzbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in GPT-2 small"
      ],
      "metadata": {
        "id": "ed8FxxC4RHIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2' #@param ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b']\n",
        "model = EasyTransformer(model_name)\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')"
      ],
      "metadata": {
        "id": "s0wnSRHCRInS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317,
          "referenced_widgets": [
            "fed2f8e0575c4c09ab04877505254b1f",
            "0135aabf40ce4418a3d013d7c503aabd",
            "12f1a150740e44dd83e7440226b34964",
            "ccf084b77f0e433ea5b78b475e5d775a",
            "bee8afe32e0e49f38bdee1f9f77d3bb0",
            "a9796fdf1df44293b1446d93a0c1c6ac",
            "2d7c5e8946fa46f5af9c0b6f062ab83e",
            "d856c45fd4144315bec934c1013a4b62",
            "4db333cefc3f481c97c6efa5e926ac75",
            "50156149fb154147850ef8ce1823911b",
            "0f1356120b554199867ec549b3a1cbba",
            "bd42c9d0c33447e2b6f06321abe101ca",
            "1ae22736f57144ebaac372067c99b68b",
            "e13e0cccded148698031d02df2355a4a",
            "436be5196dbc43a1aad7bdd74f56f868",
            "e0c3a6182fdd4344b3a5d74e66cb2b1a",
            "4c4534bf661f44f3b1cc0c5a57c07f7c",
            "9735ead6e4bd4a59979df9ec2079444d",
            "4a86515d117a4e29a7ce1a2775ae1c92",
            "ad2477d6d5ef499b88f7bb6e3cad6df2",
            "d01c7aead21d4475bd51db7ffebb025b",
            "31b24996ae8349cc964419d175595128",
            "a9c44ffa8c8d44eab7c5325716314f40",
            "462e514fe94e47d79ad3a8671c6b3979",
            "6459483de48141ba9c91bb4f2e8b86e0",
            "a0a383f04b7643bc9604588dfcddbdfc",
            "47b5617aa14a476386b105b1a620df7d",
            "1d6d0c3591144ffb8f32aae63f18f145",
            "b52471219e714dbf86ff5a864965efe3",
            "b04fadd245604e118896019a3b9d41e5",
            "66632050940047699760f576ed41c177",
            "e42b334bf51441ed8d3f46bae0e71f52",
            "d5f084efd2c746d78fcb92f78c12f6d9",
            "510a3175149644d881520df35983c828",
            "9e3437f4854c49be8d3a8a42bdcce037",
            "67d57ad655154583a1e79e5d353a59aa",
            "cddf673e137e42379b602088a1b1c6ba",
            "e33a4e424c854690b68b855cc73c4a16",
            "47a9fa3705934a54a59ced5334e99760",
            "1d5baab536844c4eb60fde2e2f69516b",
            "fdac6ecdc5a54806b2c481328433b9da",
            "e179471eeb5f47969fe8094b2d27c823",
            "aa3c3109d62649cc91b6aa8612c35869",
            "88d11b5083f44d4890e34d994592b326",
            "9eb7e52c88a54bae9d3f749c4c573441",
            "ec6d6ba1dd824b6fa626dc2bc661dda4",
            "0552091708664c3b8667830aea8ecd7c",
            "06c3bebad72749128bef975574f96682",
            "ae014cb0d544461babaf5499fe707953",
            "9fd59fafbff24a5590f674c27d04b977",
            "81e8c482443e4aad93b3b2975d6717a3",
            "cf0d21a46359412f8f96808fef3bb297",
            "213f5d2c5aac47c38dc971677719cf8f",
            "1f5e16ceccc446bab584e3f3cc48ebd5",
            "58a97cfdf61448e4a46fd21e7f1bfc1c"
          ]
        },
        "outputId": "c477a6da-e380-4372-9e2c-10ee591e82f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fed2f8e0575c4c09ab04877505254b1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd42c9d0c33447e2b6f06321abe101ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9c44ffa8c8d44eab7c5325716314f40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "510a3175149644d881520df35983c828"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eb7e52c88a54bae9d3f749c4c573441"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create some reference text to run the models on."
      ],
      "metadata": {
        "id": "AeE6JkQUGZbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Interpretability is great'\n",
        "tokens = model.to_tokens(prompt)\n",
        "prompt_2 = 'AI Alignment is great'\n",
        "tokens_2 = model.to_tokens(prompt_2)\n",
        "def show_tokens(tokens):\n",
        "    # Prints the tokens as text, separated by |\n",
        "    if type(tokens)==str:\n",
        "        # If we input text, tokenize first\n",
        "        tokens = model.to_tokens(tokens)\n",
        "    text_tokens = [model.tokenizer.decode(t) for t in tokens.squeeze()]\n",
        "    print('|'.join(text_tokens))\n",
        "show_tokens(tokens)\n",
        "show_tokens(tokens_2)"
      ],
      "metadata": {
        "id": "Z7WubLxO1LC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44750eb-3822-4c4d-e706-b2096243f454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inter|pret|ability| is| great\n",
            "AI| Al|ignment| is| great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks()\n",
        "original_logits = model(tokens)\n",
        "print('Top corner of logits')\n",
        "print(get_corner(original_logits, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYnFUEVt6ogk",
        "outputId": "d38327ba-1455-4321-9e8f-20a6fa7ffea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top corner of logits\n",
            "tensor([[[-5.0197, -4.0007, -6.4540, -6.6005],\n",
            "         [-4.1477, -2.2966, -7.4325, -6.9754],\n",
            "         [-2.7587,  1.3903, -4.3042, -6.5975],\n",
            "         [-6.1653, -5.0678, -9.1324, -9.5672]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reference: Hyperparameters for the model')\n",
        "for hyper_param in (model.cfg):\n",
        "    print(hyper_param, model.cfg[hyper_param])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h1rp7QzF4Lz",
        "outputId": "b699fd58-be38-4476-d02f-f29912b9dd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: Hyperparameters for the model\n",
            "d_model 768\n",
            "d_head 64\n",
            "n_heads 12\n",
            "d_mlp 3072\n",
            "n_layers 12\n",
            "n_ctx 1024\n",
            "eps 1e-05\n",
            "d_vocab 50257\n",
            "act_fn gelu_new\n",
            "use_attn_scale True\n",
            "use_local_attn False\n",
            "model_name gpt2\n",
            "model_type gpt2\n",
            "use_attn_result False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the model"
      ],
      "metadata": {
        "id": "uG7SDqy2A4F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can be given either text or tokens as an input (text is automatically converted to a `batch_size=1` batch of tokens)"
      ],
      "metadata": {
        "id": "lwi3obVVAk7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_tokens = model(tokens)\n",
        "logits_text = model(prompt)"
      ],
      "metadata": {
        "id": "gtCqC82RAg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model gives the same log_probs as the original Hugging Face model \n",
        "\n",
        "Though *not* the same logits, as we remove a constant offset from $W_U$"
      ],
      "metadata": {
        "id": "VatxagVlA6ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "easy_logits = model(tokens).cpu()\n",
        "original_model_logits = original_model(tokens).logits\n",
        "\n",
        "easy_log_probs = F.log_softmax(easy_logits, dim=-1)\n",
        "original_model_log_probs = F.log_softmax(original_model_logits, dim=-1)\n",
        "\n",
        "print('Fraction of log probs the same between easy model and original model:')\n",
        "print(torch.isclose(original_model_log_probs, easy_log_probs).sum()/easy_log_probs.numel())\n",
        "print('Fraction of logits the same between easy model and original model:')\n",
        "print(torch.isclose(original_model_logits, easy_logits).sum()/easy_logits.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhDSJmtHBDqD",
        "outputId": "6ddc6701-bd02-46bd-e504-03673a353746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraction of log probs the same between easy model and original model:\n",
            "tensor(1.0000)\n",
            "Fraction of logits the same between easy model and original model:\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Examples"
      ],
      "metadata": {
        "id": "1hOfkc5EA1GL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the shapes of all activations\n",
        "\n",
        "**Note:** This cell is a good reference for creating hooks - it's extremely useful to know the shapes of different activations as accessible by each hook!\n",
        "\n",
        "By convention, each activation is batch x position x ... (where the final dimension(s) is d_model, (head_index x d_head) or d_mlp). The one exception is hook_attn (attention patterns) which has shape batch x head_index x query_pos x key_pos\n",
        "\n",
        "**Reference:**\n",
        "`batch_size=4\n",
        "n_ctx=50\n",
        "d_head=64\n",
        "d_model=768\n",
        "d_mlp=3072\n",
        "n_heads=12\n",
        "n_layers=12`"
      ],
      "metadata": {
        "id": "F9LFSt_uywRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_hooks_fn = lambda name: True\n",
        "def print_shape(tensor, hook):\n",
        "    print(f'Activation at hook {hook.name} has shape:')\n",
        "    print(tensor.shape)\n",
        "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
        "logits = model.run_with_hooks(random_tokens, fwd_hooks=[(all_hooks_fn, print_shape)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXSy8SvF2AmS",
        "outputId": "cdff0055-8b6b-4fb0-8df4-ad9f97f9651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation at hook hook_embed has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook hook_pos_embed has shape:\n",
            "torch.Size([50, 768])\n",
            "Activation at hook blocks.0.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.0.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.0.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.0.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.0.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.0.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.0.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.0.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.0.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.0.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.0.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.0.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.0.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.0.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.1.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.1.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.1.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.1.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.1.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.1.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.1.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.1.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.1.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.1.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.1.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.1.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.1.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.1.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.1.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.2.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.2.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.2.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.2.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.2.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.2.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.2.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.2.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.2.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.2.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.2.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.2.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.2.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.2.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.2.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.3.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.3.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.3.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.3.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.3.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.3.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.3.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.3.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.3.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.3.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.3.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.3.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.3.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.3.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.3.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.4.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.4.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.4.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.4.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.4.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.4.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.4.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.4.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.4.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.4.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.4.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.4.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.4.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.4.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.4.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.5.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.5.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.5.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.5.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.5.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.5.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.5.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.5.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.5.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.5.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.5.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.5.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.5.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.5.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.5.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.6.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.6.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.6.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.6.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.6.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.6.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.6.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.6.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.6.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.6.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.6.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.6.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.6.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.6.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.6.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.7.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.7.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.7.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.7.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.7.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.7.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.7.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.7.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.7.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.7.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.7.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.7.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.7.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.7.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.7.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.8.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.8.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.8.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.8.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.8.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.8.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.8.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.8.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.8.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.8.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.8.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.8.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.8.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.8.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.8.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.9.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.9.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.9.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.9.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.9.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.9.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.9.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.9.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.9.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.9.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.9.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.9.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.9.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.9.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.9.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.10.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.10.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.10.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.10.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.10.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.10.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.10.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.10.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.10.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.10.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.10.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.10.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.10.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.10.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.10.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.11.hook_resid_pre has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.11.ln1.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.11.attn.hook_q has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.11.attn.hook_k has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.11.attn.hook_v has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.11.attn.hook_attn_scores has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.11.attn.hook_attn has shape:\n",
            "torch.Size([4, 12, 50, 50])\n",
            "Activation at hook blocks.11.attn.hook_z has shape:\n",
            "torch.Size([4, 50, 12, 64])\n",
            "Activation at hook blocks.11.hook_attn_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.11.hook_resid_mid has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.11.ln2.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n",
            "Activation at hook blocks.11.mlp.hook_pre has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.11.mlp.hook_post has shape:\n",
            "torch.Size([4, 50, 3072])\n",
            "Activation at hook blocks.11.hook_mlp_out has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook blocks.11.hook_resid_post has shape:\n",
            "torch.Size([4, 50, 768])\n",
            "Activation at hook ln_final.hook_scale has shape:\n",
            "torch.Size([4, 50, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the top corner of all activations\n",
        "\n",
        "**Note:** This is useful to do as a sanity check when debugging a model, to quickly and roughly compare the new activations to the original activations (without looking at the full enormous tensors)"
      ],
      "metadata": {
        "id": "-UfrPu8T0Bxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_corner(tensor, hook):\n",
        "    print(hook.name)\n",
        "    print(get_corner(tensor))\n",
        "logits = model.run_with_hooks(tokens, fwd_hooks=[(all_hooks_fn, print_corner)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEMNHBEX216N",
        "outputId": "32b08aa3-efec-4641-b66f-e95615357bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hook_embed\n",
            "tensor([[[ 0.1600, -0.1444],\n",
            "         [-0.0406, -0.2098]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "hook_pos_embed\n",
            "tensor([[-0.0134, -0.1920],\n",
            "        [ 0.0250, -0.0528]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.hook_resid_pre\n",
            "tensor([[[ 0.1466, -0.3363],\n",
            "         [-0.0156, -0.2626]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.ln1.hook_scale\n",
            "tensor([[[0.3703],\n",
            "         [0.2421]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_q\n",
            "tensor([[[[-0.6830,  0.1875],\n",
            "          [ 0.5510,  0.1701]],\n",
            "\n",
            "         [[ 0.4226,  0.8636],\n",
            "          [ 0.1361, -0.6476]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_k\n",
            "tensor([[[[-1.1447,  2.1864],\n",
            "          [ 1.4646,  0.4051]],\n",
            "\n",
            "         [[-1.5435,  2.9672],\n",
            "          [ 0.9403, -1.4134]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_v\n",
            "tensor([[[[-0.0110,  0.0460],\n",
            "          [ 0.4635,  0.0313]],\n",
            "\n",
            "         [[ 0.1250, -0.3105],\n",
            "          [ 0.3096,  0.2382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_attn_scores\n",
            "tensor([[[[-2.3346e-01, -1.0000e+05],\n",
            "          [ 6.0883e-01, -6.4138e-01]],\n",
            "\n",
            "         [[ 4.0300e+00, -1.0000e+05],\n",
            "          [ 3.6600e+00,  1.2221e+01]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_attn\n",
            "tensor([[[[1.0000e+00, 0.0000e+00],\n",
            "          [7.7733e-01, 2.2267e-01]],\n",
            "\n",
            "         [[1.0000e+00, 0.0000e+00],\n",
            "          [1.9132e-04, 9.9981e-01]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.0.attn.hook_z\n",
            "tensor([[[[-0.0110,  0.0460],\n",
            "          [ 0.4635,  0.0313]],\n",
            "\n",
            "         [[ 0.0193, -0.0334],\n",
            "          [ 0.3097,  0.2382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.hook_attn_out\n",
            "tensor([[[ 2.8893, -0.2035],\n",
            "         [ 1.4210, -0.6418]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.hook_resid_mid\n",
            "tensor([[[ 3.0359, -0.5398],\n",
            "         [ 1.4054, -0.9044]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.ln2.hook_scale\n",
            "tensor([[[1.0244],\n",
            "         [1.2495]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.mlp.hook_pre\n",
            "tensor([[[-0.0804, -1.9470],\n",
            "         [ 0.2039, -1.0426]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.mlp.hook_post\n",
            "tensor([[[-0.0376, -0.0501],\n",
            "         [ 0.1184, -0.1551]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.hook_mlp_out\n",
            "tensor([[[ 1.4300, -1.3753],\n",
            "         [-1.1018, -1.7305]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.0.hook_resid_post\n",
            "tensor([[[ 4.4659, -1.9152],\n",
            "         [ 0.3036, -2.6350]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.hook_resid_pre\n",
            "tensor([[[ 4.4659, -1.9152],\n",
            "         [ 0.3036, -2.6350]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.ln1.hook_scale\n",
            "tensor([[[5.2533],\n",
            "         [2.0368]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_q\n",
            "tensor([[[[ 0.1313, -0.2223],\n",
            "          [-0.2588, -0.5800]],\n",
            "\n",
            "         [[-2.5284, -0.2904],\n",
            "          [ 0.1441,  0.2812]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_k\n",
            "tensor([[[[-0.9837,  1.5757],\n",
            "          [-1.5389, -0.6636]],\n",
            "\n",
            "         [[ 0.5076,  1.2804],\n",
            "          [-0.3490,  0.1548]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_v\n",
            "tensor([[[[ 0.1692, -0.0567],\n",
            "          [-0.1223, -0.0142]],\n",
            "\n",
            "         [[-0.2674, -0.1515],\n",
            "          [-0.1510, -0.4548]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_attn_scores\n",
            "tensor([[[[ 7.9228e-01, -1.0000e+05],\n",
            "          [ 2.2300e+00, -4.0063e+00]],\n",
            "\n",
            "         [[ 7.1117e-01, -1.0000e+05],\n",
            "          [ 5.5577e-01, -2.6595e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9980, 0.0020]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9614, 0.0386]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.attn.hook_z\n",
            "tensor([[[[ 0.1692, -0.0567],\n",
            "          [-0.1223, -0.0142]],\n",
            "\n",
            "         [[ 0.1683, -0.0568],\n",
            "          [-0.1234, -0.0312]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.hook_attn_out\n",
            "tensor([[[ 0.2312, -0.2081],\n",
            "         [ 0.8123,  0.4268]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.hook_resid_mid\n",
            "tensor([[[ 4.6971, -2.1232],\n",
            "         [ 1.1160, -2.2082]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.ln2.hook_scale\n",
            "tensor([[[6.2248],\n",
            "         [2.2192]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.mlp.hook_pre\n",
            "tensor([[[ 0.2623, -1.0880],\n",
            "         [-0.3285, -3.8161]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.mlp.hook_post\n",
            "tensor([[[ 0.1583, -0.1507],\n",
            "         [-0.1220, -0.0002]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.hook_mlp_out\n",
            "tensor([[[-1.2580, -0.0978],\n",
            "         [-0.4065,  0.5446]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.1.hook_resid_post\n",
            "tensor([[[ 3.4391, -2.2210],\n",
            "         [ 0.7095, -1.6636]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.hook_resid_pre\n",
            "tensor([[[ 3.4391, -2.2210],\n",
            "         [ 0.7095, -1.6636]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.ln1.hook_scale\n",
            "tensor([[[21.6901],\n",
            "         [ 2.1257]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_q\n",
            "tensor([[[[ 0.1714, -0.3255],\n",
            "          [-0.0372, -0.2601]],\n",
            "\n",
            "         [[ 0.0202,  1.7867],\n",
            "          [-0.2622, -0.5518]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_k\n",
            "tensor([[[[-0.1275, -1.1551],\n",
            "          [-0.4914,  0.5046]],\n",
            "\n",
            "         [[-0.0608, -3.2119],\n",
            "          [-2.3510, -0.1151]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_v\n",
            "tensor([[[[-0.0364,  0.0073],\n",
            "          [ 0.0049, -0.0400]],\n",
            "\n",
            "         [[-0.2338, -0.0141],\n",
            "          [-0.1485, -0.0420]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_attn_scores\n",
            "tensor([[[[ 1.4845e-01, -1.0000e+05],\n",
            "          [-4.0606e-01, -4.5756e+00]],\n",
            "\n",
            "         [[-6.5884e-01, -1.0000e+05],\n",
            "          [ 2.4257e+00, -1.6846e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9848, 0.0152]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9839, 0.0161]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.attn.hook_z\n",
            "tensor([[[[-0.0364,  0.0073],\n",
            "          [ 0.0049, -0.0400]],\n",
            "\n",
            "         [[-0.0394,  0.0070],\n",
            "          [ 0.0024, -0.0400]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.hook_attn_out\n",
            "tensor([[[ 0.0806, -0.1475],\n",
            "         [ 0.0894, -0.1102]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.hook_resid_mid\n",
            "tensor([[[ 3.5198, -2.3686],\n",
            "         [ 0.7989, -1.7739]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.ln2.hook_scale\n",
            "tensor([[[21.7232],\n",
            "         [ 2.1729]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.mlp.hook_pre\n",
            "tensor([[[ 0.6213, -1.7141],\n",
            "         [-0.4335, -2.3515]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.mlp.hook_post\n",
            "tensor([[[ 0.4553, -0.0743],\n",
            "         [-0.1441, -0.0216]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.hook_mlp_out\n",
            "tensor([[[-3.5084, -3.0720],\n",
            "         [-0.8282, -0.4658]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.2.hook_resid_post\n",
            "tensor([[[ 0.0113, -5.4405],\n",
            "         [-0.0292, -2.2397]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.hook_resid_pre\n",
            "tensor([[[ 0.0113, -5.4405],\n",
            "         [-0.0292, -2.2397]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.ln1.hook_scale\n",
            "tensor([[[92.2422],\n",
            "         [ 2.2299]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_q\n",
            "tensor([[[[-0.3975, -0.0909],\n",
            "          [-0.1682,  0.0364]],\n",
            "\n",
            "         [[ 0.0580, -1.6875],\n",
            "          [-0.3656,  0.9330]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_k\n",
            "tensor([[[[ 0.0097, -0.1900],\n",
            "          [ 0.8230,  0.2071]],\n",
            "\n",
            "         [[ 0.2231, -1.0661],\n",
            "          [ 1.4244, -2.4477]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_v\n",
            "tensor([[[[ 0.0393,  0.0578],\n",
            "          [-0.0519,  0.0038]],\n",
            "\n",
            "         [[-0.0018, -1.0726],\n",
            "          [-0.2352, -0.5483]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_attn_scores\n",
            "tensor([[[[ 1.3210e+00, -1.0000e+05],\n",
            "          [ 7.4256e+00,  1.7582e+00]],\n",
            "\n",
            "         [[ 3.3527e-01, -1.0000e+05],\n",
            "          [-2.0173e+00, -5.8889e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9966, 0.0034]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9796, 0.0204]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.attn.hook_z\n",
            "tensor([[[[ 0.0393,  0.0578],\n",
            "          [-0.0519,  0.0038]],\n",
            "\n",
            "         [[ 0.0391,  0.0539],\n",
            "          [-0.0557, -0.0074]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.hook_attn_out\n",
            "tensor([[[ 0.0786,  0.0225],\n",
            "         [ 0.0839, -0.0046]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.hook_resid_mid\n",
            "tensor([[[ 0.0900, -5.4180],\n",
            "         [ 0.0547, -2.2443]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.ln2.hook_scale\n",
            "tensor([[[92.1455],\n",
            "         [ 2.2206]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.mlp.hook_pre\n",
            "tensor([[[-2.4675,  0.0471],\n",
            "         [-0.9537, -0.4964]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.mlp.hook_post\n",
            "tensor([[[-0.0164,  0.0244],\n",
            "         [-0.1624, -0.1538]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.hook_mlp_out\n",
            "tensor([[[-0.3291, -0.2717],\n",
            "         [ 0.2559, -0.0947]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.3.hook_resid_post\n",
            "tensor([[[-0.2391, -5.6897],\n",
            "         [ 0.3105, -2.3390]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.hook_resid_pre\n",
            "tensor([[[-0.2391, -5.6897],\n",
            "         [ 0.3105, -2.3390]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.ln1.hook_scale\n",
            "tensor([[[97.8012],\n",
            "         [ 2.2842]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_q\n",
            "tensor([[[[ 0.0176,  0.3904],\n",
            "          [-0.3139, -0.2219]],\n",
            "\n",
            "         [[ 0.5400, -0.4764],\n",
            "          [-0.1174,  0.7665]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_k\n",
            "tensor([[[[-0.8765, -0.1873],\n",
            "          [ 0.3582, -0.0534]],\n",
            "\n",
            "         [[ 2.0230, -0.7626],\n",
            "          [-1.2262,  0.6144]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_v\n",
            "tensor([[[[ 0.0016, -0.0403],\n",
            "          [-0.0679, -0.0094]],\n",
            "\n",
            "         [[-0.3058,  0.1767],\n",
            "          [ 0.0705, -0.0585]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_attn_scores\n",
            "tensor([[[[ 5.3280e-01, -1.0000e+05],\n",
            "          [-2.3807e+00, -5.7206e+00]],\n",
            "\n",
            "         [[ 3.4953e-01, -1.0000e+05],\n",
            "          [-1.3918e+00, -7.3798e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9658, 0.0342]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9975, 0.0025]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.attn.hook_z\n",
            "tensor([[[[ 0.0016, -0.0403],\n",
            "          [-0.0679, -0.0094]],\n",
            "\n",
            "         [[-0.0089, -0.0329],\n",
            "          [-0.0675, -0.0095]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.hook_attn_out\n",
            "tensor([[[-0.1218, -0.1480],\n",
            "         [-0.1804, -0.1694]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.hook_resid_mid\n",
            "tensor([[[-0.3609, -5.8377],\n",
            "         [ 0.1301, -2.5084]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.ln2.hook_scale\n",
            "tensor([[[97.7663],\n",
            "         [ 2.2812]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.mlp.hook_pre\n",
            "tensor([[[ 0.8037, -2.7673],\n",
            "         [-0.8216, -0.8877]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.mlp.hook_post\n",
            "tensor([[[ 0.6342, -0.0074],\n",
            "         [-0.1691, -0.1664]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.hook_mlp_out\n",
            "tensor([[[-0.2850, -0.1238],\n",
            "         [-0.4809, -0.6506]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.4.hook_resid_post\n",
            "tensor([[[-0.6459, -5.9615],\n",
            "         [-0.3508, -3.1590]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.hook_resid_pre\n",
            "tensor([[[-0.6459, -5.9615],\n",
            "         [-0.3508, -3.1590]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.ln1.hook_scale\n",
            "tensor([[[102.8123],\n",
            "         [  2.6391]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_q\n",
            "tensor([[[[-0.0060,  0.1887],\n",
            "          [-0.3377,  0.1366]],\n",
            "\n",
            "         [[-0.9963,  0.3743],\n",
            "          [ 2.4280,  2.2084]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_k\n",
            "tensor([[[[ 0.0142, -0.3002],\n",
            "          [ 0.1472,  0.9930]],\n",
            "\n",
            "         [[ 2.2038, -1.0087],\n",
            "          [-3.0268, -6.6457]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_v\n",
            "tensor([[[[-0.0215, -0.0149],\n",
            "          [ 0.0025, -0.0147]],\n",
            "\n",
            "         [[ 1.0960, -0.3051],\n",
            "          [-0.0638, -1.3372]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_attn_scores\n",
            "tensor([[[[ 5.3814e-01, -1.0000e+05],\n",
            "          [ 2.7285e+00, -1.9519e+00]],\n",
            "\n",
            "         [[ 3.0952e-01, -1.0000e+05],\n",
            "          [ 4.5407e+00, -7.1070e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_attn\n",
            "tensor([[[[1.0000e+00, 0.0000e+00],\n",
            "          [9.9081e-01, 9.1899e-03]],\n",
            "\n",
            "         [[1.0000e+00, 0.0000e+00],\n",
            "          [9.9999e-01, 8.7391e-06]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.5.attn.hook_z\n",
            "tensor([[[[-0.0215, -0.0149],\n",
            "          [ 0.0025, -0.0147]],\n",
            "\n",
            "         [[-0.0112, -0.0176],\n",
            "          [ 0.0025, -0.0147]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.hook_attn_out\n",
            "tensor([[[-0.0162, -0.1259],\n",
            "         [ 0.0186, -0.1219]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.hook_resid_mid\n",
            "tensor([[[-0.6621, -6.0874],\n",
            "         [-0.3322, -3.2810]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.ln2.hook_scale\n",
            "tensor([[[102.7626],\n",
            "         [  2.6281]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.mlp.hook_pre\n",
            "tensor([[[-1.6365,  0.3580],\n",
            "         [-0.0223, -0.9029]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.mlp.hook_post\n",
            "tensor([[[-0.0834,  0.2290],\n",
            "         [-0.0109, -0.1656]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.hook_mlp_out\n",
            "tensor([[[-0.1288, -0.1271],\n",
            "         [-0.1899,  0.7349]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.5.hook_resid_post\n",
            "tensor([[[-0.7910, -6.2145],\n",
            "         [-0.5220, -2.5461]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.hook_resid_pre\n",
            "tensor([[[-0.7910, -6.2145],\n",
            "         [-0.5220, -2.5461]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.ln1.hook_scale\n",
            "tensor([[[105.9458],\n",
            "         [  2.8768]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_q\n",
            "tensor([[[[-0.1297, -0.2325],\n",
            "          [-0.2744,  0.5696]],\n",
            "\n",
            "         [[-1.8815,  1.5393],\n",
            "          [ 1.0142,  0.3528]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_k\n",
            "tensor([[[[-0.3286,  0.8828],\n",
            "          [ 0.0438,  0.8537]],\n",
            "\n",
            "         [[-0.2936, -4.4944],\n",
            "          [-0.2204, -1.2930]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_v\n",
            "tensor([[[[ 0.0514, -0.0394],\n",
            "          [ 0.0726,  0.0171]],\n",
            "\n",
            "         [[-0.5484,  0.2877],\n",
            "          [ 0.8865, -0.2907]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_attn_scores\n",
            "tensor([[[[ 1.5434e-01, -1.0000e+05],\n",
            "          [ 1.0863e-01, -3.7348e+00]],\n",
            "\n",
            "         [[ 7.0014e-01, -1.0000e+05],\n",
            "          [ 1.0427e+00, -2.2651e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9790, 0.0210]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9647, 0.0353]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.attn.hook_z\n",
            "tensor([[[[ 0.0514, -0.0394],\n",
            "          [ 0.0726,  0.0171]],\n",
            "\n",
            "         [[ 0.0388, -0.0325],\n",
            "          [ 0.1013,  0.0063]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.hook_attn_out\n",
            "tensor([[[-0.0118, -0.0110],\n",
            "         [-0.0058, -0.0594]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.hook_resid_mid\n",
            "tensor([[[-0.8027, -6.2255],\n",
            "         [-0.5278, -2.6055]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.ln2.hook_scale\n",
            "tensor([[[105.9772],\n",
            "         [  2.8952]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.mlp.hook_pre\n",
            "tensor([[[ 0.2320, -0.9757],\n",
            "         [-0.1395, -2.6417]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.mlp.hook_post\n",
            "tensor([[[ 0.1373, -0.1608],\n",
            "         [-0.0620, -0.0104]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.hook_mlp_out\n",
            "tensor([[[-0.1333, -0.0200],\n",
            "         [-0.5677, -0.4207]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.6.hook_resid_post\n",
            "tensor([[[-0.9360, -6.2455],\n",
            "         [-1.0955, -3.0261]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.hook_resid_pre\n",
            "tensor([[[-0.9360, -6.2455],\n",
            "         [-1.0955, -3.0261]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.ln1.hook_scale\n",
            "tensor([[[107.8283],\n",
            "         [  3.1934]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_q\n",
            "tensor([[[[ 0.0529, -0.3191],\n",
            "          [ 0.1380, -0.1388]],\n",
            "\n",
            "         [[ 1.1379,  0.0989],\n",
            "          [-1.1619,  1.7103]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_k\n",
            "tensor([[[[ 1.0303, -0.2729],\n",
            "          [-0.1340, -0.0716]],\n",
            "\n",
            "         [[-5.2789, -2.9104],\n",
            "          [-1.1835,  0.7608]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_v\n",
            "tensor([[[[-0.0354,  0.0511],\n",
            "          [-0.0016, -0.0331]],\n",
            "\n",
            "         [[ 0.0496,  0.5697],\n",
            "          [ 0.1556, -0.5996]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_attn_scores\n",
            "tensor([[[[ 6.4755e-01, -1.0000e+05],\n",
            "          [-1.4314e+00, -3.4261e+00]],\n",
            "\n",
            "         [[ 6.9034e-01, -1.0000e+05],\n",
            "          [ 4.5207e+00, -1.7921e-01]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.8802, 0.1198]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9910, 0.0090]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.attn.hook_z\n",
            "tensor([[[[-0.0354,  0.0511],\n",
            "          [-0.0016, -0.0331]],\n",
            "\n",
            "         [[-0.0252,  0.1133],\n",
            "          [-0.0002, -0.0382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.hook_attn_out\n",
            "tensor([[[-0.0668,  0.0197],\n",
            "         [-0.0512, -0.0352]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.hook_resid_mid\n",
            "tensor([[[-1.0028, -6.2257],\n",
            "         [-1.1467, -3.0613]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.ln2.hook_scale\n",
            "tensor([[[107.8495],\n",
            "         [  3.2176]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.mlp.hook_pre\n",
            "tensor([[[ 0.1101,  0.3920],\n",
            "         [-0.1142, -0.2265]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.mlp.hook_post\n",
            "tensor([[[ 0.0599,  0.2558],\n",
            "         [-0.0519, -0.0930]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.hook_mlp_out\n",
            "tensor([[[-0.1073,  0.0198],\n",
            "         [-0.5983,  1.3603]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.7.hook_resid_post\n",
            "tensor([[[-1.1101, -6.2059],\n",
            "         [-1.7449, -1.7010]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.hook_resid_pre\n",
            "tensor([[[-1.1101, -6.2059],\n",
            "         [-1.7449, -1.7010]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.ln1.hook_scale\n",
            "tensor([[[108.9520],\n",
            "         [  3.5777]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_q\n",
            "tensor([[[[ 0.1549,  0.4809],\n",
            "          [ 0.0734,  0.0527]],\n",
            "\n",
            "         [[ 0.1936, -2.7093],\n",
            "          [ 0.3946,  0.3873]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_k\n",
            "tensor([[[[-0.0238, -2.3359],\n",
            "          [-0.8001,  0.2133]],\n",
            "\n",
            "         [[-1.2419,  4.4339],\n",
            "          [ 1.4451, -0.7873]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_v\n",
            "tensor([[[[ 0.0624, -0.0107],\n",
            "          [ 0.0244,  0.0348]],\n",
            "\n",
            "         [[-0.2527,  1.3336],\n",
            "          [-0.6872,  1.2470]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_attn_scores\n",
            "tensor([[[[ 1.1109e-01, -1.0000e+05],\n",
            "          [ 1.1687e+00, -3.3149e+00]],\n",
            "\n",
            "         [[ 9.2875e-01, -1.0000e+05],\n",
            "          [ 4.6011e+00, -7.5771e-01]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9888, 0.0112]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9953, 0.0047]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.attn.hook_z\n",
            "tensor([[[[ 0.0624, -0.0107],\n",
            "          [ 0.0244,  0.0348]],\n",
            "\n",
            "         [[ 0.0589,  0.0043],\n",
            "          [ 0.0211,  0.0405]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.hook_attn_out\n",
            "tensor([[[-0.0411, -0.0498],\n",
            "         [-0.0289, -0.0137]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.hook_resid_mid\n",
            "tensor([[[-1.1513, -6.2557],\n",
            "         [-1.7739, -1.7147]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.ln2.hook_scale\n",
            "tensor([[[109.0143],\n",
            "         [  3.6601]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.mlp.hook_pre\n",
            "tensor([[[ 0.1417, -1.0045],\n",
            "         [-0.8862, -0.7948]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.mlp.hook_post\n",
            "tensor([[[ 0.0788, -0.1584],\n",
            "         [-0.1665, -0.1697]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.hook_mlp_out\n",
            "tensor([[[0.0293, 0.0111],\n",
            "         [0.4555, 0.9246]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.8.hook_resid_post\n",
            "tensor([[[-1.1220, -6.2446],\n",
            "         [-1.3184, -0.7901]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.hook_resid_pre\n",
            "tensor([[[-1.1220, -6.2446],\n",
            "         [-1.3184, -0.7901]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.ln1.hook_scale\n",
            "tensor([[[109.6327],\n",
            "         [  4.2052]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_q\n",
            "tensor([[[[ 0.3234,  0.2621],\n",
            "          [-0.1864,  0.2236]],\n",
            "\n",
            "         [[ 0.4601, -0.8625],\n",
            "          [ 0.9171,  1.9632]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_k\n",
            "tensor([[[[ 0.0595, -0.2623],\n",
            "          [-0.2702,  0.1587]],\n",
            "\n",
            "         [[ 0.0263, -0.3892],\n",
            "          [ 1.0577,  0.7877]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_v\n",
            "tensor([[[[-0.0445, -0.0842],\n",
            "          [ 0.0314, -0.0117]],\n",
            "\n",
            "         [[ 0.2324, -0.9131],\n",
            "          [ 1.0140,  1.5225]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_attn_scores\n",
            "tensor([[[[ 1.2355e+00, -1.0000e+05],\n",
            "          [ 2.8021e+00, -1.0077e+00]],\n",
            "\n",
            "         [[ 1.2389e+00, -1.0000e+05],\n",
            "          [ 3.8741e+00, -1.7935e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9783, 0.0217]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9966, 0.0034]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.attn.hook_z\n",
            "tensor([[[[-0.0445, -0.0842],\n",
            "          [ 0.0314, -0.0117]],\n",
            "\n",
            "         [[-0.0385, -0.1021],\n",
            "          [ 0.0348, -0.0064]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.hook_attn_out\n",
            "tensor([[[-1.0932e-01,  8.4637e-02],\n",
            "         [-1.1288e-01,  2.4669e-05]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.9.hook_resid_mid\n",
            "tensor([[[-1.2313, -6.1599],\n",
            "         [-1.4313, -0.7900]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.ln2.hook_scale\n",
            "tensor([[[109.6836],\n",
            "         [  4.3473]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.mlp.hook_pre\n",
            "tensor([[[-1.9287,  0.1085],\n",
            "         [-1.3524, -0.7526]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.mlp.hook_post\n",
            "tensor([[[-0.0518,  0.0589],\n",
            "         [-0.1194, -0.1700]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.hook_mlp_out\n",
            "tensor([[[ 0.0074,  0.0072],\n",
            "         [-1.5900, -1.5532]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.9.hook_resid_post\n",
            "tensor([[[-1.2239, -6.1527],\n",
            "         [-3.0213, -2.3433]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.hook_resid_pre\n",
            "tensor([[[-1.2239, -6.1527],\n",
            "         [-3.0213, -2.3433]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.ln1.hook_scale\n",
            "tensor([[[109.9465],\n",
            "         [  5.1061]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_q\n",
            "tensor([[[[ 0.1435,  0.6087],\n",
            "          [ 0.4038, -0.5040]],\n",
            "\n",
            "         [[ 0.0268,  0.4666],\n",
            "          [ 0.5838, -2.1377]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_k\n",
            "tensor([[[[-0.5306,  0.4872],\n",
            "          [ 0.8474, -2.0740]],\n",
            "\n",
            "         [[ 0.9402,  1.2441],\n",
            "          [-0.5061,  1.9980]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_v\n",
            "tensor([[[[-0.0204,  0.0575],\n",
            "          [ 0.0239,  0.0285]],\n",
            "\n",
            "         [[ 0.0852, -0.4385],\n",
            "          [-0.8098,  0.1202]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_attn_scores\n",
            "tensor([[[[ 1.8808e+00, -1.0000e+05],\n",
            "          [ 2.7780e+00, -1.0338e+00]],\n",
            "\n",
            "         [[ 2.1647e+00, -1.0000e+05],\n",
            "          [ 4.8024e+00,  4.9549e-02]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.9784, 0.0216]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9914, 0.0086]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.attn.hook_z\n",
            "tensor([[[[-0.0204,  0.0575],\n",
            "          [ 0.0239,  0.0285]],\n",
            "\n",
            "         [[-0.0181,  0.0467],\n",
            "          [ 0.0168,  0.0293]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.hook_attn_out\n",
            "tensor([[[-0.0429,  0.0497],\n",
            "         [-0.0530,  0.0353]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.hook_resid_mid\n",
            "tensor([[[-1.2668, -6.1030],\n",
            "         [-3.0743, -2.3080]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.ln2.hook_scale\n",
            "tensor([[[109.9779],\n",
            "         [  5.2732]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.mlp.hook_pre\n",
            "tensor([[[-0.8390, -0.0591],\n",
            "         [-0.5972,  0.2644]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.mlp.hook_post\n",
            "tensor([[[-0.1685, -0.0282],\n",
            "         [-0.1644,  0.1598]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.hook_mlp_out\n",
            "tensor([[[-0.0691,  0.0629],\n",
            "         [ 0.3383,  1.2087]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.10.hook_resid_post\n",
            "tensor([[[-1.3360, -6.0401],\n",
            "         [-2.7359, -1.0993]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.hook_resid_pre\n",
            "tensor([[[-1.3360, -6.0401],\n",
            "         [-2.7359, -1.0993]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.ln1.hook_scale\n",
            "tensor([[[109.9140],\n",
            "         [  8.4298]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_q\n",
            "tensor([[[[-2.0579,  1.9066],\n",
            "          [-1.6736,  0.7516]],\n",
            "\n",
            "         [[-0.6028, -0.4129],\n",
            "          [-0.3271,  0.1361]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_k\n",
            "tensor([[[[-1.7122, -0.3213],\n",
            "          [ 0.1204, -0.0665]],\n",
            "\n",
            "         [[ 0.8030,  0.1721],\n",
            "          [ 0.0151,  0.0570]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_v\n",
            "tensor([[[[ 0.0431, -0.0790],\n",
            "          [ 0.1141, -0.0090]],\n",
            "\n",
            "         [[ 0.5817,  1.1435],\n",
            "          [-0.1152, -0.0842]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_attn_scores\n",
            "tensor([[[[ 9.2617e-01, -1.0000e+05],\n",
            "          [ 1.8761e+00,  8.5164e-01]],\n",
            "\n",
            "         [[ 2.5052e+00, -1.0000e+05],\n",
            "          [ 3.9733e+00, -1.7404e+00]]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_attn\n",
            "tensor([[[[1.0000, 0.0000],\n",
            "          [0.7358, 0.2642]],\n",
            "\n",
            "         [[1.0000, 0.0000],\n",
            "          [0.9967, 0.0033]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.attn.hook_z\n",
            "tensor([[[[ 0.0431, -0.0790],\n",
            "          [ 0.1141, -0.0090]],\n",
            "\n",
            "         [[ 0.1854,  0.2439],\n",
            "          [ 0.1134, -0.0092]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.hook_attn_out\n",
            "tensor([[[ 4.3697,  4.2028],\n",
            "         [ 0.7777, -1.0227]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.hook_resid_mid\n",
            "tensor([[[ 3.0337, -1.8373],\n",
            "         [-1.9582, -2.1220]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.ln2.hook_scale\n",
            "tensor([[[16.4910],\n",
            "         [10.3359]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.mlp.hook_pre\n",
            "tensor([[[-0.1672, -0.4566],\n",
            "         [-0.1546, -1.0319]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.mlp.hook_post\n",
            "tensor([[[-0.0725, -0.1479],\n",
            "         [-0.0678, -0.1560]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.hook_mlp_out\n",
            "tensor([[[-0.1139,  0.5125],\n",
            "         [ 0.3237,  0.5095]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "blocks.11.hook_resid_post\n",
            "tensor([[[ 2.9198, -1.3248],\n",
            "         [-1.6345, -1.6125]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "ln_final.hook_scale\n",
            "tensor([[[12.5450],\n",
            "         [12.2695]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cache all activations\n"
      ],
      "metadata": {
        "id": "Nld1pU8z0DFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "model.reset_hooks()\n",
        "model.cache_all(cache)\n",
        "logits = model(tokens)\n",
        "for name in cache:\n",
        "    print(name, cache[name].shape)\n",
        "model.reset_hooks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_8_SsnD3VKr",
        "outputId": "0debab60-292b-464c-9a7b-bcf08a6a8386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hook_embed torch.Size([1, 5, 768])\n",
            "hook_pos_embed torch.Size([5, 768])\n",
            "blocks.0.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.0.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.0.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.0.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.0.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.0.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.0.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.0.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.0.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.0.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.0.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.0.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.0.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.0.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.1.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.1.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.1.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.1.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.1.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.1.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.1.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.1.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.1.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.1.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.1.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.1.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.1.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.1.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.1.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.2.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.2.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.2.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.2.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.2.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.2.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.2.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.2.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.2.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.2.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.2.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.2.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.2.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.2.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.2.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.3.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.3.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.3.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.3.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.3.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.3.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.3.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.3.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.3.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.3.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.3.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.3.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.3.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.3.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.3.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.4.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.4.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.4.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.4.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.4.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.4.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.4.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.4.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.4.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.4.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.4.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.4.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.4.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.4.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.4.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.5.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.5.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.5.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.5.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.5.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.5.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.5.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.5.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.5.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.5.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.5.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.5.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.5.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.5.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.5.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.6.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.6.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.6.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.6.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.6.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.6.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.6.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.6.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.6.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.6.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.6.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.6.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.6.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.6.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.6.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.7.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.7.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.7.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.7.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.7.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.7.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.7.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.7.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.7.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.7.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.7.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.7.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.7.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.7.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.7.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.8.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.8.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.8.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.8.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.8.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.8.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.8.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.8.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.8.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.8.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.8.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.8.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.8.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.8.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.8.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.9.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.9.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.9.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.9.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.9.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.9.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.9.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.9.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.9.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.9.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.9.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.9.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.9.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.9.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.9.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.10.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.10.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.10.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.10.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.10.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.10.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.10.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.10.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.10.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.10.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.10.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.10.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.10.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.10.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.10.hook_resid_post torch.Size([1, 5, 768])\n",
            "blocks.11.hook_resid_pre torch.Size([1, 5, 768])\n",
            "blocks.11.ln1.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.11.attn.hook_q torch.Size([1, 5, 12, 64])\n",
            "blocks.11.attn.hook_k torch.Size([1, 5, 12, 64])\n",
            "blocks.11.attn.hook_v torch.Size([1, 5, 12, 64])\n",
            "blocks.11.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",
            "blocks.11.attn.hook_attn torch.Size([1, 12, 5, 5])\n",
            "blocks.11.attn.hook_z torch.Size([1, 5, 12, 64])\n",
            "blocks.11.hook_attn_out torch.Size([1, 5, 768])\n",
            "blocks.11.hook_resid_mid torch.Size([1, 5, 768])\n",
            "blocks.11.ln2.hook_scale torch.Size([1, 5, 1])\n",
            "blocks.11.mlp.hook_pre torch.Size([1, 5, 3072])\n",
            "blocks.11.mlp.hook_post torch.Size([1, 5, 3072])\n",
            "blocks.11.hook_mlp_out torch.Size([1, 5, 768])\n",
            "blocks.11.hook_resid_post torch.Size([1, 5, 768])\n",
            "ln_final.hook_scale torch.Size([1, 5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save GPU memory, we can cache activations to the CPU - note that this is much slower though, since it requires copying."
      ],
      "metadata": {
        "id": "Wikbh8pUE5l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_tokens = torch.randint(1000, 10000, (1, 300))\n",
        "cache = {}\n",
        "model.reset_hooks()\n",
        "model.cache_all(cache, device='cpu')\n",
        "print('Run time when copying to the CPU')\n",
        "%timeit logits = model(random_tokens)\n",
        "model.reset_hooks()\n",
        "model.cache_all(cache, device='cuda')\n",
        "print('Run time when just caching on GPU')\n",
        "%timeit logits = model(random_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Aqy2Y6E_Bk",
        "outputId": "cf1c96d3-efd7-4002-a817-d1f5f01c6aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run time when copying to the CPU\n",
            "1 loop, best of 5: 136 ms per loop\n",
            "Run time when just caching on GPU\n",
            "10 loops, best of 5: 31.3 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Editing Activations\n",
        "**To change an activation, add a hook to that HookPoint which returns the new activation**"
      ],
      "metadata": {
        "id": "r4tCBmPu4sY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning attention heads"
      ],
      "metadata": {
        "id": "j4eBHozL0Dwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - prune heads 0, 3 and 7 from layer 3 and heads 8 and 9 from layer 7\n",
        "layer = 3\n",
        "head_indices = torch.tensor([0, 3, 7])\n",
        "layer_2 = 7\n",
        "head_indices_2 = torch.tensor([8, 9])\n",
        "def prune_fn_1(z, hook):\n",
        "    # The shape of the z tensor is batch x pos x head_index x d_head\n",
        "    z[:, :, head_indices, :] = 0.\n",
        "    return z\n",
        "def prune_fn_2(z, hook):\n",
        "    # The shape of the z tensor is batch x pos x head_index x d_head\n",
        "    z[:, :, head_indices_2, :] = 0.\n",
        "    return z\n",
        "logits = model.run_with_hooks(tokens, fwd_hooks=[(f'blocks.{layer}.attn.hook_z', prune_fn_1),\n",
        "                                                       (f'blocks.{layer_2}.attn.hook_z', prune_fn_2)])"
      ],
      "metadata": {
        "id": "4yMuG37-4KVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restrict all attention heads to only attend to the current and previous token.\n",
        "\n",
        "**Validation:** The logits for the first 2 positions are the same, the logits for pos 3 are different"
      ],
      "metadata": {
        "id": "Ldmuq83q4S0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks()\n",
        "def filter_hook_attn(name):\n",
        "    split_name = name.split('.')\n",
        "    return (split_name[-1]=='hook_attn')\n",
        "def restrict_attn(attn, hook):\n",
        "    # Attn has shape batch x head_index x query_pos x key_pos\n",
        "    n_ctx = attn.size(-2)\n",
        "    key_pos = torch.arange(n_ctx)[None, :]\n",
        "    query_pos = torch.arange(n_ctx)[:, None]\n",
        "    mask = (key_pos>(query_pos-2)).cuda()\n",
        "    ZERO = torch.tensor(0.)\n",
        "    if torch.cuda.is_available():\n",
        "        ZERO = ZERO.cuda()\n",
        "    attn = torch.where(mask, attn, ZERO)\n",
        "    return attn\n",
        "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, restrict_attn)])\n",
        "print('New logits')\n",
        "print(get_corner(logits, 3))\n",
        "print('Original logits')\n",
        "print(get_corner(original_logits, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZfq0Wq5gpf",
        "outputId": "848c6594-90ba-47af-e9e0-1bcdd5171a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New logits\n",
            "tensor([[[-5.0197, -4.0007, -6.4540],\n",
            "         [-4.1477, -2.2966, -7.4325],\n",
            "         [-2.6209,  2.9651, -4.4607]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Original logits\n",
            "tensor([[[-5.0197, -4.0007, -6.4540],\n",
            "         [-4.1477, -2.2966, -7.4325],\n",
            "         [-2.7587,  1.3903, -4.3042]]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing attention patterns - here we do two runs of the model. First on the original text, caching attn patterns, and secondly on the new text, loading the cached patterns\n"
      ],
      "metadata": {
        "id": "Kon5lVfI0EQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_cache = {}\n",
        "def cache_attn(attn, hook):\n",
        "    attn_cache[hook.name]=attn\n",
        "\n",
        "def freeze_attn(attn, hook):\n",
        "    return attn_cache[hook.name]\n",
        "\n",
        "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",
        "\n",
        "logits_2 = model.run_with_hooks(tokens_2, fwd_hooks=[(filter_hook_attn, freeze_attn)])"
      ],
      "metadata": {
        "id": "OqliBYjD9FzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Hook Contexts"
      ],
      "metadata": {
        "id": "R67U29-hEXYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each hook point has a dictionary `hook.ctx` that can be used to store information between runs** - this is useful for keeping running totals, etc "
      ],
      "metadata": {
        "id": "UtyKcwT3EVne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A running total of times a neuron activation was positive\n"
      ],
      "metadata": {
        "id": "YLNmU4SM06hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We focus on neuron 20 in layer 7\n",
        "model.reset_hooks()\n",
        "animal_texts = ['The dog was green', 'The cat was blue', 'The squid was magenta', 'The blobfish was grey']\n",
        "layer = 7\n",
        "neuron_index = 20\n",
        "def running_total_hook(neuron_acts, hook):\n",
        "    if 'total' not in hook.ctx:\n",
        "        hook.ctx['total']=0\n",
        "    print('Neuron acts:', neuron_acts[0, :, neuron_index])\n",
        "    hook.ctx['total']+=(neuron_acts[0, :, neuron_index]>0).sum().item()\n",
        "    print('Running total:', hook.ctx['total'])\n",
        "\n",
        "for animal_text in animal_texts:\n",
        "    show_tokens(animal_text)\n",
        "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', running_total_hook)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F4wyAp49sr0",
        "outputId": "5df4ed89-6c37-4d4b-949f-8529085ca98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The| dog| was| green\n",
            "Neuron acts: tensor([-0.0099, -0.1396,  0.6828, -0.0826], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Running total: 1\n",
            "The| cat| was| blue\n",
            "Neuron acts: tensor([-0.0099, -0.1045,  0.6142, -0.0625], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Running total: 2\n",
            "The| squid| was| mag|enta\n",
            "Neuron acts: tensor([-0.0099,  0.7520,  0.7486, -0.0986, -0.0460], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Running total: 4\n",
            "The| blob|fish| was| grey\n",
            "Neuron acts: tensor([-0.0099,  0.0554,  0.4716,  0.7331, -0.0160], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Running total: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the dataset example that most activates a given neuron\n"
      ],
      "metadata": {
        "id": "3wzhyU_K07Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We focus on neuron 13 in layer 5\n",
        "model.reset_hooks(clear_contexts=True)\n",
        "animal_texts = ['The dog was green', 'The cat was blue', 'The squid was magenta', 'The blobfish was grey']\n",
        "layer = 5\n",
        "neuron_index = 13\n",
        "def best_act_hook(neuron_acts, hook, text):\n",
        "    if 'best' not in hook.ctx:\n",
        "        hook.ctx['best']=-1e3\n",
        "    print('Neuron acts:', neuron_acts[0, :, neuron_index])\n",
        "    if hook.ctx['best']<neuron_acts[0, :, neuron_index].max():\n",
        "        print(f'Updating best act from {hook.ctx[\"best\"]} to {neuron_acts[0, :, neuron_index].max().item()}')\n",
        "        hook.ctx['best'] = neuron_acts[0, :, neuron_index].max().item()\n",
        "        hook.ctx['text'] = text\n",
        "\n",
        "for animal_text in animal_texts:\n",
        "    (show_tokens(animal_text))\n",
        "    # Use partial to give the hook access to the relevant text\n",
        "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', partial(best_act_hook, text=animal_text))])\n",
        "print()\n",
        "print('Maximally activating dataset example:', model.hook_dict[f'blocks.{layer}.mlp.hook_post'].ctx['text'])\n",
        "model.reset_hooks(clear_contexts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGNdJ-FH_LPc",
        "outputId": "a5baf88d-59cd-4d3e-bff7-14132abade7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The| dog| was| green\n",
            "Neuron acts: tensor([-0.0074, -0.1690,  0.0724,  0.0520], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Updating best act from -1000.0 to 0.07240154594182968\n",
            "The| cat| was| blue\n",
            "Neuron acts: tensor([-0.0074, -0.1681,  0.1947,  0.0884], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Updating best act from 0.07240154594182968 to 0.19472447037696838\n",
            "The| squid| was| mag|enta\n",
            "Neuron acts: tensor([-0.0074, -0.1546,  0.0558, -0.1591, -0.1391], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "The| blob|fish| was| grey\n",
            "Neuron acts: tensor([-0.0074, -0.1700,  0.0445,  0.1009, -0.0254], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Maximally activating dataset example: The cat was blue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fancier Examples"
      ],
      "metadata": {
        "id": "lBzOx_1REcWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking for heads that mostly attend to the previous token\n"
      ],
      "metadata": {
        "id": "7Eb_CK6O07mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n",
        "print('Long text:', long_text)\n",
        "# We first cache attention patterns\n",
        "attn_cache = {}\n",
        "def cache_attn(attn, hook):\n",
        "    attn_cache[hook.name]=attn\n",
        "logits = model.run_with_hooks(long_text, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",
        "\n",
        "# We then go through the cache and find the average attention paid to previous tokens\n",
        "prev_token_scores = np.zeros((model.cfg['n_layers'], model.cfg['n_heads']))\n",
        "for layer in range(model.cfg['n_layers']):\n",
        "    for head in range(model.cfg['n_heads']):\n",
        "        attn = attn_cache[f\"blocks.{layer}.attn.hook_attn\"][0, head]\n",
        "        prev_token_scores[layer, head]=attn.diag(-1).mean().item()\n",
        "\n",
        "px.imshow(prev_token_scores, \n",
        "          x=[f'Head {hi}' for hi in range(model.cfg['n_heads'])], \n",
        "          y=[f'Layer {i}' for i in range(model.cfg['n_layers'])], \n",
        "          title='Prev Token Scores', \n",
        "          color_continuous_scale='Blues')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "zhMVjMYPAG9P",
        "outputId": "8545ea83-72f4-46f0-a140-a09db2342789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long text: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"1d14efb5-a38c-421c-a65c-ff4063980880\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1d14efb5-a38c-421c-a65c-ff4063980880\")) {                    Plotly.newPlot(                        \"1d14efb5-a38c-421c-a65c-ff4063980880\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"Head 0\",\"Head 1\",\"Head 2\",\"Head 3\",\"Head 4\",\"Head 5\",\"Head 6\",\"Head 7\",\"Head 8\",\"Head 9\",\"Head 10\",\"Head 11\"],\"y\":[\"Layer 0\",\"Layer 1\",\"Layer 2\",\"Layer 3\",\"Layer 4\",\"Layer 5\",\"Layer 6\",\"Layer 7\",\"Layer 8\",\"Layer 9\",\"Layer 10\",\"Layer 11\"],\"z\":[[0.043300531804561615,0.005508734378963709,0.035496026277542114,0.04028499498963356,0.08938205242156982,0.006121970247477293,0.04877782613039017,0.262128084897995,0.048664435744285583,0.04307042434811592,0.03245258331298828,0.028410306200385094],[0.2693849503993988,0.16094250977039337,0.0753670409321785,0.04223204776644707,0.07642911374568939,0.02810237556695938,0.0376884825527668,0.03528287634253502,0.029629500582814217,0.021824311465024948,0.053614530712366104,0.012745782732963562],[0.17171591520309448,0.020646080374717712,0.5643648505210876,0.2758866548538208,0.247343510389328,0.299453467130661,0.07991326600313187,0.05568588525056839,0.28546735644340515,0.3376002013683319,0.1176844909787178,0.07105860114097595],[0.009194408543407917,0.12158150225877762,0.4045916497707367,0.43522363901138306,0.016124900430440903,0.03769313544034958,0.28649359941482544,0.4620949625968933,0.3031529486179352,0.12936021387577057,0.08379186689853668,0.1935647428035736],[0.19270046055316925,0.138077974319458,0.02428715117275715,0.18242515623569489,0.03030792623758316,0.09375004470348358,0.12186259776353836,0.14334487915039062,0.028820758685469627,0.0634675920009613,0.0116934385150671,0.9948965311050415],[0.017133580520749092,0.00678565539419651,0.1119067519903183,0.05564003437757492,0.06949412077665329,0.008386536501348019,0.3256804943084717,0.017767345532774925,0.02451012283563614,0.013441098853945732,0.0247963834553957,0.01540736760944128],[0.09987256675958633,0.03510671481490135,0.01862085796892643,0.02259347401559353,0.04376755654811859,0.045475322753190994,0.016620304435491562,0.056726474314928055,0.3006802499294281,0.007870002649724483,0.014238962903618813,0.059782158583402634],[0.22037476301193237,0.008651784621179104,0.007142617367208004,0.029018813744187355,0.02698281593620777,0.036433979868888855,0.011424731463193893,0.011974976398050785,0.0935884565114975,0.019579574465751648,0.007948831655085087,0.009565788321197033],[0.019380388781428337,0.00919122900813818,0.013338861055672169,0.013080458156764507,0.015057281590998173,0.057125166058540344,0.044689808040857315,0.1341320276260376,0.018738336861133575,0.009870828129351139,0.030577920377254486,0.03827308863401413],[0.01299765519797802,0.006905591115355492,0.014211626723408699,0.0766211748123169,0.010606465861201286,0.012291195802390575,0.00805200356990099,0.022822478786110878,0.023396652191877365,0.007236766163259745,0.0232310201972723,0.008725648745894432],[0.010732803493738174,0.008366882801055908,0.009508139453828335,0.007832776755094528,0.0230303592979908,0.03691914305090904,0.009058453142642975,0.012366306968033314,0.010623447597026825,0.08946600556373596,0.011138319969177246,0.009087708778679371],[0.01936451718211174,0.016402386128902435,0.008759287185966969,0.024910807609558105,0.01801411807537079,0.016655711457133293,0.011142528615891933,0.014467434026300907,0.06150251254439354,0.010209206491708755,0.02547900564968586,0.026756813749670982]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Prev Token Scores\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1d14efb5-a38c-421c-a65c-ff4063980880');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ROME style](https://rome.baulab.info/) patching for causal tracing - we have two runs with two different prompts and different answers, eg \"Steve Jobs founded\" -> \" Apple\" and \"Bill Gates founded\" -> \" Microsoft\". We patch parts of the layer outputs or residual stream from specific tokens and positions and see which patches significantly shift the answer from \" Apple\" to \" Microsoft\""
      ],
      "metadata": {
        "id": "SD4F7zJT08JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = 'Bill Gates founded'\n",
        "response_1 = ' Microsoft'\n",
        "logit_index_1 = model.to_tokens(response_1)[0][-1]\n",
        "(show_tokens(prompt_1))\n",
        "prompt_2 = 'Steve Jobs founded'\n",
        "response_2 = ' Apple'\n",
        "logit_index_2 = model.to_tokens(response_2)[0][-1]\n",
        "show_tokens(prompt_2)\n",
        "\n",
        "model.reset_hooks()\n",
        "uncorrupted_cache = {}\n",
        "model.cache_all(uncorrupted_cache)\n",
        "logits_1 = model(prompt_1)\n",
        "model.reset_hooks()\n",
        "\n",
        "uncorrupted_logits = model(prompt_2)\n",
        "uncorrupted_log_probs = F.log_softmax(uncorrupted_logits, dim=-1)\n",
        "print('Uncorrupted log prob for', response_1, uncorrupted_log_probs[0, -1, logit_index_1].item())\n",
        "print('Uncorrupted log prob for', response_2, uncorrupted_log_probs[0, -1, logit_index_2].item())\n",
        "\n",
        "# Patch the residual stream from the Bill Gates run to the Steve Jobs run\n",
        "# at the Jobs/Gates token, at the start of layer 7\n",
        "layer = 7\n",
        "position = 1\n",
        "\n",
        "def patch_resid_pre(resid_pre, hook):\n",
        "    uncorrupted_resid_pre = uncorrupted_cache[hook.name]\n",
        "    # Move things on the Jobs/Gates token\n",
        "    resid_pre[:, position] = uncorrupted_resid_pre[:, position]\n",
        "    return resid_pre\n",
        "\n",
        "corrupted_logits = model.run_with_hooks(prompt_2, \n",
        "                    fwd_hooks=[(f'blocks.{layer}.hook_resid_pre', patch_resid_pre)])\n",
        "corrupted_log_probs = F.log_softmax(corrupted_logits, dim=-1)\n",
        "print('Corrupted (Residual) log prob for', response_1, corrupted_log_probs[0, -1, logit_index_1].item())\n",
        "print('Corrupted (Residual) log prob for', response_2, corrupted_log_probs[0, -1, logit_index_2].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBWWTocABPxh",
        "outputId": "a6b69133-41f6-4856-f601-7d9afe2d27cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bill| Gates| founded\n",
            "Steve| Jobs| founded\n",
            "Uncorrupted log prob for  Microsoft -2.890328884124756\n",
            "Uncorrupted log prob for  Apple -0.5577379465103149\n",
            "Corrupted (Residual) log prob for  Microsoft -0.5379166603088379\n",
            "Corrupted (Residual) log prob for  Apple -4.753259181976318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also patch the outputs of MLP layers 0 to 7 on the Gates/Jobs token - this time, rather than giving a hook name, we give a Boolean function that filters for the names of those hooks."
      ],
      "metadata": {
        "id": "kuuQzEE5G4Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_start = 0\n",
        "layer_end = 7\n",
        "\n",
        "def patch_mlp_post(mlp_post, hook):\n",
        "    return uncorrupted_cache[hook.name]\n",
        "\n",
        "def filter_middle_mlps(name):\n",
        "    split_name = name.split('.')\n",
        "    if split_name[-1]=='hook_post':\n",
        "        layer = int(split_name[1])\n",
        "        return (layer_start<=layer<layer_end)\n",
        "    return False\n",
        "\n",
        "corrupted_logits = model.run_with_hooks(prompt_2, \n",
        "                    fwd_hooks=[(filter_middle_mlps, patch_mlp_post)])\n",
        "corrupted_log_probs = F.log_softmax(corrupted_logits, dim=-1)\n",
        "print('Corrupted (MLP) log prob for', response_1, corrupted_log_probs[0, -1, logit_index_1].item())\n",
        "print('Corrupted (MLP) log prob for', response_2, corrupted_log_probs[0, -1, logit_index_2].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_meH0ZXGJI6",
        "outputId": "558d5a0a-cca7-4a7f-d759-54227ec1751b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupted (MLP) log prob for  Microsoft -1.2620229721069336\n",
            "Corrupted (MLP) log prob for  Apple -3.783123016357422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking for [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), by feeding in a random sequence of tokens repeated twice and looking for heads that attend from a second copy of a token to the token just after the first copy."
      ],
      "metadata": {
        "id": "sUPPNBTMQx6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 100\n",
        "rand_tokens = torch.randint(1000, 10000, (4, seq_len))\n",
        "rand_tokens_repeat = einops.repeat(rand_tokens, 'batch pos -> batch (2 pos)')\n",
        "if torch.cuda.is_available():\n",
        "    rand_tokens_repeat = rand_tokens_repeat.cuda()\n",
        "\n",
        "induction_scores_array = np.zeros((model.cfg['n_layers'], model.cfg['n_heads']))\n",
        "def calc_induction_score(attn_pattern, hook):\n",
        "    # Pattern has shape [batch, index, query_pos, key_pos]\n",
        "    induction_stripe = attn_pattern.diagonal(1-seq_len, dim1=-2, dim2=-1)\n",
        "    induction_scores = einops.reduce(induction_stripe, 'batch index pos -> index', 'mean')\n",
        "    # Store the scores in a common array\n",
        "    induction_scores_array[hook.layer()] = induction_scores.detach().cpu().numpy()\n",
        "    \n",
        "def filter_attn_hooks(hook_name):\n",
        "    split_name = hook_name.split('.')\n",
        "    return split_name[-1]=='hook_attn'\n",
        "\n",
        "induction_logits = model.run_with_hooks(rand_tokens_repeat, fwd_hooks=[(filter_attn_hooks, calc_induction_score)])\n",
        "px.imshow(induction_scores_array, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "TZIY0UTZHpsq",
        "outputId": "df0ecc3a-213a-4f6b-d575-3ebd94a885ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"12fe86b5-2db8-42a2-880f-58f43aae1eec\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"12fe86b5-2db8-42a2-880f-58f43aae1eec\")) {                    Plotly.newPlot(                        \"12fe86b5-2db8-42a2-880f-58f43aae1eec\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0035020632203668356,2.79007635981543e-05,0.00393505347892642,2.1963347762721241e-07,6.128988752607256e-05,7.796583668095991e-05,0.0033378559164702892,0.000278899387922138,0.0030271820724010468,0.0028836438432335854,0.001943807234056294,0.008677585050463676],[0.0007781091262586415,0.0002877569058910012,0.001572726876474917,0.005780879873782396,0.0019853757694363594,0.003651227103546262,0.005802062805742025,0.00493251346051693,0.006253532133996487,0.007040438242256641,0.0055720689706504345,0.00015388443716801703],[0.0023549713660031557,0.009158891625702381,0.0014135448727756739,0.0007938981871120632,0.007255479693412781,0.0008356596808880568,0.001515646930783987,0.006209324114024639,0.002419707365334034,0.0006650264258496463,0.00017806242976803333,0.005446747876703739],[0.006272189784795046,0.003342176554724574,0.001028201775625348,0.007706040982156992,0.009641511365771294,0.004881192464381456,0.001355965854600072,0.0013304018648341298,0.0034088133834302425,0.005433226004242897,0.005034281872212887,0.0037960531190037727],[0.008858667686581612,0.0071389381773769855,0.00685457605868578,0.003710533957928419,0.009235342033207417,0.005966042168438435,0.004921681713312864,0.0008256871369667351,0.007205544505268335,0.006962674669921398,0.00974726676940918,1.0415913270778177e-10],[0.437594473361969,0.941422700881958,0.008387020789086819,0.003762272885069251,0.005747206974774599,0.9243508577346802,0.004412886220961809,0.009106812067329884,0.009293312206864357,0.01628897711634636,0.012214365415275097,0.008960750885307789],[0.004078133963048458,0.00864782277494669,0.008401511237025261,0.008474895730614662,0.01179595198482275,0.006334308069199324,0.017270976677536964,0.0053151678293943405,0.00630409549921751,0.9236657619476318,0.020803622901439667,0.007359048817306757],[0.006444335915148258,0.17229069769382477,0.8618931770324707,0.00910227932035923,0.009619299322366714,0.009324056096374989,0.03064168244600296,0.0904962569475174,0.008463801816105843,0.009199746884405613,0.9160356521606445,0.03389343246817589],[0.008498496375977993,0.3934627175331116,0.008561232127249241,0.026498284190893173,0.009069789201021194,0.0062775397673249245,0.12258198857307434,0.006798404268920422,0.012915290892124176,0.01852857507765293,0.033495716750621796,0.009375843219459057],[0.22165176272392273,0.17761923372745514,0.09053890407085419,0.006701531819999218,0.06727074086666107,0.01541237160563469,0.42570626735687256,0.013782834634184837,0.03340654447674751,0.4596121609210968,0.008476323448121548,0.03106738068163395],[0.3254685699939728,0.475399374961853,0.026518942788243294,0.1319730579853058,0.04236976057291031,0.007356596179306507,0.28547102212905884,0.37500905990600586,0.031673870980739594,0.00879308395087719,0.1343468725681305,0.22224372625350952],[0.007765882182866335,0.03946180269122124,0.023418713361024857,0.004808669909834862,0.0255284421145916,0.08912672847509384,0.03484933450818062,0.04833716154098511,0.0031270377803593874,0.28877678513526917,0.3370063602924347,0.01173480786383152]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('12fe86b5-2db8-42a2-880f-58f43aae1eec');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation:** We can ablate the top few heads by this metric, and show that performance goes down substantially"
      ],
      "metadata": {
        "id": "YOI6JaIeQ6dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "induction_logits = model(rand_tokens_repeat)\n",
        "induction_log_probs = F.log_softmax(induction_logits, dim=-1)\n",
        "induction_pred_log_probs = torch.gather(induction_log_probs[:, :-1], -1, rand_tokens_repeat[:, 1:, None])[..., 0]\n",
        "print('Original loss on repeated sequence:', induction_pred_log_probs[:, seq_len:].mean())\n",
        "\n",
        "# Mask out the heads with a high induction score\n",
        "attn_head_mask = induction_scores_array>0.8\n",
        "\n",
        "def prune_attn_heads(value, hook):\n",
        "    # Value has shape [batch, pos, index, d_head]\n",
        "    mask = attn_head_mask[hook.layer()]\n",
        "    value[:, :, mask] = 0.\n",
        "    return value\n",
        "\n",
        "def filter_value_hooks(name):\n",
        "    return name.split('.')[-1]=='hook_v'\n",
        "\n",
        "ablated_logits = model.run_with_hooks(rand_tokens_repeat, fwd_hooks=[(filter_value_hooks, prune_attn_heads)])\n",
        "ablated_log_probs = F.log_softmax(ablated_logits, dim=-1)\n",
        "ablated_pred_log_probs = torch.gather(ablated_log_probs[:, :-1], -1, rand_tokens_repeat[:, 1:, None])[..., 0]\n",
        "print('Loss on repeated sequence without induction heads:', ablated_pred_log_probs[:, seq_len:].mean())\n",
        "\n",
        "px.imshow(attn_head_mask, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Mask').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "WWW5xtskKmpb",
        "outputId": "d40f8994-36c3-43a2-9943-96c584e48632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original loss on repeated sequence: tensor(-0.1070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Loss on repeated sequence without induction heads: tensor(-6.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"db712a4c-dcb2-4b89-a48d-910f74ec641e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"db712a4c-dcb2-4b89-a48d-910f74ec641e\")) {                    Plotly.newPlot(                        \"db712a4c-dcb2-4b89-a48d-910f74ec641e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,255,0,0,0,255,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,255,0,0],[0,0,255,0,0,0,0,0,0,0,255,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Mask\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('db712a4c-dcb2-4b89-a48d-910f74ec641e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "EasyTransformer [MLAB].ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fed2f8e0575c4c09ab04877505254b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0135aabf40ce4418a3d013d7c503aabd",
              "IPY_MODEL_12f1a150740e44dd83e7440226b34964",
              "IPY_MODEL_ccf084b77f0e433ea5b78b475e5d775a"
            ],
            "layout": "IPY_MODEL_bee8afe32e0e49f38bdee1f9f77d3bb0"
          }
        },
        "0135aabf40ce4418a3d013d7c503aabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9796fdf1df44293b1446d93a0c1c6ac",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7c5e8946fa46f5af9c0b6f062ab83e",
            "value": "Downloading config.json: 100%"
          }
        },
        "12f1a150740e44dd83e7440226b34964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d856c45fd4144315bec934c1013a4b62",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4db333cefc3f481c97c6efa5e926ac75",
            "value": 665
          }
        },
        "ccf084b77f0e433ea5b78b475e5d775a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50156149fb154147850ef8ce1823911b",
            "placeholder": "​",
            "style": "IPY_MODEL_0f1356120b554199867ec549b3a1cbba",
            "value": " 665/665 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "bee8afe32e0e49f38bdee1f9f77d3bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9796fdf1df44293b1446d93a0c1c6ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7c5e8946fa46f5af9c0b6f062ab83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d856c45fd4144315bec934c1013a4b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db333cefc3f481c97c6efa5e926ac75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50156149fb154147850ef8ce1823911b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1356120b554199867ec549b3a1cbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd42c9d0c33447e2b6f06321abe101ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ae22736f57144ebaac372067c99b68b",
              "IPY_MODEL_e13e0cccded148698031d02df2355a4a",
              "IPY_MODEL_436be5196dbc43a1aad7bdd74f56f868"
            ],
            "layout": "IPY_MODEL_e0c3a6182fdd4344b3a5d74e66cb2b1a"
          }
        },
        "1ae22736f57144ebaac372067c99b68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c4534bf661f44f3b1cc0c5a57c07f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_9735ead6e4bd4a59979df9ec2079444d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "e13e0cccded148698031d02df2355a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a86515d117a4e29a7ce1a2775ae1c92",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad2477d6d5ef499b88f7bb6e3cad6df2",
            "value": 548118077
          }
        },
        "436be5196dbc43a1aad7bdd74f56f868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01c7aead21d4475bd51db7ffebb025b",
            "placeholder": "​",
            "style": "IPY_MODEL_31b24996ae8349cc964419d175595128",
            "value": " 523M/523M [00:12&lt;00:00, 44.9MB/s]"
          }
        },
        "e0c3a6182fdd4344b3a5d74e66cb2b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c4534bf661f44f3b1cc0c5a57c07f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9735ead6e4bd4a59979df9ec2079444d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a86515d117a4e29a7ce1a2775ae1c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2477d6d5ef499b88f7bb6e3cad6df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d01c7aead21d4475bd51db7ffebb025b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b24996ae8349cc964419d175595128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c44ffa8c8d44eab7c5325716314f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_462e514fe94e47d79ad3a8671c6b3979",
              "IPY_MODEL_6459483de48141ba9c91bb4f2e8b86e0",
              "IPY_MODEL_a0a383f04b7643bc9604588dfcddbdfc"
            ],
            "layout": "IPY_MODEL_47b5617aa14a476386b105b1a620df7d"
          }
        },
        "462e514fe94e47d79ad3a8671c6b3979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6d0c3591144ffb8f32aae63f18f145",
            "placeholder": "​",
            "style": "IPY_MODEL_b52471219e714dbf86ff5a864965efe3",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "6459483de48141ba9c91bb4f2e8b86e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b04fadd245604e118896019a3b9d41e5",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66632050940047699760f576ed41c177",
            "value": 1042301
          }
        },
        "a0a383f04b7643bc9604588dfcddbdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42b334bf51441ed8d3f46bae0e71f52",
            "placeholder": "​",
            "style": "IPY_MODEL_d5f084efd2c746d78fcb92f78c12f6d9",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 3.16MB/s]"
          }
        },
        "47b5617aa14a476386b105b1a620df7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d6d0c3591144ffb8f32aae63f18f145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b52471219e714dbf86ff5a864965efe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b04fadd245604e118896019a3b9d41e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66632050940047699760f576ed41c177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e42b334bf51441ed8d3f46bae0e71f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f084efd2c746d78fcb92f78c12f6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "510a3175149644d881520df35983c828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e3437f4854c49be8d3a8a42bdcce037",
              "IPY_MODEL_67d57ad655154583a1e79e5d353a59aa",
              "IPY_MODEL_cddf673e137e42379b602088a1b1c6ba"
            ],
            "layout": "IPY_MODEL_e33a4e424c854690b68b855cc73c4a16"
          }
        },
        "9e3437f4854c49be8d3a8a42bdcce037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47a9fa3705934a54a59ced5334e99760",
            "placeholder": "​",
            "style": "IPY_MODEL_1d5baab536844c4eb60fde2e2f69516b",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "67d57ad655154583a1e79e5d353a59aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdac6ecdc5a54806b2c481328433b9da",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e179471eeb5f47969fe8094b2d27c823",
            "value": 456318
          }
        },
        "cddf673e137e42379b602088a1b1c6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa3c3109d62649cc91b6aa8612c35869",
            "placeholder": "​",
            "style": "IPY_MODEL_88d11b5083f44d4890e34d994592b326",
            "value": " 446k/446k [00:00&lt;00:00, 4.61MB/s]"
          }
        },
        "e33a4e424c854690b68b855cc73c4a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a9fa3705934a54a59ced5334e99760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5baab536844c4eb60fde2e2f69516b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdac6ecdc5a54806b2c481328433b9da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e179471eeb5f47969fe8094b2d27c823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa3c3109d62649cc91b6aa8612c35869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88d11b5083f44d4890e34d994592b326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb7e52c88a54bae9d3f749c4c573441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec6d6ba1dd824b6fa626dc2bc661dda4",
              "IPY_MODEL_0552091708664c3b8667830aea8ecd7c",
              "IPY_MODEL_06c3bebad72749128bef975574f96682"
            ],
            "layout": "IPY_MODEL_ae014cb0d544461babaf5499fe707953"
          }
        },
        "ec6d6ba1dd824b6fa626dc2bc661dda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fd59fafbff24a5590f674c27d04b977",
            "placeholder": "​",
            "style": "IPY_MODEL_81e8c482443e4aad93b3b2975d6717a3",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "0552091708664c3b8667830aea8ecd7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf0d21a46359412f8f96808fef3bb297",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_213f5d2c5aac47c38dc971677719cf8f",
            "value": 1355256
          }
        },
        "06c3bebad72749128bef975574f96682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f5e16ceccc446bab584e3f3cc48ebd5",
            "placeholder": "​",
            "style": "IPY_MODEL_58a97cfdf61448e4a46fd21e7f1bfc1c",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 4.86MB/s]"
          }
        },
        "ae014cb0d544461babaf5499fe707953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd59fafbff24a5590f674c27d04b977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e8c482443e4aad93b3b2975d6717a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf0d21a46359412f8f96808fef3bb297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "213f5d2c5aac47c38dc971677719cf8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f5e16ceccc446bab584e3f3cc48ebd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a97cfdf61448e4a46fd21e7f1bfc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}