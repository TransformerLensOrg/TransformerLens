{"cells":[{"cell_type":"markdown","source":[" <p>This notebook covers the creation of an automatic circuit discovery experiment, and detailed explanation of how to generate automatic circuit pictures for the IOI task</p>"],"metadata":{}},{"cell_type":"markdown","source":[" <h3>Sort out whether we're in a notebook or not</h3>"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import os\n","\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    os.system(\"pip install git+https://github.com/ArthurConmy/Easy-Transformer.git\")\n","\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Jupyter notebook - intended for development only!\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Imports</h2>"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["from typing import List, Tuple, Dict, Union, Optional, Callable, Any\n","from time import ctime\n","import einops\n","import torch\n","import numpy as np\n","from copy import deepcopy\n","from collections import OrderedDict\n","import pickle\n","from subprocess import call\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","if ipython is not None:\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")\n","from easy_transformer import EasyTransformer\n","from easy_transformer.utils_circuit_discovery import (\n","    evaluate_circuit,\n","    patch_all,\n","    direct_path_patching,\n","    logit_diff_io_s,\n","    Circuit,\n","    logit_diff_from_logits,\n","    get_datasets,\n",")\n","from easy_transformer.experiments import (\n","    get_act_hook,\n",")\n","from easy_transformer.ioi_utils import (\n","    show_pp,\n",")\n","from easy_transformer.ioi_dataset import IOIDataset\n","import os\n","\n","file_prefix = \"archive/\" if os.path.exists(\"archive\") else \"\""],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Load in the model</h2>"],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["model_name = \"gpt2\" # @param ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b']\n","model = EasyTransformer.from_pretrained(model_name)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/data2/Users/arthur/Easy-Transformer/easy_transformer/components.py:615: UserWarning: Moved LN1 to the attention block\n","  warnings.warn(\"Moved LN1 to the attention block\")\n"]},{"output_type":"stream","name":"stdout","text":["Moving model to device:  cuda\n","Finished loading pretrained model gpt2 into EasyTransformer!\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Make the dataset</h2>"],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["template = \"Last month it was {month} so this month it is\"\n","all_months = [\n","    \"January\",\n","    \"February\",\n","    \"March\",\n","    \"April\",\n","    \"May\",\n","    \"June\",\n","    \"July\",\n","    \"August\",\n","    \"September\",\n","    \"October\",\n","    \"November\",\n","    \"December\",\n","]\n","sentences = []\n","answers = []\n","wrongs = []\n","batch_size = 12\n","for month_idx in range(batch_size):\n","    cur_sentence = template.format(month=all_months[month_idx])\n","    cur_ans = all_months[(month_idx + 1) % batch_size]\n","    sentences.append(cur_sentence)\n","    answers.append(cur_ans)\n","    wrongs.append(all_months[month_idx])\n","\n","tokens = model.to_tokens(sentences, prepend_bos=True)\n","answers = torch.tensor(model.tokenizer(answers)[\"input_ids\"]).squeeze()\n","wrongs = torch.tensor(model.tokenizer(wrongs)[\"input_ids\"]).squeeze()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h3>Make the positions labels (step 1)</h3>"],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["positions = OrderedDict()\n","ones = torch.ones(size=(batch_size,)).long()\n","positions[\"Last\"] = ones.clone()\n","positions[\"word month\"] = ones.clone() * 2\n","positions[\"month\"] = ones.clone() * 5\n","positions[\"word month 2\"] = ones.clone() * 8\n","positions[\"END\"] = ones.clone() * 10"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h3>Make the baseline dataset (step 2)</h3>"],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["baseline_data = tokens.clone()\n","baseline_data[0] = model.to_tokens(\"This time it is here and last time it was\", prepend_bos=True)\n","baseline_data = einops.repeat(baseline_data[0], \"s -> b s\", b=baseline_data.shape[0])"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h3>Define the metric (step 3)</h3>"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["def day_metric(model, dataset):\n","    logits = model(tokens)\n","    logits_on_correct = logits[torch.arange(batch_size), -1, answers]\n","    logits_on_wrong = logits[torch.arange(batch_size), -1, wrongs]\n","    ans = torch.mean(logits_on_correct - logits_on_wrong)\n","    return ans.item()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" Make the circuit object"],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["h = Circuit(\n","    model,\n","    metric=day_metric,\n","    orig_data=tokens,\n","    new_data=baseline_data,\n","    threshold=0.25,\n","    orig_positions=positions,\n","    new_positions=positions, # in some datasets we might want to patch from different positions; not here\n",")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/data2/Users/arthur/Easy-Transformer/easy_transformer/utils_circuit_discovery.py:560: UserWarning: Caches already exist, overwriting\n","  warnings.warn(\"Caches already exist, overwriting\")\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2> Run path patching! </h2>\n"," <p> Only the first two lines of this cell matter; the rest are for saving images. This cell takes several minutes to run. If you cancel and then call h.show(), you can see intermediate representations of the circuit. </p>"],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["while h.current_node is not None and IN_COLAB: # skip this for faster development in development\n","    h.eval(show_graphics=False, verbose=True)\n","\n","    a = h.show()\n","    # save digraph object\n","    with open(file_prefix + \"hypothesis_tree.dot\", \"w\") as f:\n","        f.write(a.source)\n","\n","    # convert to png\n","    call(\n","        [\n","            \"dot\",\n","            \"-Tpng\",\n","            \"hypothesis_tree.dot\",\n","            \"-o\",\n","            file_prefix + f\"gpt2_hypothesis_tree_{ctime()}.png\",\n","            \"-Gdpi=600\",\n","        ]\n","    )"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h2> Show the circuit </h2>"],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["h.show()"],"outputs":[{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"8pt\" height=\"8pt\"\n viewBox=\"0.00 0.00 8.00 8.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 4)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-4 4,-4 4,4 -4,4\"/>\n</g>\n</svg>\n","text/plain":["<graphviz.graphs.Digraph at 0x7fef30233eb0>"]},"metadata":{},"execution_count":10}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>What about if we run the circuit on the original data ONLY at the nodes in the graph?</h2>"],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["if IN_COLAB:\n","    evaluate_circuit(h) # positive, but very small - we've likely missed some indices. Project: find which ones!"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" <h1>IOI Patching</h1>\n"," <p>The rest of this notebook covers how the direct_path_patching function works internally, using the IOI dataset as an example</p>"],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["N = 50\n","\n","dataset_orig = IOIDataset(\n","    prompt_type=\"mixed\",\n","    N=N, \n","    tokenizer=model.tokenizer,\n","    prepend_bos=False,\n",")\n","\n","# baseline dataset\n","dataset_new = (\n","    dataset_orig.gen_flipped_prompts((\"IO\", \"RAND\"))\n","    .gen_flipped_prompts((\"S\", \"RAND\"))\n","    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",")\n","\n","print(\n","    f\"These dataset objects hold labels to all the relevant words in the sentences: {dataset_orig.word_idx.keys()}\"\n",")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/data2/Users/arthur/Easy-Transformer/easy_transformer/ioi_dataset.py:769: UserWarning: Some groups have less than 5 prompts, they have lengths [1, 4, 3, 2, 4, 2, 3, 2, 2, 3, 4]\n","  warnings.warn(\n","/home/exx/miniconda3/envs/unity_env_arthur/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"output_type":"stream","name":"stdout","text":["These dataset objects hold labels to all the relevant words in the sentences: dict_keys(['IO', 'IO-1', 'IO+1', 'S', 'S-1', 'S+1', 'S2', 'end', 'starts', 'punct'])\n"]},{"output_type":"stream","name":"stderr","text":["/data2/Users/arthur/Easy-Transformer/easy_transformer/ioi_dataset.py:769: UserWarning: Some groups have less than 5 prompts, they have lengths [1, 4, 3, 2, 4, 2, 3, 2, 2, 3, 4]\n","  warnings.warn(\n","/data2/Users/arthur/Easy-Transformer/easy_transformer/ioi_dataset.py:501: UserWarning: S2 index has been computed as the same for S and S2\n","  warnings.warn(\"S2 index has been computed as the same for S and S2\")\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Get the initial logit difference</h2>"],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["model.reset_hooks()\n","logit_diff_initial = logit_diff_io_s(model, dataset_orig)\n","print(f\"Initial logit difference: {logit_diff_initial:.3f}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Initial logit difference: 3.430\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Simplest path patching run</h2>"],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["receivers_to_senders = {\n","    (\"blocks.11.hook_resid_post\", None): [\n","        (\"blocks.9.attn.hook_result\", 9, \"end\"),\n","        (\"blocks.10.attn.hook_result\", 0, \"end\"),\n","        (\"blocks.9.attn.hook_result\", 6, \"end\"),\n","        # (\"blocks.11.attn.hook_result\", 10, \"end\"),\n","    ]\n","}\n","\n","# the IOI paper claims that heads 9.9, 10.0, 9.6 are the most important heads for writing to the residual stream\n","# the above object specifies that we should patch the three edges from these heads to the end state of the residual stream\n","# the string literals will become familiar after learning https://github.com/neelnanda-io/Easy-Transformer/blob/main/EasyTransformer_Demo.ipynb"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["# <h2>Now do the direct path patching</h2>\n","\n","model = direct_path_patching(  # direct path patching returns a model with attached hooks that are relevant for the patch\n","    model=model,\n","    orig_data=dataset_orig.toks.long(),\n","    new_data=dataset_new.toks.long(),  # all hooks that aren't senders will be patched to new_data\n","    receivers_to_senders=receivers_to_senders,\n","    orig_positions=dataset_orig.word_idx,\n","    new_positions=dataset_new.word_idx,\n",")\n","\n","new_logit_diff = logit_diff_io_s(model, dataset_orig)\n","print(f\"New logit difference: {new_logit_diff:.3f}\")  # this should be negative: without these heads, the model can't distinguish between IO and S!"],"outputs":[{"output_type":"stream","name":"stdout","text":["New logit difference: -0.492\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Do direct_path_patching with all possible features</h2>\n"," <p>the hooks ...hook_k_input (and q_input, v_input) allow editing of the Q, K and V inputs to the attention heads\n"," the hooks ...hook_resid_mid allow editing of the input to MLPs\n"," the hook blocks.0.hook_resid_pre allows editing from the embeddings</p>"],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["receivers_to_senders = {\n","    (\"blocks.11.hook_resid_post\", None): [\n","        (\"blocks.9.attn.hook_result\", 9, \"end\"),\n","    ],\n","    (\"blocks.9.attn.hook_k_input\", 9): [\n","        (\"blocks.0.hook_mlp_out\", None, \"IO\"),\n","    ],\n","    (\"blocks.0.hook_resid_mid\", None): [\n","        (\"blocks.0.hook_resid_pre\", None, \"IO\"),\n","    ],\n","}\n","\n","# Now do the direct path patching\n","model = direct_path_patching(\n","    model=model,\n","    orig_data=dataset_orig.toks.long(),\n","    new_data=dataset_new.toks.long(),\n","    receivers_to_senders=receivers_to_senders,\n","    orig_positions=dataset_orig.word_idx,\n","    new_positions=dataset_new.word_idx,\n",")\n","\n","ans = logit_diff_io_s(model, dataset_orig)\n","model.reset_hooks()\n","print(f\"{ans=}\")\n","print(f\"{logit_diff_initial=}, {ans=} (this difference should be small but not 0)\")\n","assert np.abs(logit_diff_initial - ans) > 1e-9, \"!!!\""],"outputs":[{"output_type":"stream","name":"stdout","text":["ans=1.3425135612487793\n","logit_diff_initial=3.430229425430298, ans=1.3425135612487793 (this difference should be small but not 0)\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Automatic circuit discovery</h2>"],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["model.reset_hooks()\n","\n","# construct the position labels\n","orig_positions = OrderedDict()\n","new_positions = OrderedDict()\n","keys = [\"IO\", \"S+1\", \"S\", \"S2\", \"end\"]\n","for key in keys:\n","    orig_positions[key] = dataset_orig.word_idx[key]\n","    new_positions[key] = dataset_new.word_idx[key]\n","\n","# make the tree object\n","h = Circuit(\n","    model,\n","    metric=logit_diff_io_s,\n","    dataset=dataset_orig,  # metric is a function of the hooked model and the dataset, so keep context about dataset_orig inside the dataset object\n","    orig_data=dataset_orig.toks.long(),\n","    new_data=dataset_new.toks.long(),\n","    threshold=0.25,\n","    orig_positions=orig_positions,\n","    new_positions=new_positions,\n","    use_caching=True,\n",")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/data2/Users/arthur/Easy-Transformer/easy_transformer/utils_circuit_discovery.py:560: UserWarning: Caches already exist, overwriting\n","  warnings.warn(\"Caches already exist, overwriting\")\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" <h2>Run circuit discovery</h2>"],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["while h.current_node is not None and IN_COLAB:\n","    h.eval(show_graphics=True, verbose=True)\n","\n","    a = h.show()\n","    # save digraph object\n","    with open(file_prefix + \"hypothesis_tree.dot\", \"w\") as f:\n","        f.write(a.source)\n","\n","    # convert to png\n","    call(\n","        [\n","            \"dot\",\n","            \"-Tpng\",\n","            \"hypothesis_tree.dot\",\n","            \"-o\",\n","            file_prefix + f\"gpt2_hypothesis_tree_{ctime()}.png\",\n","            \"-Gdpi=600\",\n","        ]\n","    )"],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}