{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Othello_GPT.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo notebook porting the weights of the Othello-GPT Model from the excellent [Emergent World Representations](https://arxiv.org/pdf/2210.13382.pdf) paper to my TransformerLens library. Check out the paper's [blog post](https://thegradient.pub/othello/), [paper](https://arxiv.org/pdf/2210.13382.pdf), and [github](https://github.com/likenneth/othello_world/)\n",
    "\n",
    "I think this is a super interesting paper, and I want to better enable work trying to reverse-engineer this model! I'm particularly curious about: \n",
    "* Why non-linear probes work much better than linear probes? \n",
    "    * Is the model internally representing the board in a usable yet non-linear way? \n",
    "    * Is there a representation of simpler concepts (eg diagonal lines in the board, number of black pieces, whether a cell is blank)) that the non-linear probe uses to compute board positions, but where the model internally reasons in this simpler representation?\n",
    "* What's going up with the model editing? \n",
    "    * The paper edits across many layers at once. What's the minimal edit that works? \n",
    "        * Can we edit just before the final layer? \n",
    "        * Can we do a single edit rather than across many layers?\n",
    "    * If we contrast model activations pre and post edit, what changes? \n",
    "        * Which components shift their output and how does this affect the logits? \n",
    "        * Is there significant depth of composition, or does it just affect the output logits?\n",
    "* Can we find any non-trivial circuits in the model?\n",
    "    * Start with [exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), like direct logit attribution, or just looking at head attention patterns, and try to get traction\n",
    "    * Pick a simple sub-task, eg figuring out whether a cell is blank, and try to interpret that.\n",
    "\n",
    "\n",
    "I uploaded pre-converted checkpoints to HuggingFace, which can be automatically downloaded, and there's a code snippet to do this after the setup.\n",
    "\n",
    "If you want to use the author's code, I wrote a script to load and convert checkpoints from the author's code, given below this.\n",
    "\n",
    "To get started, check out the transformer lens [main tutorial](https://neelnanda.io/transformer-lens-demo) and [tutorial on exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), and the author's [excellent Github](https://github.com/likenneth/othello_world/) (Ot**hello world**) for various notebooks demonstrating their code, showing how to load inputs, etc. And check out my [concrete open problems in mechanistic interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj) sequence, especially the algorithmic problems post, for tips on this style of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setup (Skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "import os\n",
    "\n",
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEVELOPMENT_MODE = False\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "\n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    %pip install torchtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7eb230d9-d14a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7eb230d9-d14a\",\n",
       "      Hello,\n",
       "      {\"name\": \"Neel\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x13f282a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2b63dad90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        labels={\"x\": xaxis, \"y\": yaxis},\n",
    "        **kwargs,\n",
    "    ).show(renderer)\n",
    "\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\": xaxis, \"y\": yaxis}, **kwargs).show(\n",
    "        renderer\n",
    "    )\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y, x=x, labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis}, **kwargs\n",
    "    ).show(renderer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Othello GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_AND_CONVERT_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers=8,\n",
    "    d_model=512,\n",
    "    d_head=64,\n",
    "    n_heads=8,\n",
    "    d_mlp=2048,\n",
    "    d_vocab=61,\n",
    "    n_ctx=59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\",\n",
    ")\n",
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566376c7d21e4645958b59f758d2a971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./synthetic_model.pth:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = utils.download_file_from_hf(\n",
    "    \"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\"\n",
    ")\n",
    "# champion_ship_sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to load and convert one of the author's checkpoints to TransformerLens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_transformer_lens_format(in_sd, n_layers=8, n_heads=8):\n",
    "    out_sd = {}\n",
    "    out_sd[\"pos_embed.W_pos\"] = in_sd[\"pos_emb\"].squeeze(0)\n",
    "    out_sd[\"embed.W_E\"] = in_sd[\"tok_emb.weight\"]\n",
    "\n",
    "    out_sd[\"ln_final.w\"] = in_sd[\"ln_f.weight\"]\n",
    "    out_sd[\"ln_final.b\"] = in_sd[\"ln_f.bias\"]\n",
    "    out_sd[\"unembed.W_U\"] = in_sd[\"head.weight\"].T\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        out_sd[f\"blocks.{layer}.ln1.w\"] = in_sd[f\"blocks.{layer}.ln1.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln1.b\"] = in_sd[f\"blocks.{layer}.ln1.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.ln2.w\"] = in_sd[f\"blocks.{layer}.ln2.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln2.b\"] = in_sd[f\"blocks.{layer}.ln2.bias\"]\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.attn.W_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.query.weight\"],\n",
    "            \"(head d_head) d_model -> head d_model d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_Q\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.query.bias\"],\n",
    "            \"(head d_head) -> head d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_K\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.key.weight\"],\n",
    "            \"(head d_head) d_model -> head d_model d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_K\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.key.bias\"],\n",
    "            \"(head d_head) -> head d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_V\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.value.weight\"],\n",
    "            \"(head d_head) d_model -> head d_model d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_V\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.value.bias\"],\n",
    "            \"(head d_head) -> head d_head\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.W_O\"] = einops.rearrange(\n",
    "            in_sd[f\"blocks.{layer}.attn.proj.weight\"],\n",
    "            \"d_model (head d_head) -> head d_head d_model\",\n",
    "            head=n_heads,\n",
    "        )\n",
    "        out_sd[f\"blocks.{layer}.attn.b_O\"] = in_sd[f\"blocks.{layer}.attn.proj.bias\"]\n",
    "\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_in\"] = in_sd[f\"blocks.{layer}.mlp.0.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_in\"] = in_sd[f\"blocks.{layer}.mlp.0.weight\"].T\n",
    "        out_sd[f\"blocks.{layer}.mlp.b_out\"] = in_sd[f\"blocks.{layer}.mlp.2.bias\"]\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_out\"] = in_sd[f\"blocks.{layer}.mlp.2.weight\"].T\n",
    "\n",
    "    return out_sd\n",
    "\n",
    "\n",
    "if LOAD_AND_CONVERT_CHECKPOINT:\n",
    "    synthetic_checkpoint = torch.load(\"/workspace/othello_world/gpt_synthetic.ckpt\")\n",
    "    for name, param in synthetic_checkpoint.items():\n",
    "        if name.startswith(\"blocks.0\") or not name.startswith(\"blocks\"):\n",
    "            print(name, param.shape)\n",
    "\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=8,\n",
    "        d_model=512,\n",
    "        d_head=64,\n",
    "        n_heads=8,\n",
    "        d_mlp=2048,\n",
    "        d_vocab=61,\n",
    "        n_ctx=59,\n",
    "        act_fn=\"gelu\",\n",
    "        normalization_type=\"LNPre\",\n",
    "    )\n",
    "    model = HookedTransformer(cfg)\n",
    "\n",
    "    model.load_and_process_state_dict(\n",
    "        convert_to_transformer_lens_format(synthetic_checkpoint)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing code for the synthetic checkpoint giving the correct outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 41, 40, 34, 40, 41,  3, 11, 21, 43, 40, 21, 28, 50, 33, 50, 33,  5,\n",
       "         33,  5, 52, 46, 14, 46, 14, 47, 38, 57, 36, 50, 38, 15, 28, 26, 28, 59,\n",
       "         50, 28, 14, 28, 28, 28, 28, 45, 28, 35, 15, 14, 30, 59, 49, 59, 15, 15,\n",
       "         14, 15,  8,  7,  8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example input\n",
    "sample_input = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            20,\n",
    "            19,\n",
    "            18,\n",
    "            10,\n",
    "            2,\n",
    "            1,\n",
    "            27,\n",
    "            3,\n",
    "            41,\n",
    "            42,\n",
    "            34,\n",
    "            12,\n",
    "            4,\n",
    "            40,\n",
    "            11,\n",
    "            29,\n",
    "            43,\n",
    "            13,\n",
    "            48,\n",
    "            56,\n",
    "            33,\n",
    "            39,\n",
    "            22,\n",
    "            44,\n",
    "            24,\n",
    "            5,\n",
    "            46,\n",
    "            6,\n",
    "            32,\n",
    "            36,\n",
    "            51,\n",
    "            58,\n",
    "            52,\n",
    "            60,\n",
    "            21,\n",
    "            53,\n",
    "            26,\n",
    "            31,\n",
    "            37,\n",
    "            9,\n",
    "            25,\n",
    "            38,\n",
    "            23,\n",
    "            50,\n",
    "            45,\n",
    "            17,\n",
    "            47,\n",
    "            28,\n",
    "            35,\n",
    "            30,\n",
    "            54,\n",
    "            16,\n",
    "            59,\n",
    "            49,\n",
    "            57,\n",
    "            14,\n",
    "            15,\n",
    "            55,\n",
    "            7,\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "# The argmax of the output (ie the most likely next move from each position)\n",
    "sample_output = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            21,\n",
    "            41,\n",
    "            40,\n",
    "            34,\n",
    "            40,\n",
    "            41,\n",
    "            3,\n",
    "            11,\n",
    "            21,\n",
    "            43,\n",
    "            40,\n",
    "            21,\n",
    "            28,\n",
    "            50,\n",
    "            33,\n",
    "            50,\n",
    "            33,\n",
    "            5,\n",
    "            33,\n",
    "            5,\n",
    "            52,\n",
    "            46,\n",
    "            14,\n",
    "            46,\n",
    "            14,\n",
    "            47,\n",
    "            38,\n",
    "            57,\n",
    "            36,\n",
    "            50,\n",
    "            38,\n",
    "            15,\n",
    "            28,\n",
    "            26,\n",
    "            28,\n",
    "            59,\n",
    "            50,\n",
    "            28,\n",
    "            14,\n",
    "            28,\n",
    "            28,\n",
    "            28,\n",
    "            28,\n",
    "            45,\n",
    "            28,\n",
    "            35,\n",
    "            15,\n",
    "            14,\n",
    "            30,\n",
    "            59,\n",
    "            49,\n",
    "            59,\n",
    "            15,\n",
    "            15,\n",
    "            14,\n",
    "            15,\n",
    "            8,\n",
    "            7,\n",
    "            8,\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "model(sample_input).argmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
