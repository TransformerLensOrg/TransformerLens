{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a template accompanying the transformer tutorial here: https://www.youtube.com/watch?v=1tKYVRdx_P0 and the complete solution here: https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb\n",
    "\n",
    "Instruction: Start at section \"Actual Code!\". Each of the Transformer's layers is given, with code to define its weights, but an empty forward function. Write code that uses the weights and biases defined in the init to calculate the output, and compare it to the library's reference GPT-2 with the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "  %pip install git+https://github.com/neelnanda-io/Easy-Transformer.git\n",
    "  %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "  %pip install fancy_einsum\n",
    "  %pip install einops\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook - intended for development only!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from easy_transformer import EasyTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Inputs & Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the point of a transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformers exist to model text!**\n",
    "\n",
    "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distn over tokens. And you can repeatedly sample from this to generate text!\n",
    "\n",
    "### How is the model trained?\n",
    "\n",
    "You give it a bunch of text, and train it to predict the next token.\n",
    "\n",
    "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, ie it produces 100 predictions. This is kinda weird but it's much easier to make one that does this. And it also makes training more efficient, because you can 100 bits of feedback rather than just one.\n",
    "\n",
    "#### Objection: Isn't this trivial for the first 99?\n",
    "\n",
    "No! We make the transformer have *causal attention*. The core thing is that it can only move information forwards in the sequence. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. (Jargon: *autoregressive*)\n",
    "\n",
    "### Key takeaway:\n",
    "\n",
    "Transformers are *sequence modelling engines*. It does the same processing in parallel at each sequence position, can move information between positions with attention, and conceptually can take a sequence of arbitrary length (not actually true, see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens - Transformer Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core point: Input is language (ie a sequence of characters, strings, etc)\n",
    "\n",
    "### How do we convert language to vectors?\n",
    "\n",
    "ML models take in vectors, not weird shit like language - how do we convert?\n",
    "\n",
    "#### Idea: integers to vectors\n",
    "\n",
    "We basically make a lookup table. Called an embedding. \n",
    "\n",
    "Jargon: **One-hot encoding** We map eg numbers from 1 to 100, to a 100-dim vector, with a 1 in the kth position, 0 everywhere else. Key intuition is that one-hot encodings let you think about each integer independently - useful when integers = labels. \n",
    "\n",
    "Dimensions = things that vary independently. Each input has its own dimension, so each input can be thought of independently, we don't bake in any relation.\n",
    "\n",
    "Lookup tables <=> Multiply a fixed matrix by the one-hot encoded vector. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: Language to sequence of integers\n",
    "\n",
    "Core idea: We need a model that can deal with arbitrary text. We want to convert this into integers, *and* we want these integers to be in a bounded range. \n",
    "\n",
    "**Idea:** Form a vocabulary!\n",
    "\n",
    "**Idea 1:** Get a dictionary! \n",
    "\n",
    "**Problem:** It can't cope with arbitrary text (eg URLs, punctuation, etc) Can't cope with mispellings.\n",
    "\n",
    "**Idea 2:** Vocab = 256 ASCII characters. Fixed vocab size, can do arbitrary text, etc.\n",
    "\n",
    "**Problem:** Loses structure of language - some sequences of characters are more meaningful than others.\n",
    "\n",
    "Eg \"language\" is a lot more meaningful than \"hjksdfiu\" - we want the first to be a single token, second to not be. It's a more efficient use of our vocab.\n",
    "\n",
    "#### What Actually Happens?\n",
    "\n",
    "This super cursed thing called Byte-Pair Encodings\n",
    "\n",
    "Ġ ~ means begins with a space, tokens with a leading space vs not are different.\n",
    "\n",
    "We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Eg \" t\" is the most common pair, so it's our next token! Repeat 50000 times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n:n[1])\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets to weird esoteric shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Revolution', 50237),\n",
       " ('Ġsnipers', 50238),\n",
       " ('Ġreverted', 50239),\n",
       " ('Ġconglomerate', 50240),\n",
       " ('Terry', 50241),\n",
       " ('794', 50242),\n",
       " ('Ġharsher', 50243),\n",
       " ('Ġdesolate', 50244),\n",
       " ('ĠHitman', 50245),\n",
       " ('Commission', 50246),\n",
       " ('Ġ(/', 50247),\n",
       " ('âĢ¦.\"', 50248),\n",
       " ('Compar', 50249),\n",
       " ('Ġamplification', 50250),\n",
       " ('ominated', 50251),\n",
       " ('Ġregress', 50252),\n",
       " ('ĠCollider', 50253),\n",
       " ('Ġinformants', 50254),\n",
       " ('Ġgazed', 50255),\n",
       " ('<|endoftext|>', 50256)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocab[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `to_tokens` method to convert text to numbers\n",
    "\n",
    "Prepends with a special token to give attention a resting position, disable with `prepend_bos=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,\n",
      "          6067,     0]])\n",
      "tensor([[15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,  6067,\n",
      "             0]])\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\"))\n",
    "print(reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\", prepend_bos=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rant: Tokenization is a Headache\n",
    "\n",
    "Whether a word begins with a capital or space matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arithmetic is a total mess: Length is inconsistent, common numbers bundle together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '568',\n",
       " '73',\n",
       " '+',\n",
       " '318',\n",
       " '46',\n",
       " '23',\n",
       " '=',\n",
       " '123',\n",
       " '45',\n",
       " '67',\n",
       " '89',\n",
       " '-',\n",
       " '1',\n",
       " '000000',\n",
       " '000']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "\n",
    "* We learn a dictionary of vocab of tokens (sub-words).\n",
    "\n",
    "* We (approx) losslessly convert language to integers via tokenizing it.\n",
    "\n",
    "* We convert integers to vectors via a lookup table.\n",
    "\n",
    "* Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits - Transformer Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Probability distribution over next tokens. (for every *prefix* of the sequence - given n tokens, we make n next token predictions)\n",
    "\n",
    "**Problem:** How to convert a vector to a probability distribution? \n",
    "\n",
    "**Answer:** Use a softmax ($x_i \\to \\frac{e^{x_i}}{\\sum e^{x_j}}$), exponential makes everything positive, normalization makes it add to one.\n",
    "\n",
    "So the model outputs a tensor of logits, one vector of size $d_{vocab}$ for each input token.\n",
    "\n",
    "We can use this to generate things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Convert text to tokens\n",
    "\n",
    "Shape = batch x position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]])\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Map tokens to logits\n",
    "\n",
    "(run_with_cache means cache all intermediate activations, not important right now)\n",
    "\n",
    "shape = batch x position x d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens.cuda()\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Convert the logits to a distribution with a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "print(log_probs.shape)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus step:** What is the most likely next token at each position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(reference_gpt2.to_str_tokens(reference_text), reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Map distribution to a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(314, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Add this to the end of the input, re-run\n",
    "\n",
    "(More efficient ways to do this, but whatever, doesn't matter conceptually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0,   314]], device='cuda:0')\n",
      "torch.Size([1, 36])\n",
      "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n",
      "torch.Size([1, 36, 50257])\n",
      "tensor(716, device='cuda:0')\n",
      " am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "next_tokens = torch.cat([tokens, torch.tensor(next_token, device='cuda', dtype=torch.int64)[None, None]], dim=-1)\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "print(\"New Input:\", next_tokens)\n",
    "print(next_tokens.shape)\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
    "\n",
    "print(new_logits.shape)\n",
    "print(new_logits[-1, -1].argmax(-1))\n",
    "\n",
    "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways:\n",
    "\n",
    "* Takes in language, predicts next token (for *each* token in a causal way)\n",
    "* We convert language to a sequence of integers with a tokenizer.\n",
    "* We convert integers to vectors with a lookup table.\n",
    "\n",
    "* Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
    "\n",
    "* We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
    "\n",
    "* Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](transformer_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-Level architecture:\n",
    "\n",
    "Go watch my [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!\n",
    "\n",
    "(Diagram is bottom to top)\n",
    "\n",
    "* Input tokens, integers\n",
    "* Embedding is a lookup table mapping tokens to vectors\n",
    "    * Lives in the *residual stream*\n",
    "* Residual stream - the sum of all previous outputs of layers of the model, is the input to each new layer.\n",
    "    * *Really* fundamental. It's the central object of the transformer.\n",
    "        * It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n",
    "* Then we have a series of $n_{layers}$ transformer blocks\n",
    "    * Confusing jargon - a block contains an attention layer *and* an MLP layer, but we say a transformer has k layers if it has k blocks (ie 2k total layers).\n",
    "* First we have attention. This moves information from prior positions in the sequence to the current token. \n",
    "    * We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only, so later tokens get more room to look back.\n",
    "        * We look backwards so we can predict the next token without cheating.\n",
    "    * Only bit of a transformer that moves information between positions.\n",
    "    * Made up of $n_heads$ heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination.\n",
    "        * The heads act independently and additively, we just add their outputs together, and back to the stream\n",
    "    * Each head:\n",
    "        * Produces an attention pattern for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
    "            * Do this for each pair of tokens\n",
    "            * Copy information in the same way from each source token.\n",
    "                * What information we copy *does* depend on the source token's *residual stream*. This does not necessarily mean the info of what text token is at the source token's position\n",
    "                * Copy = apply a linear map.\n",
    "        * Fundamental point: Figuring out *which* source tokens to copy info from is a separate circuit from figuring out *how* to copy that information.\n",
    "        * Internal head dimension of $d_{head} = \\frac{d_{model}}{n_{heads}}\n",
    "* MLP Layers - standard neural network. Single hidden layer, linear map -> GELU activation -> linear map\n",
    "    * Exact activation not conceptually important.\n",
    "    * Middle dimension normally $d_{mlp} = 4 \\times d_{model}$\n",
    "        * Exactly why the ratios are what they are isn't super important - doesn't matter that much, people basically cargo-cult GPT did.\n",
    "    * Intuition - once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc.\n",
    "        * Big open problem in transformer mechanistic interpretability is what is going on inside MLPs?! See [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.\n",
    "        * Underlying intuition - linear map -> non-linearity -> linear map is the most powerful force in the universe and can approximate arbitrary functions. Idk man it just works\n",
    "* Finally, we unembed!\n",
    "    * Apply a linear map, going from final residual stream to a vector of logits - this is the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus things - less conceptually important but key technical details\n",
    "* LayerNorm\n",
    "    * Simple normalization function applied at the start of each layer - MLP, Attn and Unembed\n",
    "    * Converts each input vector (independently in parallel for each batch x position residual stream vector) to have mean zero and variance 1.\n",
    "    * Then applies an elementwise scaling and translation\n",
    "    * Cool maths tangent: The scale & translate is just a linear map. LayerNorm is only applied immediately before another linear map. Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.\n",
    "        * `fold_ln=True` flag in `from_pretrained` does this for you.\n",
    "    * LayerNorm is super fucking annoying, because the scale part is not linear, so you can't think about different bits of the input independently. But it's *almost* linear - if you're changing a small part of the input it's linear, but if you're changing enough to alter the norm substantially it's not linear :(\n",
    "* Positional Information\n",
    "    * This is totally fucked and messy, sorry!\n",
    "    * **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default\n",
    "        * This is dumb because nearby tokens are more relevant.\n",
    "    * There's a lot of dumb hacks for this.\n",
    "    * We'll focus on **learned, absolute positional embeddings**. This means we learn a lookup table mapping the index of the position of each token to a residual stream vector, and add this to the embed.\n",
    "        * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)\n",
    "        * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print All Activation Shapes of Reference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key:\n",
    "```\n",
    "batch = 1\n",
    "position = 35\n",
    "d_model = 768\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "d_mlp = 3072 (4 * d_model)\n",
    "d_head = 64 (d_model / n_heads)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 35, 768])\n",
      "hook_pos_embed torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_attn torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
      "ln_final.hook_scale torch.Size([1, 35, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(activation_name, activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print All Parameters Shapes of Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "blocks.0.ln1.w torch.Size([768])\n",
      "blocks.0.ln1.b torch.Size([768])\n",
      "blocks.0.ln2.w torch.Size([768])\n",
      "blocks.0.ln2.b torch.Size([768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "ln_final.w torch.Size([768])\n",
      "ln_final.b torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyTransformerConfig(n_layers=12, d_model=768, n_ctx=1024, d_head=64, model_name='gpt2-small', n_heads=12, d_mlp=3072, act_fn='gelu_new', d_vocab=50257, eps=1e-05, use_attn_result=False, use_attn_scale=True, use_local_attn=False, model_family='gpt2', checkpoint=None, tokenizer_name='gpt2', window_size=None, attn_types=None, init_mode='gpt2', normalization_type='LN', device='cuda', attention_dir='causal', attn_only=False, seed=42, initializer_range=0.02886751345948129, init_weights=False, scale_attn_by_inverse_layer_idx=False, positional_embedding_type='standard', final_rms=False, d_vocab_out=50257, parallel_attn_mlp=False, rotary_dim=64, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a stripped down config for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are great, write lightweight ones to use as you go!\n",
    "\n",
    "**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    random_input = torch.randn(shape).cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    random_input = torch.randint(100, 1000, shape).cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str): \n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    reference_output = gpt2_layer(reference_input)\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make mean 0\n",
    "Normalize to have variance 1\n",
    "Scale with learned weights\n",
    "Translate with learned bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "    \n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically a lookup table from tokens to residual stream vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "Embeddings: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "           2.8267e-02,  5.4490e-02],\n",
       "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "           1.0172e-02, -1.5573e-04],\n",
       "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "           1.9325e-02, -2.1424e-02],\n",
       "         ...,\n",
       "         [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
       "          -2.3037e-03, -4.3189e-03],\n",
       "         [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
       "          -4.8160e-03, -9.2252e-04],\n",
       "         [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
       "          -3.2512e-03, -9.6509e-04]]], device='cuda:0',\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (incl current token)\n",
    "    * Linear map from input -> query, key shape [batch, position, head_index, d_head]\n",
    "    * Dot product every *pair* of queries and keys to get attn_scores [batch, head_index, query_pos, key_pos] (query = dest, key = source)\n",
    "    * Scale and mask attn_scores to make it lower triangular, ie causal\n",
    "    * softmax row-wise, to get a probability distribution along each the key_pos dimension - this is our attention pattern!\n",
    "* **Step 2:** Move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
    "    * Linear map from input -> value [batch, key_pos, head_index, d_head]\n",
    "    * Mix along the key_pos with attn pattern to get z, a mixed value [batch, query_pos, head_index, d_head]\n",
    "    * Map to output, [batch, position, d_model] (position = query_pos, we've summed over all heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>/*! For license information please see loader.js.LICENSE.txt */\n",
       "var loader;loader=(()=>{var __webpack_modules__={907:module=>{\"use strict\";module.exports=function iota(n){for(var result=new Array(n),i=0;i<n;++i)result[i]=i;return result}},738:module=>{function isBuffer(obj){return!!obj.constructor&&\"function\"==typeof obj.constructor.isBuffer&&obj.constructor.isBuffer(obj)}module.exports=function(obj){return null!=obj&&(isBuffer(obj)||function isSlowBuffer(obj){return\"function\"==typeof obj.readFloatLE&&\"function\"==typeof obj.slice&&isBuffer(obj.slice(0,0))}(obj)||!!obj._isBuffer)}},861:(module,__unused_webpack_exports,__webpack_require__)=>{var iota=__webpack_require__(907),isBuffer=__webpack_require__(738),hasTypedArrays=\"undefined\"!=typeof Float64Array;function compare1st(a,b){return a[0]-b[0]}function order(){var i,stride=this.stride,terms=new Array(stride.length);for(i=0;i<terms.length;++i)terms[i]=[Math.abs(stride[i]),i];terms.sort(compare1st);var result=new Array(terms.length);for(i=0;i<result.length;++i)result[i]=terms[i][1];return result}function compileConstructor(dtype,dimension){var className=[\"View\",dimension,\"d\",dtype].join(\"\");dimension<0&&(className=\"View_Nil\"+dtype);var useGetters=\"generic\"===dtype;if(-1===dimension){var code=\"function \"+className+\"(a){this.data=a;};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return -1};proto.size=0;proto.dimension=-1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function(){return new \"+className+\"(this.data);};proto.get=proto.set=function(){};proto.pick=function(){return null};return function construct_\"+className+\"(a){return new \"+className+\"(a);}\";return new Function(code)()}if(0===dimension){code=\"function \"+className+\"(a,d) {this.data = a;this.offset = d};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return this.offset};proto.dimension=0;proto.size=1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function \"+className+\"_copy() {return new \"+className+\"(this.data,this.offset)};proto.pick=function \"+className+\"_pick(){return TrivialArray(this.data);};proto.valueOf=proto.get=function \"+className+\"_get(){return \"+(useGetters?\"this.data.get(this.offset)\":\"this.data[this.offset]\")+\"};proto.set=function \"+className+\"_set(v){return \"+(useGetters?\"this.data.set(this.offset,v)\":\"this.data[this.offset]=v\")+\"};return function construct_\"+className+\"(a,b,c,d){return new \"+className+\"(a,d)}\";return new Function(\"TrivialArray\",code)(CACHED_CONSTRUCTORS[dtype][0])}code=[\"'use strict'\"];var indices=iota(dimension),args=indices.map((function(i){return\"i\"+i})),index_str=\"this.offset+\"+indices.map((function(i){return\"this.stride[\"+i+\"]*i\"+i})).join(\"+\"),shapeArg=indices.map((function(i){return\"b\"+i})).join(\",\"),strideArg=indices.map((function(i){return\"c\"+i})).join(\",\");code.push(\"function \"+className+\"(a,\"+shapeArg+\",\"+strideArg+\",d){this.data=a\",\"this.shape=[\"+shapeArg+\"]\",\"this.stride=[\"+strideArg+\"]\",\"this.offset=d|0}\",\"var proto=\"+className+\".prototype\",\"proto.dtype='\"+dtype+\"'\",\"proto.dimension=\"+dimension),code.push(\"Object.defineProperty(proto,'size',{get:function \"+className+\"_size(){return \"+indices.map((function(i){return\"this.shape[\"+i+\"]\"})).join(\"*\"),\"}})\"),1===dimension?code.push(\"proto.order=[0]\"):(code.push(\"Object.defineProperty(proto,'order',{get:\"),dimension<4?(code.push(\"function \"+className+\"_order(){\"),2===dimension?code.push(\"return (Math.abs(this.stride[0])>Math.abs(this.stride[1]))?[1,0]:[0,1]}})\"):3===dimension&&code.push(\"var s0=Math.abs(this.stride[0]),s1=Math.abs(this.stride[1]),s2=Math.abs(this.stride[2]);if(s0>s1){if(s1>s2){return [2,1,0];}else if(s0>s2){return [1,2,0];}else{return [1,0,2];}}else if(s0>s2){return [2,0,1];}else if(s2>s1){return [0,1,2];}else{return [0,2,1];}}})\")):code.push(\"ORDER})\")),code.push(\"proto.set=function \"+className+\"_set(\"+args.join(\",\")+\",v){\"),useGetters?code.push(\"return this.data.set(\"+index_str+\",v)}\"):code.push(\"return this.data[\"+index_str+\"]=v}\"),code.push(\"proto.get=function \"+className+\"_get(\"+args.join(\",\")+\"){\"),useGetters?code.push(\"return this.data.get(\"+index_str+\")}\"):code.push(\"return this.data[\"+index_str+\"]}\"),code.push(\"proto.index=function \"+className+\"_index(\",args.join(),\"){return \"+index_str+\"}\"),code.push(\"proto.hi=function \"+className+\"_hi(\"+args.join(\",\")+\"){return new \"+className+\"(this.data,\"+indices.map((function(i){return[\"(typeof i\",i,\"!=='number'||i\",i,\"<0)?this.shape[\",i,\"]:i\",i,\"|0\"].join(\"\")})).join(\",\")+\",\"+indices.map((function(i){return\"this.stride[\"+i+\"]\"})).join(\",\")+\",this.offset)}\");var a_vars=indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})),c_vars=indices.map((function(i){return\"c\"+i+\"=this.stride[\"+i+\"]\"}));code.push(\"proto.lo=function \"+className+\"_lo(\"+args.join(\",\")+\"){var b=this.offset,d=0,\"+a_vars.join(\",\")+\",\"+c_vars.join(\",\"));for(var i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){d=i\"+i+\"|0;b+=c\"+i+\"*d;a\"+i+\"-=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"c\"+i})).join(\",\")+\",b)}\"),code.push(\"proto.step=function \"+className+\"_step(\"+args.join(\",\")+\"){var \"+indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i+\"=this.stride[\"+i+\"]\"})).join(\",\")+\",c=this.offset,d=0,ceil=Math.ceil\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'){d=i\"+i+\"|0;if(d<0){c+=b\"+i+\"*(a\"+i+\"-1);a\"+i+\"=ceil(-a\"+i+\"/d)}else{a\"+i+\"=ceil(a\"+i+\"/d)}b\"+i+\"*=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i})).join(\",\")+\",c)}\");var tShape=new Array(dimension),tStride=new Array(dimension);for(i=0;i<dimension;++i)tShape[i]=\"a[i\"+i+\"]\",tStride[i]=\"b[i\"+i+\"]\";code.push(\"proto.transpose=function \"+className+\"_transpose(\"+args+\"){\"+args.map((function(n,idx){return n+\"=(\"+n+\"===undefined?\"+idx+\":\"+n+\"|0)\"})).join(\";\"),\"var a=this.shape,b=this.stride;return new \"+className+\"(this.data,\"+tShape.join(\",\")+\",\"+tStride.join(\",\")+\",this.offset)}\"),code.push(\"proto.pick=function \"+className+\"_pick(\"+args+\"){var a=[],b=[],c=this.offset\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){c=(c+this.stride[\"+i+\"]*i\"+i+\")|0}else{a.push(this.shape[\"+i+\"]);b.push(this.stride[\"+i+\"])}\");return code.push(\"var ctor=CTOR_LIST[a.length+1];return ctor(this.data,a,b,c)}\"),code.push(\"return function construct_\"+className+\"(data,shape,stride,offset){return new \"+className+\"(data,\"+indices.map((function(i){return\"shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"stride[\"+i+\"]\"})).join(\",\")+\",offset)}\"),new Function(\"CTOR_LIST\",\"ORDER\",code.join(\"\\n\"))(CACHED_CONSTRUCTORS[dtype],order)}var CACHED_CONSTRUCTORS={float32:[],float64:[],int8:[],int16:[],int32:[],uint8:[],uint16:[],uint32:[],array:[],uint8_clamped:[],bigint64:[],biguint64:[],buffer:[],generic:[]};module.exports=function wrappedNDArrayCtor(data,shape,stride,offset){if(void 0===data)return(0,CACHED_CONSTRUCTORS.array[0])([]);\"number\"==typeof data&&(data=[data]),void 0===shape&&(shape=[data.length]);var d=shape.length;if(void 0===stride){stride=new Array(d);for(var i=d-1,sz=1;i>=0;--i)stride[i]=sz,sz*=shape[i]}if(void 0===offset){offset=0;for(i=0;i<d;++i)stride[i]<0&&(offset-=(shape[i]-1)*stride[i])}for(var dtype=function arrayDType(data){if(isBuffer(data))return\"buffer\";if(hasTypedArrays)switch(Object.prototype.toString.call(data)){case\"[object Float64Array]\":return\"float64\";case\"[object Float32Array]\":return\"float32\";case\"[object Int8Array]\":return\"int8\";case\"[object Int16Array]\":return\"int16\";case\"[object Int32Array]\":return\"int32\";case\"[object Uint8Array]\":return\"uint8\";case\"[object Uint16Array]\":return\"uint16\";case\"[object Uint32Array]\":return\"uint32\";case\"[object Uint8ClampedArray]\":return\"uint8_clamped\";case\"[object BigInt64Array]\":return\"bigint64\";case\"[object BigUint64Array]\":return\"biguint64\"}return Array.isArray(data)?\"array\":\"generic\"}(data),ctor_list=CACHED_CONSTRUCTORS[dtype];ctor_list.length<=d+1;)ctor_list.push(compileConstructor(dtype,ctor_list.length-1));return(0,ctor_list[d+1])(data,shape,stride,offset)}},829:(__unused_webpack_module,exports)=>{\"use strict\";function _defineProperties(a,b){for(var c,d=0;d<b.length;d++)(c=b[d]).enumerable=c.enumerable||!1,c.configurable=!0,\"value\"in c&&(c.writable=!0),Object.defineProperty(a,c.key,c)}exports.g=function fromArrayBuffer(a){if(!a instanceof ArrayBuffer)throw new Error(\"Argument must be an ArrayBuffer.\");var b=new DataViewReader(a),c=b.readUint8(),d=b.readAndASCIIDecodeBytes(5);if(147!=c||\"NUMPY\"!=d)throw new Error('unknown file type: \"'.concat(c).concat(d,'\"'));var e,f=b.readUint8(),h=(b.readUint8(),10+(e=1>=f?b.readUint16(!0):b.readUint32(!0)));0!=h%16&&console.warn(\"NPY file header is incorrectly padded. (\".concat(h,\" is not evenly divisible by 16.)\"));var j=function parseHeaderStr(a){var b=a.toLowerCase().replace(\"(\",\"[\").replace(\"),\",\"]\").replace(\"[,\",\"[1,]\").replace(\",]\",\",1]\").replace(/'/g,'\"');return JSON.parse(b)}(b.readAndASCIIDecodeBytes(e));if(j.fortran_order)throw new Error(\"NPY file is written in Fortran byte order, support for this byte order is not yet implemented.\");return{data:new(typedArrayConstructorForDescription(j.descr))(a,b.offset),shape:j.shape}};var DataViewReader=function(){function a(b){(function _classCallCheck(a,b){if(!(a instanceof b))throw new TypeError(\"Cannot call a class as a function\")})(this,a),b instanceof DataView?this.dataView=b:b instanceof ArrayBuffer&&(this.dataView=new DataView(b)),this.offset=0}return function _createClass(a,b,c){return b&&_defineProperties(a.prototype,b),c&&_defineProperties(a,c),a}(a,[{key:\"readBytes\",value:function c(a){var b=new DataView(this.dataView.buffer,this.offset,a);return this.offset+=a,b}},{key:\"readAndASCIIDecodeBytes\",value:function c(a){var b=new Uint8Array(this.dataView.buffer,this.offset,a);return this.offset+=a,this._decodeASCIIByteArray(b)}},{key:\"readUint8\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint8(this.offset,a);return this.offset+=Uint8Array.BYTES_PER_ELEMENT,b}},{key:\"readUint16\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint16(this.offset,a);return this.offset+=Uint16Array.BYTES_PER_ELEMENT,b}},{key:\"readUint32\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint32(this.offset,a);return this.offset+=Uint32Array.BYTES_PER_ELEMENT,b}},{key:\"_decodeASCIIByteArray\",value:function k(a){var b=String.fromCharCode,c=[],d=!0,e=!1,f=void 0;try{for(var g,h=a[Symbol.iterator]();!(d=(g=h.next()).done);d=!0){var j=b(g.value);c.push(j)}}catch(a){e=!0,f=a}finally{try{d||null==h.return||h.return()}finally{if(e)throw f}}return c.join(\"\")}}]),a}();function typedArrayConstructorForDescription(a){switch(a){case\"|u1\":return Uint8Array;case\"<u2\":return Uint16Array;case\"<u4\":return Uint32Array;case\"<u8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit unsigned integer values, support for this dtype is not yet implemented.\");case\"|i1\":return Int8Array;case\"<i2\":return Int16Array;case\"<i4\":return Int32Array;case\"<i8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit integer values, support for this dtype is not yet implemented.\");case\"<f2\":throw new Error(\"Because JavaScript doesn't currently include standard support for 16-bit floating point values, support for this dtype is not yet implemented.\");case\"<f4\":return Float32Array;case\"<f8\":return Float64Array;default:throw new Error(\"Unknown or not yet implemented numpy dtype description: \"+dtype)}}},843:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const zlib_inflate=__webpack_require__(948),utils=__webpack_require__(236),strings=__webpack_require__(373),msg=__webpack_require__(898),ZStream=__webpack_require__(292),GZheader=__webpack_require__(401),toString=Object.prototype.toString,{Z_NO_FLUSH,Z_FINISH,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR}=__webpack_require__(619);function Inflate(options){this.options=utils.assign({chunkSize:65536,windowBits:15,to:\"\"},options||{});const opt=this.options;opt.raw&&opt.windowBits>=0&&opt.windowBits<16&&(opt.windowBits=-opt.windowBits,0===opt.windowBits&&(opt.windowBits=-15)),!(opt.windowBits>=0&&opt.windowBits<16)||options&&options.windowBits||(opt.windowBits+=32),opt.windowBits>15&&opt.windowBits<48&&0==(15&opt.windowBits)&&(opt.windowBits|=15),this.err=0,this.msg=\"\",this.ended=!1,this.chunks=[],this.strm=new ZStream,this.strm.avail_out=0;let status=zlib_inflate.inflateInit2(this.strm,opt.windowBits);if(status!==Z_OK)throw new Error(msg[status]);if(this.header=new GZheader,zlib_inflate.inflateGetHeader(this.strm,this.header),opt.dictionary&&(\"string\"==typeof opt.dictionary?opt.dictionary=strings.string2buf(opt.dictionary):\"[object ArrayBuffer]\"===toString.call(opt.dictionary)&&(opt.dictionary=new Uint8Array(opt.dictionary)),opt.raw&&(status=zlib_inflate.inflateSetDictionary(this.strm,opt.dictionary),status!==Z_OK)))throw new Error(msg[status])}function inflate(input,options){const inflator=new Inflate(options);if(inflator.push(input),inflator.err)throw inflator.msg||msg[inflator.err];return inflator.result}Inflate.prototype.push=function(data,flush_mode){const strm=this.strm,chunkSize=this.options.chunkSize,dictionary=this.options.dictionary;let status,_flush_mode,last_avail_out;if(this.ended)return!1;for(_flush_mode=flush_mode===~~flush_mode?flush_mode:!0===flush_mode?Z_FINISH:Z_NO_FLUSH,\"[object ArrayBuffer]\"===toString.call(data)?strm.input=new Uint8Array(data):strm.input=data,strm.next_in=0,strm.avail_in=strm.input.length;;){for(0===strm.avail_out&&(strm.output=new Uint8Array(chunkSize),strm.next_out=0,strm.avail_out=chunkSize),status=zlib_inflate.inflate(strm,_flush_mode),status===Z_NEED_DICT&&dictionary&&(status=zlib_inflate.inflateSetDictionary(strm,dictionary),status===Z_OK?status=zlib_inflate.inflate(strm,_flush_mode):status===Z_DATA_ERROR&&(status=Z_NEED_DICT));strm.avail_in>0&&status===Z_STREAM_END&&strm.state.wrap>0&&0!==data[strm.next_in];)zlib_inflate.inflateReset(strm),status=zlib_inflate.inflate(strm,_flush_mode);switch(status){case Z_STREAM_ERROR:case Z_DATA_ERROR:case Z_NEED_DICT:case Z_MEM_ERROR:return this.onEnd(status),this.ended=!0,!1}if(last_avail_out=strm.avail_out,strm.next_out&&(0===strm.avail_out||status===Z_STREAM_END))if(\"string\"===this.options.to){let next_out_utf8=strings.utf8border(strm.output,strm.next_out),tail=strm.next_out-next_out_utf8,utf8str=strings.buf2string(strm.output,next_out_utf8);strm.next_out=tail,strm.avail_out=chunkSize-tail,tail&&strm.output.set(strm.output.subarray(next_out_utf8,next_out_utf8+tail),0),this.onData(utf8str)}else this.onData(strm.output.length===strm.next_out?strm.output:strm.output.subarray(0,strm.next_out));if(status!==Z_OK||0!==last_avail_out){if(status===Z_STREAM_END)return status=zlib_inflate.inflateEnd(this.strm),this.onEnd(status),this.ended=!0,!0;if(0===strm.avail_in)break}}return!0},Inflate.prototype.onData=function(chunk){this.chunks.push(chunk)},Inflate.prototype.onEnd=function(status){status===Z_OK&&(\"string\"===this.options.to?this.result=this.chunks.join(\"\"):this.result=utils.flattenChunks(this.chunks)),this.chunks=[],this.err=status,this.msg=this.strm.msg},module.exports.rr=inflate,__webpack_require__(619)},236:module=>{\"use strict\";const _has=(obj,key)=>Object.prototype.hasOwnProperty.call(obj,key);module.exports.assign=function(obj){const sources=Array.prototype.slice.call(arguments,1);for(;sources.length;){const source=sources.shift();if(source){if(\"object\"!=typeof source)throw new TypeError(source+\"must be non-object\");for(const p in source)_has(source,p)&&(obj[p]=source[p])}}return obj},module.exports.flattenChunks=chunks=>{let len=0;for(let i=0,l=chunks.length;i<l;i++)len+=chunks[i].length;const result=new Uint8Array(len);for(let i=0,pos=0,l=chunks.length;i<l;i++){let chunk=chunks[i];result.set(chunk,pos),pos+=chunk.length}return result}},373:module=>{\"use strict\";let STR_APPLY_UIA_OK=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch(__){STR_APPLY_UIA_OK=!1}const _utf8len=new Uint8Array(256);for(let q=0;q<256;q++)_utf8len[q]=q>=252?6:q>=248?5:q>=240?4:q>=224?3:q>=192?2:1;_utf8len[254]=_utf8len[254]=1,module.exports.string2buf=str=>{let buf,c,c2,m_pos,i,str_len=str.length,buf_len=0;for(m_pos=0;m_pos<str_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),buf_len+=c<128?1:c<2048?2:c<65536?3:4;for(buf=new Uint8Array(buf_len),i=0,m_pos=0;i<buf_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),c<128?buf[i++]=c:c<2048?(buf[i++]=192|c>>>6,buf[i++]=128|63&c):c<65536?(buf[i++]=224|c>>>12,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c):(buf[i++]=240|c>>>18,buf[i++]=128|c>>>12&63,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c);return buf};module.exports.buf2string=(buf,max)=>{let i,out;const len=max||buf.length,utf16buf=new Array(2*len);for(out=0,i=0;i<len;){let c=buf[i++];if(c<128){utf16buf[out++]=c;continue}let c_len=_utf8len[c];if(c_len>4)utf16buf[out++]=65533,i+=c_len-1;else{for(c&=2===c_len?31:3===c_len?15:7;c_len>1&&i<len;)c=c<<6|63&buf[i++],c_len--;c_len>1?utf16buf[out++]=65533:c<65536?utf16buf[out++]=c:(c-=65536,utf16buf[out++]=55296|c>>10&1023,utf16buf[out++]=56320|1023&c)}}return((buf,len)=>{if(len<65534&&buf.subarray&&STR_APPLY_UIA_OK)return String.fromCharCode.apply(null,buf.length===len?buf:buf.subarray(0,len));let result=\"\";for(let i=0;i<len;i++)result+=String.fromCharCode(buf[i]);return result})(utf16buf,out)},module.exports.utf8border=(buf,max)=>{(max=max||buf.length)>buf.length&&(max=buf.length);let pos=max-1;for(;pos>=0&&128==(192&buf[pos]);)pos--;return pos<0||0===pos?max:pos+_utf8len[buf[pos]]>max?pos:max}},69:module=>{\"use strict\";module.exports=(adler,buf,len,pos)=>{let s1=65535&adler|0,s2=adler>>>16&65535|0,n=0;for(;0!==len;){n=len>2e3?2e3:len,len-=n;do{s1=s1+buf[pos++]|0,s2=s2+s1|0}while(--n);s1%=65521,s2%=65521}return s1|s2<<16|0}},619:module=>{\"use strict\";module.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},869:module=>{\"use strict\";const crcTable=new Uint32Array((()=>{let c,table=[];for(var n=0;n<256;n++){c=n;for(var k=0;k<8;k++)c=1&c?3988292384^c>>>1:c>>>1;table[n]=c}return table})());module.exports=(crc,buf,len,pos)=>{const t=crcTable,end=pos+len;crc^=-1;for(let i=pos;i<end;i++)crc=crc>>>8^t[255&(crc^buf[i])];return-1^crc}},401:module=>{\"use strict\";module.exports=function GZheader(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name=\"\",this.comment=\"\",this.hcrc=0,this.done=!1}},264:module=>{\"use strict\";module.exports=function inflate_fast(strm,start){let _in,last,_out,beg,end,dmax,wsize,whave,wnext,s_window,hold,bits,lcode,dcode,lmask,dmask,here,op,len,dist,from,from_source,input,output;const state=strm.state;_in=strm.next_in,input=strm.input,last=_in+(strm.avail_in-5),_out=strm.next_out,output=strm.output,beg=_out-(start-strm.avail_out),end=_out+(strm.avail_out-257),dmax=state.dmax,wsize=state.wsize,whave=state.whave,wnext=state.wnext,s_window=state.window,hold=state.hold,bits=state.bits,lcode=state.lencode,dcode=state.distcode,lmask=(1<<state.lenbits)-1,dmask=(1<<state.distbits)-1;top:do{bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=lcode[hold&lmask];dolen:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,0===op)output[_out++]=65535&here;else{if(!(16&op)){if(0==(64&op)){here=lcode[(65535&here)+(hold&(1<<op)-1)];continue dolen}if(32&op){state.mode=12;break top}strm.msg=\"invalid literal/length code\",state.mode=30;break top}len=65535&here,op&=15,op&&(bits<op&&(hold+=input[_in++]<<bits,bits+=8),len+=hold&(1<<op)-1,hold>>>=op,bits-=op),bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=dcode[hold&dmask];dodist:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,!(16&op)){if(0==(64&op)){here=dcode[(65535&here)+(hold&(1<<op)-1)];continue dodist}strm.msg=\"invalid distance code\",state.mode=30;break top}if(dist=65535&here,op&=15,bits<op&&(hold+=input[_in++]<<bits,bits+=8,bits<op&&(hold+=input[_in++]<<bits,bits+=8)),dist+=hold&(1<<op)-1,dist>dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(hold>>>=op,bits-=op,op=_out-beg,dist>op){if(op=dist-op,op>whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(from=0,from_source=s_window,0===wnext){if(from+=wsize-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}else if(wnext<op){if(from+=wsize+wnext-op,op-=wnext,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);if(from=0,wnext<len){op=wnext,len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}}else if(from+=wnext-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}for(;len>2;)output[_out++]=from_source[from++],output[_out++]=from_source[from++],output[_out++]=from_source[from++],len-=3;len&&(output[_out++]=from_source[from++],len>1&&(output[_out++]=from_source[from++]))}else{from=_out-dist;do{output[_out++]=output[from++],output[_out++]=output[from++],output[_out++]=output[from++],len-=3}while(len>2);len&&(output[_out++]=output[from++],len>1&&(output[_out++]=output[from++]))}break}}break}}while(_in<last&&_out<end);len=bits>>3,_in-=len,bits-=len<<3,hold&=(1<<bits)-1,strm.next_in=_in,strm.next_out=_out,strm.avail_in=_in<last?last-_in+5:5-(_in-last),strm.avail_out=_out<end?end-_out+257:257-(_out-end),state.hold=hold,state.bits=bits}},948:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const adler32=__webpack_require__(69),crc32=__webpack_require__(869),inflate_fast=__webpack_require__(264),inflate_table=__webpack_require__(241),{Z_FINISH,Z_BLOCK,Z_TREES,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR,Z_BUF_ERROR,Z_DEFLATED}=__webpack_require__(619),zswap32=q=>(q>>>24&255)+(q>>>8&65280)+((65280&q)<<8)+((255&q)<<24);function InflateState(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const inflateResetKeep=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return strm.total_in=strm.total_out=state.total=0,strm.msg=\"\",state.wrap&&(strm.adler=1&state.wrap),state.mode=1,state.last=0,state.havedict=0,state.dmax=32768,state.head=null,state.hold=0,state.bits=0,state.lencode=state.lendyn=new Int32Array(852),state.distcode=state.distdyn=new Int32Array(592),state.sane=1,state.back=-1,Z_OK},inflateReset=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return state.wsize=0,state.whave=0,state.wnext=0,inflateResetKeep(strm)},inflateReset2=(strm,windowBits)=>{let wrap;if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return windowBits<0?(wrap=0,windowBits=-windowBits):(wrap=1+(windowBits>>4),windowBits<48&&(windowBits&=15)),windowBits&&(windowBits<8||windowBits>15)?Z_STREAM_ERROR:(null!==state.window&&state.wbits!==windowBits&&(state.window=null),state.wrap=wrap,state.wbits=windowBits,inflateReset(strm))},inflateInit2=(strm,windowBits)=>{if(!strm)return Z_STREAM_ERROR;const state=new InflateState;strm.state=state,state.window=null;const ret=inflateReset2(strm,windowBits);return ret!==Z_OK&&(strm.state=null),ret};let lenfix,distfix,virgin=!0;const fixedtables=state=>{if(virgin){lenfix=new Int32Array(512),distfix=new Int32Array(32);let sym=0;for(;sym<144;)state.lens[sym++]=8;for(;sym<256;)state.lens[sym++]=9;for(;sym<280;)state.lens[sym++]=7;for(;sym<288;)state.lens[sym++]=8;for(inflate_table(1,state.lens,0,288,lenfix,0,state.work,{bits:9}),sym=0;sym<32;)state.lens[sym++]=5;inflate_table(2,state.lens,0,32,distfix,0,state.work,{bits:5}),virgin=!1}state.lencode=lenfix,state.lenbits=9,state.distcode=distfix,state.distbits=5},updatewindow=(strm,src,end,copy)=>{let dist;const state=strm.state;return null===state.window&&(state.wsize=1<<state.wbits,state.wnext=0,state.whave=0,state.window=new Uint8Array(state.wsize)),copy>=state.wsize?(state.window.set(src.subarray(end-state.wsize,end),0),state.wnext=0,state.whave=state.wsize):(dist=state.wsize-state.wnext,dist>copy&&(dist=copy),state.window.set(src.subarray(end-copy,end-copy+dist),state.wnext),(copy-=dist)?(state.window.set(src.subarray(end-copy,end),0),state.wnext=copy,state.whave=state.wsize):(state.wnext+=dist,state.wnext===state.wsize&&(state.wnext=0),state.whave<state.wsize&&(state.whave+=dist))),0};module.exports.inflateReset=inflateReset,module.exports.inflateReset2=inflateReset2,module.exports.inflateResetKeep=inflateResetKeep,module.exports.inflateInit=strm=>inflateInit2(strm,15),module.exports.inflateInit2=inflateInit2,module.exports.inflate=(strm,flush)=>{let state,input,output,next,put,have,left,hold,bits,_in,_out,copy,from,from_source,here_bits,here_op,here_val,last_bits,last_op,last_val,len,ret,here=0;const hbuf=new Uint8Array(4);let opts,n;const order=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(!strm||!strm.state||!strm.output||!strm.input&&0!==strm.avail_in)return Z_STREAM_ERROR;state=strm.state,12===state.mode&&(state.mode=13),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,_in=have,_out=left,ret=Z_OK;inf_leave:for(;;)switch(state.mode){case 1:if(0===state.wrap){state.mode=13;break}for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(2&state.wrap&&35615===hold){state.check=0,hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0),hold=0,bits=0,state.mode=2;break}if(state.flags=0,state.head&&(state.head.done=!1),!(1&state.wrap)||(((255&hold)<<8)+(hold>>8))%31){strm.msg=\"incorrect header check\",state.mode=30;break}if((15&hold)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(hold>>>=4,bits-=4,len=8+(15&hold),0===state.wbits)state.wbits=len;else if(len>state.wbits){strm.msg=\"invalid window size\",state.mode=30;break}state.dmax=1<<state.wbits,strm.adler=state.check=1,state.mode=512&hold?10:12,hold=0,bits=0;break;case 2:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.flags=hold,(255&state.flags)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(57344&state.flags){strm.msg=\"unknown header flags set\",state.mode=30;break}state.head&&(state.head.text=hold>>8&1),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=3;case 3:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.time=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,hbuf[2]=hold>>>16&255,hbuf[3]=hold>>>24&255,state.check=crc32(state.check,hbuf,4,0)),hold=0,bits=0,state.mode=4;case 4:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.xflags=255&hold,state.head.os=hold>>8),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=5;case 5:if(1024&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length=hold,state.head&&(state.head.extra_len=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0}else state.head&&(state.head.extra=null);state.mode=6;case 6:if(1024&state.flags&&(copy=state.length,copy>have&&(copy=have),copy&&(state.head&&(len=state.head.extra_len-state.length,state.head.extra||(state.head.extra=new Uint8Array(state.head.extra_len)),state.head.extra.set(input.subarray(next,next+copy),len)),512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,state.length-=copy),state.length))break inf_leave;state.length=0,state.mode=7;case 7:if(2048&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.name+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.name=null);state.length=0,state.mode=8;case 8:if(4096&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.comment+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.comment=null);state.mode=9;case 9:if(512&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(65535&state.check)){strm.msg=\"header crc mismatch\",state.mode=30;break}hold=0,bits=0}state.head&&(state.head.hcrc=state.flags>>9&1,state.head.done=!0),strm.adler=state.check=0,state.mode=12;break;case 10:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}strm.adler=state.check=zswap32(hold),hold=0,bits=0,state.mode=11;case 11:if(0===state.havedict)return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,Z_NEED_DICT;strm.adler=state.check=1,state.mode=12;case 12:if(flush===Z_BLOCK||flush===Z_TREES)break inf_leave;case 13:if(state.last){hold>>>=7&bits,bits-=7&bits,state.mode=27;break}for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}switch(state.last=1&hold,hold>>>=1,bits-=1,3&hold){case 0:state.mode=14;break;case 1:if(fixedtables(state),state.mode=20,flush===Z_TREES){hold>>>=2,bits-=2;break inf_leave}break;case 2:state.mode=17;break;case 3:strm.msg=\"invalid block type\",state.mode=30}hold>>>=2,bits-=2;break;case 14:for(hold>>>=7&bits,bits-=7&bits;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if((65535&hold)!=(hold>>>16^65535)){strm.msg=\"invalid stored block lengths\",state.mode=30;break}if(state.length=65535&hold,hold=0,bits=0,state.mode=15,flush===Z_TREES)break inf_leave;case 15:state.mode=16;case 16:if(copy=state.length,copy){if(copy>have&&(copy=have),copy>left&&(copy=left),0===copy)break inf_leave;output.set(input.subarray(next,next+copy),put),have-=copy,next+=copy,left-=copy,put+=copy,state.length-=copy;break}state.mode=12;break;case 17:for(;bits<14;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.nlen=257+(31&hold),hold>>>=5,bits-=5,state.ndist=1+(31&hold),hold>>>=5,bits-=5,state.ncode=4+(15&hold),hold>>>=4,bits-=4,state.nlen>286||state.ndist>30){strm.msg=\"too many length or distance symbols\",state.mode=30;break}state.have=0,state.mode=18;case 18:for(;state.have<state.ncode;){for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.lens[order[state.have++]]=7&hold,hold>>>=3,bits-=3}for(;state.have<19;)state.lens[order[state.have++]]=0;if(state.lencode=state.lendyn,state.lenbits=7,opts={bits:state.lenbits},ret=inflate_table(0,state.lens,0,19,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid code lengths set\",state.mode=30;break}state.have=0,state.mode=19;case 19:for(;state.have<state.nlen+state.ndist;){for(;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_val<16)hold>>>=here_bits,bits-=here_bits,state.lens[state.have++]=here_val;else{if(16===here_val){for(n=here_bits+2;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold>>>=here_bits,bits-=here_bits,0===state.have){strm.msg=\"invalid bit length repeat\",state.mode=30;break}len=state.lens[state.have-1],copy=3+(3&hold),hold>>>=2,bits-=2}else if(17===here_val){for(n=here_bits+3;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=3+(7&hold),hold>>>=3,bits-=3}else{for(n=here_bits+7;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=11+(127&hold),hold>>>=7,bits-=7}if(state.have+copy>state.nlen+state.ndist){strm.msg=\"invalid bit length repeat\",state.mode=30;break}for(;copy--;)state.lens[state.have++]=len}}if(30===state.mode)break;if(0===state.lens[256]){strm.msg=\"invalid code -- missing end-of-block\",state.mode=30;break}if(state.lenbits=9,opts={bits:state.lenbits},ret=inflate_table(1,state.lens,0,state.nlen,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid literal/lengths set\",state.mode=30;break}if(state.distbits=6,state.distcode=state.distdyn,opts={bits:state.distbits},ret=inflate_table(2,state.lens,state.nlen,state.ndist,state.distcode,0,state.work,opts),state.distbits=opts.bits,ret){strm.msg=\"invalid distances set\",state.mode=30;break}if(state.mode=20,flush===Z_TREES)break inf_leave;case 20:state.mode=21;case 21:if(have>=6&&left>=258){strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,inflate_fast(strm,_out),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,12===state.mode&&(state.back=-1);break}for(state.back=0;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_op&&0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.lencode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,state.length=here_val,0===here_op){state.mode=26;break}if(32&here_op){state.back=-1,state.mode=12;break}if(64&here_op){strm.msg=\"invalid literal/length code\",state.mode=30;break}state.extra=15&here_op,state.mode=22;case 22:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}state.was=state.length,state.mode=23;case 23:for(;here=state.distcode[hold&(1<<state.distbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.distcode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,64&here_op){strm.msg=\"invalid distance code\",state.mode=30;break}state.offset=here_val,state.extra=15&here_op,state.mode=24;case 24:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.offset+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}if(state.offset>state.dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break}state.mode=25;case 25:if(0===left)break inf_leave;if(copy=_out-left,state.offset>copy){if(copy=state.offset-copy,copy>state.whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break}copy>state.wnext?(copy-=state.wnext,from=state.wsize-copy):from=state.wnext-copy,copy>state.length&&(copy=state.length),from_source=state.window}else from_source=output,from=put-state.offset,copy=state.length;copy>left&&(copy=left),left-=copy,state.length-=copy;do{output[put++]=from_source[from++]}while(--copy);0===state.length&&(state.mode=21);break;case 26:if(0===left)break inf_leave;output[put++]=state.length,left--,state.mode=21;break;case 27:if(state.wrap){for(;bits<32;){if(0===have)break inf_leave;have--,hold|=input[next++]<<bits,bits+=8}if(_out-=left,strm.total_out+=_out,state.total+=_out,_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,put-_out):adler32(state.check,output,_out,put-_out)),_out=left,(state.flags?hold:zswap32(hold))!==state.check){strm.msg=\"incorrect data check\",state.mode=30;break}hold=0,bits=0}state.mode=28;case 28:if(state.wrap&&state.flags){for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(4294967295&state.total)){strm.msg=\"incorrect length check\",state.mode=30;break}hold=0,bits=0}state.mode=29;case 29:ret=Z_STREAM_END;break inf_leave;case 30:ret=Z_DATA_ERROR;break inf_leave;case 31:return Z_MEM_ERROR;case 32:default:return Z_STREAM_ERROR}return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,(state.wsize||_out!==strm.avail_out&&state.mode<30&&(state.mode<27||flush!==Z_FINISH))&&updatewindow(strm,strm.output,strm.next_out,_out-strm.avail_out)?(state.mode=31,Z_MEM_ERROR):(_in-=strm.avail_in,_out-=strm.avail_out,strm.total_in+=_in,strm.total_out+=_out,state.total+=_out,state.wrap&&_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,strm.next_out-_out):adler32(state.check,output,_out,strm.next_out-_out)),strm.data_type=state.bits+(state.last?64:0)+(12===state.mode?128:0)+(20===state.mode||15===state.mode?256:0),(0===_in&&0===_out||flush===Z_FINISH)&&ret===Z_OK&&(ret=Z_BUF_ERROR),ret)},module.exports.inflateEnd=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;let state=strm.state;return state.window&&(state.window=null),strm.state=null,Z_OK},module.exports.inflateGetHeader=(strm,head)=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return 0==(2&state.wrap)?Z_STREAM_ERROR:(state.head=head,head.done=!1,Z_OK)},module.exports.inflateSetDictionary=(strm,dictionary)=>{const dictLength=dictionary.length;let state,dictid,ret;return strm&&strm.state?(state=strm.state,0!==state.wrap&&11!==state.mode?Z_STREAM_ERROR:11===state.mode&&(dictid=1,dictid=adler32(dictid,dictionary,dictLength,0),dictid!==state.check)?Z_DATA_ERROR:(ret=updatewindow(strm,dictionary,dictLength,dictLength),ret?(state.mode=31,Z_MEM_ERROR):(state.havedict=1,Z_OK))):Z_STREAM_ERROR},module.exports.inflateInfo=\"pako inflate (from Nodeca project)\"},241:module=>{\"use strict\";const lbase=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),lext=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),dbase=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),dext=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]);module.exports=(type,lens,lens_index,codes,table,table_index,work,opts)=>{const bits=opts.bits;let incr,fill,low,mask,next,end,len=0,sym=0,min=0,max=0,root=0,curr=0,drop=0,left=0,used=0,huff=0,base=null,base_index=0;const count=new Uint16Array(16),offs=new Uint16Array(16);let here_bits,here_op,here_val,extra=null,extra_index=0;for(len=0;len<=15;len++)count[len]=0;for(sym=0;sym<codes;sym++)count[lens[lens_index+sym]]++;for(root=bits,max=15;max>=1&&0===count[max];max--);if(root>max&&(root=max),0===max)return table[table_index++]=20971520,table[table_index++]=20971520,opts.bits=1,0;for(min=1;min<max&&0===count[min];min++);for(root<min&&(root=min),left=1,len=1;len<=15;len++)if(left<<=1,left-=count[len],left<0)return-1;if(left>0&&(0===type||1!==max))return-1;for(offs[1]=0,len=1;len<15;len++)offs[len+1]=offs[len]+count[len];for(sym=0;sym<codes;sym++)0!==lens[lens_index+sym]&&(work[offs[lens[lens_index+sym]]++]=sym);if(0===type?(base=extra=work,end=19):1===type?(base=lbase,base_index-=257,extra=lext,extra_index-=257,end=256):(base=dbase,extra=dext,end=-1),huff=0,sym=0,len=min,next=table_index,curr=root,drop=0,low=-1,used=1<<root,mask=used-1,1===type&&used>852||2===type&&used>592)return 1;for(;;){here_bits=len-drop,work[sym]<end?(here_op=0,here_val=work[sym]):work[sym]>end?(here_op=extra[extra_index+work[sym]],here_val=base[base_index+work[sym]]):(here_op=96,here_val=0),incr=1<<len-drop,fill=1<<curr,min=fill;do{fill-=incr,table[next+(huff>>drop)+fill]=here_bits<<24|here_op<<16|here_val|0}while(0!==fill);for(incr=1<<len-1;huff&incr;)incr>>=1;if(0!==incr?(huff&=incr-1,huff+=incr):huff=0,sym++,0==--count[len]){if(len===max)break;len=lens[lens_index+work[sym]]}if(len>root&&(huff&mask)!==low){for(0===drop&&(drop=root),next+=min,curr=len-drop,left=1<<curr;curr+drop<max&&(left-=count[curr+drop],!(left<=0));)curr++,left<<=1;if(used+=1<<curr,1===type&&used>852||2===type&&used>592)return 1;low=huff&mask,table[low]=root<<24|curr<<16|next-table_index|0}}return 0!==huff&&(table[next+huff]=len-drop<<24|64<<16|0),opts.bits=root,0}},898:module=>{\"use strict\";module.exports={2:\"need dictionary\",1:\"stream end\",0:\"\",\"-1\":\"file error\",\"-2\":\"stream error\",\"-3\":\"data error\",\"-4\":\"insufficient memory\",\"-5\":\"buffer error\",\"-6\":\"incompatible version\"}},292:module=>{\"use strict\";module.exports=function ZStream(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg=\"\",this.state=null,this.data_type=2,this.adler=0}},330:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{\"use strict\";__webpack_require__.d(__webpack_exports__,{default:()=>__WEBPACK_DEFAULT_EXPORT__});var numpy_parser__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(829),ndarray__WEBPACK_IMPORTED_MODULE_1__=__webpack_require__(861),pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__=__webpack_require__(843);const __WEBPACK_DEFAULT_EXPORT__=loader={unpack_obj:function unpack_obj(obj){if(Array.isArray(obj)){var ret=[];for(var v of obj)ret.push(unpack_obj(v));return ret}if(obj instanceof Object){if(obj.hasOwnProperty(\"__type__\"))return function unpack_custom_data(obj){if(\"npy\"==obj.__type__){var uint8arr;if(window.obj=obj,obj.hasOwnProperty(\"zdata\")){const compressed=Uint8Array.from(window.atob(obj.zdata),(c=>c.charCodeAt(0)));uint8arr=(0,pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__.rr)(compressed)}else uint8arr=Uint8Array.from(window.atob(obj.data),(c=>c.charCodeAt(0)));var arr=(0,numpy_parser__WEBPACK_IMPORTED_MODULE_0__.g)(uint8arr.buffer);if(arr=ndarray__WEBPACK_IMPORTED_MODULE_1__(arr.data,arr.shape),obj.hasOwnProperty(\"min\")){let scale=\"uint8\"===arr.dtype?255:65535;for(var size=1,i=0;i<arr.shape.length;i++)size*=arr.shape[i];var arr_=ndarray__WEBPACK_IMPORTED_MODULE_1__(new Float32Array(size),arr.shape);for(i=0;i<arr.data.length;i++)arr_.data[i]=obj.min+(obj.max-obj.min)*arr.data[i]/scale;return arr_}return arr}return{}}(obj);ret={};for(var k of Object.keys(obj))ret[k]=unpack_obj(obj[k]);return ret}return obj}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.n=module=>{var getter=module&&module.__esModule?()=>module.default:()=>module;return __webpack_require__.d(getter,{a:getter}),getter},__webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(330)})().default;</script>\n",
       "<script>var AttentionMulti;AttentionMulti=(()=>{\"use strict\";var __webpack_modules__={143:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{function noop(){}__webpack_require__.d(__webpack_exports__,{default:()=>AttentionMulti_svelte});function run(fn){return fn()}function blank_object(){return Object.create(null)}function run_all(fns){fns.forEach(run)}function is_function(thing){return\"function\"==typeof thing}function safe_not_equal(a,b){return a!=a?b==b:a!==b||a&&\"object\"==typeof a||\"function\"==typeof a}function is_empty(obj){return 0===Object.keys(obj).length}function get_slot_context(definition,ctx,$$scope,fn){return definition[1]&&fn?function internal_assign(tar,src){for(const k in src)tar[k]=src[k];return tar}($$scope.ctx.slice(),definition[1](fn(ctx))):$$scope.ctx}function get_slot_changes(definition,$$scope,dirty,fn){if(definition[2]&&fn){const lets=definition[2](fn(dirty));if(void 0===$$scope.dirty)return lets;if(\"object\"==typeof lets){const merged=[],len=Math.max($$scope.dirty.length,lets.length);for(let i=0;i<len;i+=1)merged[i]=$$scope.dirty[i]|lets[i];return merged}return $$scope.dirty|lets}return $$scope.dirty}new Set;function append(target,node){target.appendChild(node)}function insert(target,node,anchor){target.insertBefore(node,anchor||null)}function detach(node){node.parentNode.removeChild(node)}function destroy_each(iterations,detaching){for(let i=0;i<iterations.length;i+=1)iterations[i]&&iterations[i].d(detaching)}function internal_element(name){return document.createElement(name)}function internal_text(data){return document.createTextNode(data)}function space(){return internal_text(\" \")}function listen(node,event,handler,options){return node.addEventListener(event,handler,options),()=>node.removeEventListener(event,handler,options)}function attr(node,attribute,value){null==value?node.removeAttribute(attribute):node.getAttribute(attribute)!==value&&node.setAttribute(attribute,value)}function set_data(text,data){data=\"\"+data,text.wholeText!==data&&(text.data=data)}function set_style(node,key,value,important){node.style.setProperty(key,value,important?\"important\":\"\")}new Set;let current_component;function set_current_component(component){current_component=component}function get_current_component(){if(!current_component)throw new Error(\"Function called outside component initialization\");return current_component}const dirty_components=[],binding_callbacks=[],render_callbacks=[],flush_callbacks=[],resolved_promise=Promise.resolve();let update_scheduled=!1;function schedule_update(){update_scheduled||(update_scheduled=!0,resolved_promise.then(flush))}function add_render_callback(fn){render_callbacks.push(fn)}function add_flush_callback(fn){flush_callbacks.push(fn)}let flushing=!1;const seen_callbacks=new Set;function flush(){if(!flushing){flushing=!0;do{for(let i=0;i<dirty_components.length;i+=1){const component=dirty_components[i];set_current_component(component),update(component.$$)}for(set_current_component(null),dirty_components.length=0;binding_callbacks.length;)binding_callbacks.pop()();for(let i=0;i<render_callbacks.length;i+=1){const callback=render_callbacks[i];seen_callbacks.has(callback)||(seen_callbacks.add(callback),callback())}render_callbacks.length=0}while(dirty_components.length);for(;flush_callbacks.length;)flush_callbacks.pop()();update_scheduled=!1,flushing=!1,seen_callbacks.clear()}}function update($$){if(null!==$$.fragment){$$.update(),run_all($$.before_update);const dirty=$$.dirty;$$.dirty=[-1],$$.fragment&&$$.fragment.p($$.ctx,dirty),$$.after_update.forEach(add_render_callback)}}const outroing=new Set;let outros;function group_outros(){outros={r:0,c:[],p:outros}}function check_outros(){outros.r||run_all(outros.c),outros=outros.p}function transition_in(block,local){block&&block.i&&(outroing.delete(block),block.i(local))}function transition_out(block,local,detach,callback){if(block&&block.o){if(outroing.has(block))return;outroing.add(block),outros.c.push((()=>{outroing.delete(block),callback&&(detach&&block.d(1),callback())})),block.o(local)}}\"undefined\"!=typeof window?window:\"undefined\"!=typeof globalThis?globalThis:global;new Set([\"allowfullscreen\",\"allowpaymentrequest\",\"async\",\"autofocus\",\"autoplay\",\"checked\",\"controls\",\"default\",\"defer\",\"disabled\",\"formnovalidate\",\"hidden\",\"ismap\",\"loop\",\"multiple\",\"muted\",\"nomodule\",\"novalidate\",\"open\",\"playsinline\",\"readonly\",\"required\",\"reversed\",\"selected\"]);let SvelteElement;function bind(component,name,callback){const index=component.$$.props[name];void 0!==index&&(component.$$.bound[index]=callback,callback(component.$$.ctx[index]))}function create_component(block){block&&block.c()}function mount_component(component,target,anchor){const{fragment,on_mount,on_destroy,after_update}=component.$$;fragment&&fragment.m(target,anchor),add_render_callback((()=>{const new_on_destroy=on_mount.map(run).filter(is_function);on_destroy?on_destroy.push(...new_on_destroy):run_all(new_on_destroy),component.$$.on_mount=[]})),after_update.forEach(add_render_callback)}function destroy_component(component,detaching){const $$=component.$$;null!==$$.fragment&&(run_all($$.on_destroy),$$.fragment&&$$.fragment.d(detaching),$$.on_destroy=$$.fragment=null,$$.ctx=[])}function init(component,options,instance,create_fragment,not_equal,props,dirty=[-1]){const parent_component=current_component;set_current_component(component);const prop_values=options.props||{},$$=component.$$={fragment:null,ctx:null,props,update:noop,not_equal,bound:blank_object(),on_mount:[],on_destroy:[],before_update:[],after_update:[],context:new Map(parent_component?parent_component.$$.context:[]),callbacks:blank_object(),dirty,skip_bound:!1};let ready=!1;if($$.ctx=instance?instance(component,prop_values,((i,ret,...rest)=>{const value=rest.length?rest[0]:ret;return $$.ctx&&not_equal($$.ctx[i],$$.ctx[i]=value)&&(!$$.skip_bound&&$$.bound[i]&&$$.bound[i](value),ready&&function make_dirty(component,i){-1===component.$$.dirty[0]&&(dirty_components.push(component),schedule_update(),component.$$.dirty.fill(0)),component.$$.dirty[i/31|0]|=1<<i%31}(component,i)),ret})):[],$$.update(),ready=!0,run_all($$.before_update),$$.fragment=!!create_fragment&&create_fragment($$.ctx),options.target){if(options.hydrate){const nodes=function children(element){return Array.from(element.childNodes)}(options.target);$$.fragment&&$$.fragment.l(nodes),nodes.forEach(detach)}else $$.fragment&&$$.fragment.c();options.intro&&transition_in(component.$$.fragment),mount_component(component,options.target,options.anchor),flush()}set_current_component(parent_component)}\"function\"==typeof HTMLElement&&(SvelteElement=class extends HTMLElement{constructor(){super(),this.attachShadow({mode:\"open\"})}connectedCallback(){for(const key in this.$$.slotted)this.appendChild(this.$$.slotted[key])}attributeChangedCallback(attr,_oldValue,newValue){this[attr]=newValue}$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}});class SvelteComponent{$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}}function norm(x,ord=2){if(!(\"length\"in x))return function norm_nd(x,ord=2){for(var S=0,i=0;i<x.shape[0];i++)S+=Math.pow(Math.abs(x.get(i)),ord);return Math.pow(S,1/ord)}(x,ord);for(var S=0,i=0;i<x.length;i++)S+=Math.pow(Math.abs(x[i]),ord);return Math.pow(S,1/ord)}function normalize(v,ord=2){var v_norm=norm(v,ord);return v.map((x=>x/(1e-4+v_norm)))}function hue_to_rgb(theta){for(var colors=[[1,0,0],[1,1,0],[0,1,0],[0,1,1],[0,0,1],[1,0,1]].map((c=>normalize(c,1)));theta<0;)theta+=360;theta%=360;var d_theta=360/colors.length,n=Math.floor(theta/d_theta),mix_coef=(theta-n*d_theta)/d_theta,v=function interp(a,b,s){return a.map(((x,i)=>(1-s)*x+s*b[i]))}(colors[n],colors[(n+1)%colors.length],mix_coef);return v=normalize(v,1)}const __rgb_hue_vector_cache=[];function rgb_to_css(color){return`rgb(${255*color[0]}, ${255*color[1]}, ${255*color[2]})`}function sparse_color_map(v,zero_c=[.98,.98,.98],isolate_channel,hues){const v_len=\"length\"in v?v.length:v.shape[0],elem=\"length\"in v?i=>v[i]:i=>v.get(i);if(null==hues&&(hues=function rgb_hue_vector(n){if(n in __rgb_hue_vector_cache)return __rgb_hue_vector_cache[n];let out=[];for(let i=0;i<n;i++){const hue=360*i/n;out.push(hue_to_rgb(hue))}return __rgb_hue_vector_cache[n]=out,out}(v_len)),console.log(\"Hues\",hues),null==isolate_channel){for(var S=[0,0,0],i=0;i<v_len;i++){const ei=elem(i);if(0!=ei)for(var rgb=hues[i],j=0;j<3;j++)S[j]+=ei*rgb[j]}S=normalize(S,1);var mag=norm(v,2);for(mag=Math.max(0,Math.min(1,mag)),j=0;j<3;j++)S[j]=mag*S[j]+(1-mag)*zero_c[j];return S}var Ci=hues[i=isolate_channel];for(S=[0,0,0],j=0;j<3;j++)S[j]=elem(i)*Ci[j]+(1-elem(i))*zero_c[j];return S}function sparse_color_map_css(v,zero_c=[.98,.98,.98],isolate_channel,hues){return rgb_to_css(sparse_color_map(v,zero_c,isolate_channel,hues))}function create_if_block(ctx){let div0,t,div1;return{c(){div0=internal_element(\"div\"),t=space(),div1=internal_element(\"div\"),attr(div0,\"class\",\"focus-top svelte-1vzuzhc\"),set_style(div0,\"height\",ctx[2]*ctx[3]/ctx[0].shape[0]+\"px\"),attr(div1,\"class\",\"focus-bottom svelte-1vzuzhc\"),set_style(div1,\"height\",ctx[2]*(1-(ctx[3]+1)/ctx[0].shape[0])+\"px\")},m(target,anchor){insert(target,div0,anchor),insert(target,t,anchor),insert(target,div1,anchor)},p(ctx,dirty){13&dirty&&set_style(div0,\"height\",ctx[2]*ctx[3]/ctx[0].shape[0]+\"px\"),13&dirty&&set_style(div1,\"height\",ctx[2]*(1-(ctx[3]+1)/ctx[0].shape[0])+\"px\")},d(detaching){detaching&&detach(div0),detaching&&detach(t),detaching&&detach(div1)}}}function create_fragment(ctx){let div,canvas_1,t,if_block=null!=ctx[3]&&create_if_block(ctx);return{c(){div=internal_element(\"div\"),canvas_1=internal_element(\"canvas\"),t=space(),if_block&&if_block.c(),set_style(canvas_1,\"width\",ctx[1]+\"px\"),set_style(canvas_1,\"height\",ctx[2]+\"px\"),attr(canvas_1,\"class\",\"svelte-1vzuzhc\"),attr(div,\"class\",\"container svelte-1vzuzhc\"),set_style(div,\"width\",ctx[1]+\"px\"),set_style(div,\"height\",ctx[2]+\"px\")},m(target,anchor){insert(target,div,anchor),append(div,canvas_1),ctx[8](canvas_1),append(div,t),if_block&&if_block.m(div,null)},p(ctx,[dirty]){2&dirty&&set_style(canvas_1,\"width\",ctx[1]+\"px\"),4&dirty&&set_style(canvas_1,\"height\",ctx[2]+\"px\"),null!=ctx[3]?if_block?if_block.p(ctx,dirty):(if_block=create_if_block(ctx),if_block.c(),if_block.m(div,null)):if_block&&(if_block.d(1),if_block=null),2&dirty&&set_style(div,\"width\",ctx[1]+\"px\"),4&dirty&&set_style(div,\"height\",ctx[2]+\"px\")},i:noop,o:noop,d(detaching){detaching&&detach(div),ctx[8](null),if_block&&if_block.d()}}}function instance($$self,$$props,$$invalidate){let canvas,{array}=$$props,{width}=$$props,{height}=$$props,{hues}=$$props,{focus_token}=$$props,{isolate_channel}=$$props,{color_map=sparse_color_map}=$$props;function get_color(array,x,y,isolate_channel,hues){if(x<y)return[255,255,255];var v=array.pick(x,y,null);return color_map(v,void 0,isolate_channel,hues).map((c=>255*c))}function draw(canvas,array,isolate_channel,hues){if(null!=canvas&&null!=array){canvas.width=array.shape[0],canvas.height=array.shape[1];for(var ctx=canvas.getContext(\"2d\"),imgData=ctx.getImageData(0,0,canvas.width,canvas.height),x=0;x<canvas.width;x++)for(var y=0;y<canvas.height;y++){for(var ind=x*canvas.width+y,color=get_color(array,x,y,isolate_channel,hues=hues),channel=0;channel<3;channel++)imgData.data[4*ind+channel]=color[channel];imgData.data[4*ind+3]=255}ctx.putImageData(imgData,0,0)}}return function onMount(fn){get_current_component().$$.on_mount.push(fn)}((()=>draw(canvas,array,isolate_channel))),$$self.$$set=$$props=>{\"array\"in $$props&&$$invalidate(0,array=$$props.array),\"width\"in $$props&&$$invalidate(1,width=$$props.width),\"height\"in $$props&&$$invalidate(2,height=$$props.height),\"hues\"in $$props&&$$invalidate(5,hues=$$props.hues),\"focus_token\"in $$props&&$$invalidate(3,focus_token=$$props.focus_token),\"isolate_channel\"in $$props&&$$invalidate(6,isolate_channel=$$props.isolate_channel),\"color_map\"in $$props&&$$invalidate(7,color_map=$$props.color_map)},$$self.$$.update=()=>{113&$$self.$$.dirty&&draw(canvas,array,isolate_channel,hues)},[array,width,height,focus_token,canvas,hues,isolate_channel,color_map,function canvas_1_binding($$value){binding_callbacks[$$value?\"unshift\":\"push\"]((()=>{canvas=$$value,$$invalidate(4,canvas)}))}]}const ArrayImage_svelte=class ArrayImage extends SvelteComponent{constructor(options){super(),document.getElementById(\"svelte-1vzuzhc-style\")||function add_css(){var style=internal_element(\"style\");style.id=\"svelte-1vzuzhc-style\",style.textContent=\".container.svelte-1vzuzhc.svelte-1vzuzhc{position:relative;border:1px solid #aaa}.container.svelte-1vzuzhc>.svelte-1vzuzhc{position:absolute;width:100%;left:0px}.container.svelte-1vzuzhc canvas.svelte-1vzuzhc{top:0px;height:100%;image-rendering:pixelated}.container.svelte-1vzuzhc .focus-top.svelte-1vzuzhc,.container.svelte-1vzuzhc .focus-bottom.svelte-1vzuzhc{background:#aaa;opacity:0.3}.container.svelte-1vzuzhc .focus-top.svelte-1vzuzhc{top:0px}.container.svelte-1vzuzhc .focus-bottom.svelte-1vzuzhc{bottom:0px}\",append(document.head,style)}(),init(this,options,instance,create_fragment,safe_not_equal,{array:0,width:1,height:2,hues:5,focus_token:3,isolate_channel:6,color_map:7})}};function soft_update(v,val){return\"soft\"==v.mode&&(v.value=val),v}function hard_toggle_update(v,val){if(\"soft\"==v.mode)v.value=val,v.mode=\"hard\";else{if(\"hard\"!=v.mode||v.value==val)return function unset(v){return v.value=void 0,v.mode=\"soft\",v}(v);v.value=val}return v}function LockableValueToggle_svelte_create_fragment(ctx){let div,current,mounted,dispose;const default_slot_template=ctx[4].default,default_slot=function create_slot(definition,ctx,$$scope,fn){if(definition){const slot_ctx=get_slot_context(definition,ctx,$$scope,fn);return definition[0](slot_ctx)}}(default_slot_template,ctx,ctx[3],null);return{c(){div=internal_element(\"div\"),default_slot&&default_slot.c(),attr(div,\"style\",ctx[2])},m(target,anchor){insert(target,div,anchor),default_slot&&default_slot.m(div,null),current=!0,mounted||(dispose=[listen(div,\"mouseover\",ctx[5]),listen(div,\"click\",ctx[6]),listen(div,\"mouseout\",ctx[7])],mounted=!0)},p(ctx,[dirty]){default_slot&&default_slot.p&&8&dirty&&function update_slot(slot,slot_definition,ctx,$$scope,dirty,get_slot_changes_fn,get_slot_context_fn){const slot_changes=get_slot_changes(slot_definition,$$scope,dirty,get_slot_changes_fn);if(slot_changes){const slot_context=get_slot_context(slot_definition,ctx,$$scope,get_slot_context_fn);slot.p(slot_context,slot_changes)}}(default_slot,default_slot_template,ctx,ctx[3],dirty,null,null),(!current||4&dirty)&&attr(div,\"style\",ctx[2])},i(local){current||(transition_in(default_slot,local),current=!0)},o(local){transition_out(default_slot,local),current=!1},d(detaching){detaching&&detach(div),default_slot&&default_slot.d(detaching),mounted=!1,run_all(dispose)}}}function LockableValueToggle_svelte_instance($$self,$$props,$$invalidate){let{$$slots:slots={},$$scope}=$$props,{lock}=$$props,{set_value}=$$props,{style=\"\"}=$$props;return $$self.$$set=$$props=>{\"lock\"in $$props&&$$invalidate(0,lock=$$props.lock),\"set_value\"in $$props&&$$invalidate(1,set_value=$$props.set_value),\"style\"in $$props&&$$invalidate(2,style=$$props.style),\"$$scope\"in $$props&&$$invalidate(3,$$scope=$$props.$$scope)},[lock,set_value,style,$$scope,slots,()=>{$$invalidate(0,lock=soft_update(lock,set_value))},()=>{$$invalidate(0,lock=hard_toggle_update(lock,set_value))},()=>{$$invalidate(0,lock=soft_update(lock,void 0))}]}const LockableValueToggle_svelte=class LockableValueToggle extends SvelteComponent{constructor(options){super(),init(this,options,LockableValueToggle_svelte_instance,LockableValueToggle_svelte_create_fragment,safe_not_equal,{lock:0,set_value:1,style:2})}};function get_each_context(ctx,list,i){const child_ctx=ctx.slice();return child_ctx[32]=list[i],child_ctx[34]=i,child_ctx}function get_each_context_1(ctx,list,i){const child_ctx=ctx.slice();return child_ctx[35]=list[i],child_ctx}function create_if_block_4(ctx){let t0,t1,t2,t1_value=ctx[15][ctx[10]]+\"\";return{c(){t0=internal_text(\"(\"),t1=internal_text(t1_value),t2=internal_text(\")\")},m(target,anchor){insert(target,t0,anchor),insert(target,t1,anchor),insert(target,t2,anchor)},p(ctx,dirty){33792&dirty[0]&&t1_value!==(t1_value=ctx[15][ctx[10]]+\"\")&&set_data(t1,t1_value)},d(detaching){detaching&&detach(t0),detaching&&detach(t1),detaching&&detach(t2)}}}function create_if_block_3(ctx){let div,arrayimage,div_style_value,current;return arrayimage=new ArrayImage_svelte({props:{array:ctx[7],width:\"200\",height:\"200\",focus_token:ctx[9],isolate_channel:ctx[10]}}),{c(){div=internal_element(\"div\"),create_component(arrayimage.$$.fragment),attr(div,\"style\",div_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"\":\"display:none;\"))},m(target,anchor){insert(target,div,anchor),mount_component(arrayimage,div,null),current=!0},p(ctx,dirty){const arrayimage_changes={};128&dirty[0]&&(arrayimage_changes.array=ctx[7]),512&dirty[0]&&(arrayimage_changes.focus_token=ctx[9]),1024&dirty[0]&&(arrayimage_changes.isolate_channel=ctx[10]),arrayimage.$set(arrayimage_changes),(!current||2048&dirty[0]&&div_style_value!==(div_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"\":\"display:none;\")))&&attr(div,\"style\",div_style_value)},i(local){current||(transition_in(arrayimage.$$.fragment,local),current=!0)},o(local){transition_out(arrayimage.$$.fragment,local),current=!1},d(detaching){detaching&&detach(div),destroy_component(arrayimage)}}}function create_if_block_2(ctx){let div0,t1,div1,current,each_value_1=range(ctx[12].shape[2]),each_blocks=[];for(let i=0;i<each_value_1.length;i+=1)each_blocks[i]=create_each_block_1(get_each_context_1(ctx,each_value_1,i));const out=i=>transition_out(each_blocks[i],1,1,(()=>{each_blocks[i]=null}));return{c(){div0=internal_element(\"div\"),div0.textContent=\"Attention Heads (hover to focus, click to lock)\",t1=space(),div1=internal_element(\"div\");for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].c();attr(div0,\"class\",\"figcaption svelte-xqk9oe\"),set_style(div0,\"grid-column\",\"heads\"),attr(div1,\"class\",\"heads svelte-xqk9oe\")},m(target,anchor){insert(target,div0,anchor),insert(target,t1,anchor),insert(target,div1,anchor);for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].m(div1,null);current=!0},p(ctx,dirty){if(122566&dirty[0]){let i;for(each_value_1=range(ctx[12].shape[2]),i=0;i<each_value_1.length;i+=1){const child_ctx=get_each_context_1(ctx,each_value_1,i);each_blocks[i]?(each_blocks[i].p(child_ctx,dirty),transition_in(each_blocks[i],1)):(each_blocks[i]=create_each_block_1(child_ctx),each_blocks[i].c(),transition_in(each_blocks[i],1),each_blocks[i].m(div1,null))}for(group_outros(),i=each_value_1.length;i<each_blocks.length;i+=1)out(i);check_outros()}},i(local){if(!current){for(let i=0;i<each_value_1.length;i+=1)transition_in(each_blocks[i]);current=!0}},o(local){each_blocks=each_blocks.filter(Boolean);for(let i=0;i<each_blocks.length;i+=1)transition_out(each_blocks[i]);current=!1},d(detaching){detaching&&detach(div0),detaching&&detach(t1),detaching&&detach(div1),destroy_each(each_blocks,detaching)}}}function create_default_slot_1(ctx){let div1,arrayimage0,t0,div0,div1_style_value,t1,div3,arrayimage1,t2,div2,div3_style_value,t3,current,raw0_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\",raw1_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\";return arrayimage0=new ArrayImage_svelte({props:{array:ctx[7],width:\"60\",height:\"60\",isolate_channel:ctx[35]}}),arrayimage1=new ArrayImage_svelte({props:{array:ctx[6],width:\"60\",height:\"60\",isolate_channel:ctx[35]}}),{c(){div1=internal_element(\"div\"),create_component(arrayimage0.$$.fragment),t0=space(),div0=internal_element(\"div\"),t1=space(),div3=internal_element(\"div\"),create_component(arrayimage1.$$.fragment),t2=space(),div2=internal_element(\"div\"),t3=space(),attr(div0,\"class\",\"head-label svelte-xqk9oe\"),set_style(div0,\"background\",ctx[14][ctx[35]]),attr(div1,\"class\",\"head-icon svelte-xqk9oe\"),attr(div1,\"style\",div1_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"\":\"display:none;\")),attr(div2,\"class\",\"head-label svelte-xqk9oe\"),set_style(div2,\"background\",ctx[14][ctx[35]]),attr(div3,\"class\",\"head-icon svelte-xqk9oe\"),attr(div3,\"style\",div3_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"display:none;\":\"\"))},m(target,anchor){insert(target,div1,anchor),mount_component(arrayimage0,div1,null),append(div1,t0),append(div1,div0),div0.innerHTML=raw0_value,insert(target,t1,anchor),insert(target,div3,anchor),mount_component(arrayimage1,div3,null),append(div3,t2),append(div3,div2),div2.innerHTML=raw1_value,insert(target,t3,anchor),current=!0},p(ctx,dirty){const arrayimage0_changes={};128&dirty[0]&&(arrayimage0_changes.array=ctx[7]),4096&dirty[0]&&(arrayimage0_changes.isolate_channel=ctx[35]),arrayimage0.$set(arrayimage0_changes),(!current||36864&dirty[0])&&raw0_value!==(raw0_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\")&&(div0.innerHTML=raw0_value),(!current||20480&dirty[0])&&set_style(div0,\"background\",ctx[14][ctx[35]]),(!current||7684&dirty[0]&&div1_style_value!==(div1_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"\":\"display:none;\")))&&attr(div1,\"style\",div1_style_value);const arrayimage1_changes={};64&dirty[0]&&(arrayimage1_changes.array=ctx[6]),4096&dirty[0]&&(arrayimage1_changes.isolate_channel=ctx[35]),arrayimage1.$set(arrayimage1_changes),(!current||36864&dirty[0])&&raw1_value!==(raw1_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\")&&(div2.innerHTML=raw1_value),(!current||20480&dirty[0])&&set_style(div2,\"background\",ctx[14][ctx[35]]),(!current||7684&dirty[0]&&div3_style_value!==(div3_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"display:none;\":\"\")))&&attr(div3,\"style\",div3_style_value)},i(local){current||(transition_in(arrayimage0.$$.fragment,local),transition_in(arrayimage1.$$.fragment,local),current=!0)},o(local){transition_out(arrayimage0.$$.fragment,local),transition_out(arrayimage1.$$.fragment,local),current=!1},d(detaching){detaching&&detach(div1),destroy_component(arrayimage0),detaching&&detach(t1),detaching&&detach(div3),destroy_component(arrayimage1),detaching&&detach(t3)}}}function create_each_block_1(ctx){let lockablevaluetoggle,updating_lock,current;function lockablevaluetoggle_lock_binding(value){ctx[22].call(null,value)}let lockablevaluetoggle_props={set_value:ctx[35],$$slots:{default:[create_default_slot_1]},$$scope:{ctx}};return void 0!==ctx[1]&&(lockablevaluetoggle_props.lock=ctx[1]),lockablevaluetoggle=new LockableValueToggle_svelte({props:lockablevaluetoggle_props}),binding_callbacks.push((()=>bind(lockablevaluetoggle,\"lock\",lockablevaluetoggle_lock_binding))),{c(){create_component(lockablevaluetoggle.$$.fragment)},m(target,anchor){mount_component(lockablevaluetoggle,target,anchor),current=!0},p(ctx,dirty){const lockablevaluetoggle_changes={};4096&dirty[0]&&(lockablevaluetoggle_changes.set_value=ctx[35]),57028&dirty[0]|128&dirty[1]&&(lockablevaluetoggle_changes.$$scope={dirty,ctx}),!updating_lock&&2&dirty[0]&&(updating_lock=!0,lockablevaluetoggle_changes.lock=ctx[1],add_flush_callback((()=>updating_lock=!1))),lockablevaluetoggle.$set(lockablevaluetoggle_changes)},i(local){current||(transition_in(lockablevaluetoggle.$$.fragment,local),current=!0)},o(local){transition_out(lockablevaluetoggle.$$.fragment,local),current=!1},d(detaching){destroy_component(lockablevaluetoggle,detaching)}}}function AttentionMulti_svelte_create_if_block(ctx){let div3,div0,t1,div1,t2,div2,nobr,input,t3,span,t4,b,t5,t6,current,mounted,dispose,t5_value=ctx[2]?\"target\":\"source\",each_value=ctx[5],each_blocks=[];for(let i=0;i<each_value.length;i+=1)each_blocks[i]=create_each_block(get_each_context(ctx,each_value,i));const out=i=>transition_out(each_blocks[i],1,1,(()=>{each_blocks[i]=null}));let if_block=void 0!==ctx[7]&&create_if_block_1(ctx);return{c(){div3=internal_element(\"div\"),div0=internal_element(\"div\"),div0.textContent=\"Tokens (hover to focus, click to lock)\",t1=space(),div1=internal_element(\"div\");for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].c();t2=space(),div2=internal_element(\"div\"),nobr=internal_element(\"nobr\"),input=internal_element(\"input\"),t3=space(),span=internal_element(\"span\"),t4=internal_text(\"Selected is\\n            \"),b=internal_element(\"b\"),t5=internal_text(t5_value),t6=space(),if_block&&if_block.c(),attr(div0,\"class\",\"figcaption svelte-xqk9oe\"),set_style(div0,\"grid-column\",\"left\"),attr(div1,\"class\",\"tokens svelte-xqk9oe\"),attr(input,\"class\",\"hover-mode svelte-xqk9oe\"),attr(input,\"type\",\"checkbox\"),attr(span,\"class\",\"hover-mode-text svelte-xqk9oe\"),set_style(span,\"white-space\",\"nowrap\"),attr(div2,\"class\",\"toggle\"),attr(div3,\"class\",\"tokens-container svelte-xqk9oe\")},m(target,anchor){insert(target,div3,anchor),append(div3,div0),append(div3,t1),append(div3,div1);for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].m(div1,null);append(div3,t2),append(div3,div2),append(div2,nobr),append(nobr,input),input.checked=ctx[2],append(nobr,t3),append(nobr,span),append(span,t4),append(span,b),append(b,t5),append(nobr,t6),if_block&&if_block.m(nobr,null),current=!0,mounted||(dispose=[listen(input,\"change\",ctx[24]),listen(span,\"click\",ctx[25])],mounted=!0)},p(ctx,dirty){if(561&dirty[0]){let i;for(each_value=ctx[5],i=0;i<each_value.length;i+=1){const child_ctx=get_each_context(ctx,each_value,i);each_blocks[i]?(each_blocks[i].p(child_ctx,dirty),transition_in(each_blocks[i],1)):(each_blocks[i]=create_each_block(child_ctx),each_blocks[i].c(),transition_in(each_blocks[i],1),each_blocks[i].m(div1,null))}for(group_outros(),i=each_value.length;i<each_blocks.length;i+=1)out(i);check_outros()}4&dirty[0]&&(input.checked=ctx[2]),(!current||4&dirty[0])&&t5_value!==(t5_value=ctx[2]?\"target\":\"source\")&&set_data(t5,t5_value),void 0!==ctx[7]?if_block?if_block.p(ctx,dirty):(if_block=create_if_block_1(ctx),if_block.c(),if_block.m(nobr,null)):if_block&&(if_block.d(1),if_block=null)},i(local){if(!current){for(let i=0;i<each_value.length;i+=1)transition_in(each_blocks[i]);current=!0}},o(local){each_blocks=each_blocks.filter(Boolean);for(let i=0;i<each_blocks.length;i+=1)transition_out(each_blocks[i]);current=!1},d(detaching){detaching&&detach(div3),destroy_each(each_blocks,detaching),if_block&&if_block.d(),mounted=!1,run_all(dispose)}}}function create_default_slot(ctx){let span,t,span_class_value,t_value=ctx[32]+\"\";return{c(){span=internal_element(\"span\"),t=internal_text(t_value),attr(span,\"class\",span_class_value=\"token \"+(ctx[34]==ctx[9]?\"selected\":\"\")+\" svelte-xqk9oe\"),set_style(span,\"background\",ctx[4][ctx[34]])},m(target,anchor){insert(target,span,anchor),append(span,t)},p(ctx,dirty){32&dirty[0]&&t_value!==(t_value=ctx[32]+\"\")&&set_data(t,t_value),512&dirty[0]&&span_class_value!==(span_class_value=\"token \"+(ctx[34]==ctx[9]?\"selected\":\"\")+\" svelte-xqk9oe\")&&attr(span,\"class\",span_class_value),16&dirty[0]&&set_style(span,\"background\",ctx[4][ctx[34]])},d(detaching){detaching&&detach(span)}}}function create_each_block(ctx){let lockablevaluetoggle,updating_lock,current;function lockablevaluetoggle_lock_binding_1(value){ctx[23].call(null,value)}let lockablevaluetoggle_props={set_value:ctx[34],style:\"display: inline\",$$slots:{default:[create_default_slot]},$$scope:{ctx}};return void 0!==ctx[0]&&(lockablevaluetoggle_props.lock=ctx[0]),lockablevaluetoggle=new LockableValueToggle_svelte({props:lockablevaluetoggle_props}),binding_callbacks.push((()=>bind(lockablevaluetoggle,\"lock\",lockablevaluetoggle_lock_binding_1))),{c(){create_component(lockablevaluetoggle.$$.fragment)},m(target,anchor){mount_component(lockablevaluetoggle,target,anchor),current=!0},p(ctx,dirty){const lockablevaluetoggle_changes={};560&dirty[0]|128&dirty[1]&&(lockablevaluetoggle_changes.$$scope={dirty,ctx}),!updating_lock&&1&dirty[0]&&(updating_lock=!0,lockablevaluetoggle_changes.lock=ctx[0],add_flush_callback((()=>updating_lock=!1))),lockablevaluetoggle.$set(lockablevaluetoggle_changes)},i(local){current||(transition_in(lockablevaluetoggle.$$.fragment,local),current=!0)},o(local){transition_out(lockablevaluetoggle.$$.fragment,local),current=!1},d(detaching){destroy_component(lockablevaluetoggle,detaching)}}}function create_if_block_1(ctx){let input,t0,span,t1,b,t2,mounted,dispose,t2_value=ctx[3]?\"info-weighted\":\"unmodified\";return{c(){input=internal_element(\"input\"),t0=space(),span=internal_element(\"span\"),t1=internal_text(\"Attention is\\n            \"),b=internal_element(\"b\"),t2=internal_text(t2_value),attr(input,\"class\",\"info-mode svelte-xqk9oe\"),attr(input,\"type\",\"checkbox\"),attr(span,\"class\",\"info-mode-text svelte-xqk9oe\"),set_style(span,\"white-space\",\"nowrap\")},m(target,anchor){insert(target,input,anchor),input.checked=ctx[3],insert(target,t0,anchor),insert(target,span,anchor),append(span,t1),append(span,b),append(b,t2),mounted||(dispose=[listen(input,\"change\",ctx[26]),listen(span,\"click\",ctx[27])],mounted=!0)},p(ctx,dirty){8&dirty[0]&&(input.checked=ctx[3]),8&dirty[0]&&t2_value!==(t2_value=ctx[3]?\"info-weighted\":\"unmodified\")&&set_data(t2,t2_value)},d(detaching){detaching&&detach(input),detaching&&detach(t0),detaching&&detach(span),mounted=!1,run_all(dispose)}}}function AttentionMulti_svelte_create_fragment(ctx){let div2,div0,t0,t1,t2,div1,arrayimage,div1_style_value,t3,t4,if_block3_anchor,current,if_block0=null!=ctx[10]&&create_if_block_4(ctx),if_block1=void 0!==ctx[7]&&create_if_block_3(ctx);arrayimage=new ArrayImage_svelte({props:{array:ctx[6],width:\"200\",height:\"200\",focus_token:ctx[9],isolate_channel:ctx[10]}});let if_block2=ctx[13]>1&&create_if_block_2(ctx),if_block3=ctx[8]&&AttentionMulti_svelte_create_if_block(ctx);return{c(){div2=internal_element(\"div\"),div0=internal_element(\"div\"),t0=internal_text(\"Attention Pattern\\n        \"),if_block0&&if_block0.c(),t1=space(),if_block1&&if_block1.c(),t2=space(),div1=internal_element(\"div\"),create_component(arrayimage.$$.fragment),t3=space(),if_block2&&if_block2.c(),t4=space(),if_block3&&if_block3.c(),if_block3_anchor=function empty(){return internal_text(\"\")}(),attr(div0,\"class\",\"figcaption svelte-xqk9oe\"),set_style(div0,\"grid-column\",\"big-attn\"),attr(div1,\"style\",div1_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"display:none\":\"\")),attr(div2,\"class\",\"attn-container svelte-xqk9oe\")},m(target,anchor){insert(target,div2,anchor),append(div2,div0),append(div0,t0),if_block0&&if_block0.m(div0,null),append(div2,t1),if_block1&&if_block1.m(div2,null),append(div2,t2),append(div2,div1),mount_component(arrayimage,div1,null),append(div2,t3),if_block2&&if_block2.m(div2,null),insert(target,t4,anchor),if_block3&&if_block3.m(target,anchor),insert(target,if_block3_anchor,anchor),current=!0},p(ctx,dirty){null!=ctx[10]?if_block0?if_block0.p(ctx,dirty):(if_block0=create_if_block_4(ctx),if_block0.c(),if_block0.m(div0,null)):if_block0&&(if_block0.d(1),if_block0=null),void 0!==ctx[7]?if_block1?(if_block1.p(ctx,dirty),128&dirty[0]&&transition_in(if_block1,1)):(if_block1=create_if_block_3(ctx),if_block1.c(),transition_in(if_block1,1),if_block1.m(div2,t2)):if_block1&&(group_outros(),transition_out(if_block1,1,1,(()=>{if_block1=null})),check_outros());const arrayimage_changes={};64&dirty[0]&&(arrayimage_changes.array=ctx[6]),512&dirty[0]&&(arrayimage_changes.focus_token=ctx[9]),1024&dirty[0]&&(arrayimage_changes.isolate_channel=ctx[10]),arrayimage.$set(arrayimage_changes),(!current||2048&dirty[0]&&div1_style_value!==(div1_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"display:none\":\"\")))&&attr(div1,\"style\",div1_style_value),ctx[13]>1?if_block2?(if_block2.p(ctx,dirty),8192&dirty[0]&&transition_in(if_block2,1)):(if_block2=create_if_block_2(ctx),if_block2.c(),transition_in(if_block2,1),if_block2.m(div2,null)):if_block2&&(group_outros(),transition_out(if_block2,1,1,(()=>{if_block2=null})),check_outros()),ctx[8]?if_block3?(if_block3.p(ctx,dirty),256&dirty[0]&&transition_in(if_block3,1)):(if_block3=AttentionMulti_svelte_create_if_block(ctx),if_block3.c(),transition_in(if_block3,1),if_block3.m(if_block3_anchor.parentNode,if_block3_anchor)):if_block3&&(group_outros(),transition_out(if_block3,1,1,(()=>{if_block3=null})),check_outros())},i(local){current||(transition_in(if_block1),transition_in(arrayimage.$$.fragment,local),transition_in(if_block2),transition_in(if_block3),current=!0)},o(local){transition_out(if_block1),transition_out(arrayimage.$$.fragment,local),transition_out(if_block2),transition_out(if_block3),current=!1},d(detaching){detaching&&detach(div2),if_block0&&if_block0.d(),if_block1&&if_block1.d(),destroy_component(arrayimage),if_block2&&if_block2.d(),detaching&&detach(t4),if_block3&&if_block3.d(detaching),detaching&&detach(if_block3_anchor)}}}function range(n){return[...Array(n).keys()]}function reduce_Y(arr){if(void 0!==arr){for(var arr_=[],x=0;x<arr.shape[0];x++){arr_.push([]);for(var c=0;c<arr.shape[2];c++){for(var temp=0,y=0;y<arr.shape[1];y++)temp=Math.max(temp,arr.pick(x,y,c));arr_[x].push(temp)}}return arr_}}function reduce_X(arr){if(void 0!==arr){for(var arr_=[],y=0;y<arr.shape[0];y++){arr_.push([]);for(var c=0;c<arr.shape[2];c++){for(var temp=0,x=0;x<arr.shape[1];x++)temp=Math.max(temp,arr.pick(x,y,c));arr_[y].push(temp)}}return arr_}}function AttentionMulti_svelte_instance($$self,$$props,$$invalidate){let focus_token,focus_head,show_info_weighted,attention_show,attention_reduce_dst_unmodified,attention_reduce_src_unmodified,attention_reduce_dst_info_weighted,attention_reduce_src_info_weighted,attention_reduce_dst,attention_reduce_src,N_heads,colors,head_labels_,{tokens}=$$props,{attention}=$$props,{info_weighted}=$$props,{head_labels}=$$props,{show_tokens=!0}=$$props,{focus_token_lock={value:void 0,mode:\"soft\"}}=$$props,{focus_head_lock={value:void 0,mode:\"soft\"}}=$$props,{hover_token_is_target=!1}=$$props,{_show_info_weighted=!1}=$$props;function token_color(attention,focus_token_value,tok_i,isolate_channel,hover_token_is_target){if(null==focus_token_value)return sparse_color_map_css((hover_token_is_target?attention_reduce_src:attention_reduce_dst)[tok_i],void 0,isolate_channel);let tok_from,tok_to;hover_token_is_target?(tok_from=tok_i,tok_to=focus_token_value):(tok_from=focus_token_value,tok_to=tok_i);let color=function get_color(array,x,y,isolate_channel){return x<y?\"#FFF\":sparse_color_map_css(array.pick(x,y,null),void 0,isolate_channel)}(attention,tok_from,tok_to,isolate_channel);return\"#FFF\"===color&&(color=\"#DDD\"),color}let{all_token_colors}=$$props;return $$self.$$set=$$props=>{\"tokens\"in $$props&&$$invalidate(5,tokens=$$props.tokens),\"attention\"in $$props&&$$invalidate(6,attention=$$props.attention),\"info_weighted\"in $$props&&$$invalidate(7,info_weighted=$$props.info_weighted),\"head_labels\"in $$props&&$$invalidate(17,head_labels=$$props.head_labels),\"show_tokens\"in $$props&&$$invalidate(8,show_tokens=$$props.show_tokens),\"focus_token_lock\"in $$props&&$$invalidate(0,focus_token_lock=$$props.focus_token_lock),\"focus_head_lock\"in $$props&&$$invalidate(1,focus_head_lock=$$props.focus_head_lock),\"hover_token_is_target\"in $$props&&$$invalidate(2,hover_token_is_target=$$props.hover_token_is_target),\"_show_info_weighted\"in $$props&&$$invalidate(3,_show_info_weighted=$$props._show_info_weighted),\"all_token_colors\"in $$props&&$$invalidate(4,all_token_colors=$$props.all_token_colors)},$$self.$$.update=()=>{1&$$self.$$.dirty[0]&&$$invalidate(9,focus_token=focus_token_lock.value),2&$$self.$$.dirty[0]&&$$invalidate(10,focus_head=focus_head_lock.value),136&$$self.$$.dirty[0]&&$$invalidate(11,show_info_weighted=_show_info_weighted&&void 0!==info_weighted),2240&$$self.$$.dirty[0]&&$$invalidate(12,attention_show=show_info_weighted?info_weighted:attention),4096&$$self.$$.dirty[0]&&(window.attention_show=attention_show),64&$$self.$$.dirty[0]&&$$invalidate(18,attention_reduce_dst_unmodified=reduce_Y(attention)),64&$$self.$$.dirty[0]&&$$invalidate(19,attention_reduce_src_unmodified=reduce_X(attention)),128&$$self.$$.dirty[0]&&$$invalidate(20,attention_reduce_dst_info_weighted=reduce_Y(info_weighted)),128&$$self.$$.dirty[0]&&$$invalidate(21,attention_reduce_src_info_weighted=reduce_X(info_weighted)),1312768&$$self.$$.dirty[0]&&(attention_reduce_dst=show_info_weighted?attention_reduce_dst_info_weighted:attention_reduce_dst_unmodified),2623488&$$self.$$.dirty[0]&&(attention_reduce_src=show_info_weighted?attention_reduce_src_info_weighted:attention_reduce_src_unmodified),64&$$self.$$.dirty[0]&&$$invalidate(13,N_heads=attention.shape[2]),8192&$$self.$$.dirty[0]&&$$invalidate(14,colors=range(N_heads).map((i=>sparse_color_map_css(range(N_heads).map((x=>1)),void 0,i)))),139264&$$self.$$.dirty[0]&&$$invalidate(15,head_labels_=null!=head_labels?head_labels:range(N_heads)),5668&$$self.$$.dirty[0]&&$$invalidate(4,all_token_colors=range(tokens.length).map((i=>token_color(attention_show,focus_token,i,focus_head,hover_token_is_target))))},[focus_token_lock,focus_head_lock,hover_token_is_target,_show_info_weighted,all_token_colors,tokens,attention,info_weighted,show_tokens,focus_token,focus_head,show_info_weighted,attention_show,N_heads,colors,head_labels_,function head_intensity(head_i,focus_token_value,hover_token_is_target){if(null==focus_token_value)var v=1;else{var reduced_attn=hover_token_is_target?attention_reduce_src:attention_reduce_dst;v=Math.max(0,Math.min(1,reduced_attn[focus_token_value][head_i]))}return\"\"+v},head_labels,attention_reduce_dst_unmodified,attention_reduce_src_unmodified,attention_reduce_dst_info_weighted,attention_reduce_src_info_weighted,function lockablevaluetoggle_lock_binding(value){focus_head_lock=value,$$invalidate(1,focus_head_lock)},function lockablevaluetoggle_lock_binding_1(value){focus_token_lock=value,$$invalidate(0,focus_token_lock)},function input_change_handler(){hover_token_is_target=this.checked,$$invalidate(2,hover_token_is_target)},()=>$$invalidate(2,hover_token_is_target^=!0),function input_change_handler_1(){_show_info_weighted=this.checked,$$invalidate(3,_show_info_weighted)},()=>$$invalidate(3,_show_info_weighted^=!0)]}const AttentionMulti_svelte=class AttentionMulti extends SvelteComponent{constructor(options){super(),document.getElementById(\"svelte-xqk9oe-style\")||function AttentionMulti_svelte_add_css(){var style=internal_element(\"style\");style.id=\"svelte-xqk9oe-style\",style.textContent=\".attn-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[big-attn] min-content [heads] minmax(min-content, 624px);gap:12px}.figcaption.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;white-space:nowrap}.tokens-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[left] min-content [right] minmax(min-content, 800px) [end];gap:12px;margin-top:24px;color:white}.tokens.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-row:main;grid-column-start:left;grid-column-end:end;cursor:pointer;height:min-content;line-height:110%}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe{white-space:pre-wrap}.tokens.svelte-xqk9oe .selected.svelte-xqk9oe.svelte-xqk9oe{border:1px solid #999;z-index:10}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe:not(.selected){z-index:0;padding:1px}.hover-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;grid-column:settings;cursor:pointer}.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{margin-right:8px}.heads.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-column:heads;grid-row:main;display:flex;flex-direction:row;flex-wrap:wrap;gap:6px;height:min-content}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe.svelte-xqk9oe{position:relative;width:62px;height:62px}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe>.svelte-xqk9oe{position:absolute;right:0px;top:0px}.heads.svelte-xqk9oe .head-icon .head-label.svelte-xqk9oe.svelte-xqk9oe{background:#333;color:#eee;font-size:65%;padding:1px;border-bottom-left-radius:2px;padding-left:4px;padding-right:2px;min-width:14px;opacity:0.75}\",append(document.head,style)}(),init(this,options,AttentionMulti_svelte_instance,AttentionMulti_svelte_create_fragment,safe_not_equal,{tokens:5,attention:6,info_weighted:7,head_labels:17,show_tokens:8,focus_token_lock:0,focus_head_lock:1,hover_token_is_target:2,_show_info_weighted:3,all_token_colors:4},[-1,-1])}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(143)})().default;</script>\n",
       "        \n",
       "        <div id=\"AttentionMulti_3ea3db3\"></div>\n",
       "        <script>\n",
       "        ( () => {\n",
       "            var data = {\n",
       "\"tokens\": [\n",
       "\"<|endoftext|>\",\n",
       "\"I\",\n",
       "\" am\",\n",
       "\" an\",\n",
       "\" amazing\",\n",
       "\" aut\",\n",
       "\"ore\",\n",
       "\"gressive\",\n",
       "\",\",\n",
       "\" dec\",\n",
       "\"oder\",\n",
       "\"-\",\n",
       "\"only\",\n",
       "\",\",\n",
       "\" G\",\n",
       "\"PT\",\n",
       "\"-\",\n",
       "\"2\",\n",
       "\" style\",\n",
       "\" transformer\",\n",
       "\".\",\n",
       "\" One\",\n",
       "\" day\",\n",
       "\" I\",\n",
       "\" will\",\n",
       "\" exceed\",\n",
       "\" human\",\n",
       "\" level\",\n",
       "\" intelligence\",\n",
       "\" and\",\n",
       "\" take\",\n",
       "\" over\",\n",
       "\" the\",\n",
       "\" world\",\n",
       "\"!\"\n",
       "],\n",
       "\"attention\": {\n",
       "\"__type__\": \"npy\",\n",
       "\"zdata\": \"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\",\n",
       "\"min\": 0.0,\n",
       "\"max\": 1.0\n",
       "}\n",
       "};\n",
       "            data = loader.unpack_obj(data);\n",
       "            window.AttentionMulti_data = data;\n",
       "            var AttentionMulti_inst = new AttentionMulti({\n",
       "                \"target\": document.getElementById(\"AttentionMulti_3ea3db3\"),\n",
       "                \"props\": data\n",
       "                });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<pysvelte.html.Html at 0x7fa397a59ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pysvelte\n",
    "pysvelte.AttentionMulti(tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache['blocks.0.attn.hook_attn'][0].permute(1, 2, 0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9663e-01,  1.6986e-02,  3.4781e-02,  ...,  3.3120e-02,\n",
       "          -2.3129e-02,  1.8103e-01],\n",
       "         [ 1.3167e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1997e-03,\n",
       "           5.3076e-03,  1.3511e-01],\n",
       "         [ 8.9738e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
       "           2.7959e-03,  9.0785e-02],\n",
       "         ...,\n",
       "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
       "          -4.9522e-04, -8.6008e-03],\n",
       "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6688e-02,\n",
       "          -1.4368e-02,  3.3245e-02],\n",
       "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
       "          -8.9028e-03,  4.8796e-02]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"blocks.0.ln1.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
       "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
       "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
       "         ...,\n",
       "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
       "         [-0.0862,  0.7839,  0.9046,  ..., -0.2175, -0.5953,  0.8555],\n",
       "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "    \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3911,  0.1543,  0.6005,  ...,  1.7198,  1.7365,  0.3930],\n",
       "         [-0.9039, -0.0360,  0.2351,  ..., -0.4148,  0.3562,  0.3936],\n",
       "         [-0.9647, -2.4819, -1.4995,  ...,  1.4046,  0.7616,  0.5918],\n",
       "         ...,\n",
       "         [-0.7421,  0.9251, -0.3218,  ...,  0.2921,  0.1097, -0.5344],\n",
       "         [-1.3221,  0.8960,  1.1795,  ..., -0.5544, -0.4071,  0.9255],\n",
       "         [ 1.1209, -0.8919,  1.3737,  ..., -0.1356,  0.3434,  0.4517]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized_resid_final: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized_resid_final: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0878,  -54.3452,\n",
       "           -42.3645],\n",
       "         [-128.0392, -127.9936, -130.7010,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8820,  ..., -128.5181, -126.6027,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7749, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6724, -104.4888, -108.7361,  ..., -118.3552, -113.8766,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "    \n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Tokens: torch.Size([2, 4])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_final: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "Embeddings: torch.Size([1, 35, 768])\n",
      "Tokens: torch.Size([1, 35])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_final: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0878,  -54.3452,\n",
       "           -42.3645],\n",
       "         [-128.0392, -127.9936, -130.7010,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8820,  ..., -128.5181, -126.6027,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7749, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6724, -104.4888, -108.7361,  ..., -118.3552, -113.8766,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoTransformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False))\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "demo_gpt2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a test string - the intro paragraph of today's featured Wikipedia article. Let's calculate the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"Mini scule is a species of microhylid frog endemic to Madagascar that was described in 2019. The scientific name of the species refers to its size, being a pun on the word minuscule. It is very small, measuring only 8.4 to 10.8 mm (0.33 to 0.43 in) in snout–vent length. It has bronze underparts with a brown groin and back of the thigh, cream upperparts with brown flecking, a dark brown side of the head, and a red iris. On the hind feet, the first toe is absent and the second and fifth toes are strongly reduced. The frog is known only from the Sainte Luce Reserve, where it inhabits areas with deep leaf litter near semi-permanent water bodies. Specimens of frogs from Mandena, the Vohimena mountains, the southern Anosy Mountains, and Tsitongambarika may also be of this species. Along with Mini mum and Mini ature, the other two species in its genus, it received media attention when first described due to the wordplay in its scientific name. (Full article...)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
    "demo_logits = demo_gpt2(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7186, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Loss as average prob tensor(0.0243, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Loss as 'uniform over this many variables' tensor(41.2079, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Uniform loss over the vocab 10.82490511970208\n"
     ]
    }
   ],
   "source": [
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()\n",
    "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
    "print(loss)\n",
    "print(\"Loss as average prob\", (-loss).exp())\n",
    "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also greedily generate text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac70024b344ea787bd2f70c4f9b69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on\"\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model!\n",
    "\n",
    "This is a lightweight demonstration of how you can actually train your own GPT-2 with this code! Here we train a tiny model on a tiny dataset, but it's fundamentally the same code for training a larger/more real model (though you'll need beefier GPUs and data parallelism to do it remotely efficiently, and fancier parallelism for much bigger ones).\n",
    "\n",
    "For our purposes, we'll train 2L 4 heads per layer model, with context length 256, for 1000 steps of batch size 8, just to show what it looks like (and so the notebook doesn't melt your colab lol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install datasets\n",
    "    %pip install transformers\n",
    "import datasets\n",
    "import transformers\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1000\n",
    "log_every = 10\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create Data\n",
    "\n",
    "We load in a tiny dataset I made, with the first 10K entries in the Pile (inspired by Stas' version for OpenWebText!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using custom data configuration NeelNanda--pile-10k-30e9c14cdcda6c5c\n",
      "Reusing dataset parquet (/workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-30e9c14cdcda6c5c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-30e9c14cdcda6c5c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5a7efe79fba39fa4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-30e9c14cdcda6c5c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5d90b6c801666252.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-30e9c14cdcda6c5c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-63f545415985c072.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-30e9c14cdcda6c5c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e1d3c0b4a6cf8c82.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "print(dataset)\n",
    "print(dataset[0]['text'][:100])\n",
    "tokens_dataset = tokenize_and_concatenate(dataset, reference_gpt2.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoTransformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DemoTransformer(model_cfg)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer\n",
    "We use AdamW - it's a pretty standard optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 8506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd43306e46843b1abcafea463782a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 10.8938\n",
      "Step: 10, Loss: 8.6160\n",
      "Step: 20, Loss: 7.4170\n",
      "Step: 30, Loss: 7.4342\n",
      "Step: 40, Loss: 7.1660\n",
      "Step: 50, Loss: 5.9145\n",
      "Step: 60, Loss: 7.3014\n",
      "Step: 70, Loss: 7.7666\n",
      "Step: 80, Loss: 7.5511\n",
      "Step: 90, Loss: 6.2643\n",
      "Step: 100, Loss: 7.1839\n",
      "Step: 110, Loss: 5.3938\n",
      "Step: 120, Loss: 7.3348\n",
      "Step: 130, Loss: 5.9235\n",
      "Step: 140, Loss: 6.0241\n",
      "Step: 150, Loss: 6.5177\n",
      "Step: 160, Loss: 7.0355\n",
      "Step: 170, Loss: 6.0351\n",
      "Step: 180, Loss: 6.7511\n",
      "Step: 190, Loss: 5.9056\n",
      "Step: 200, Loss: 6.2145\n",
      "Step: 210, Loss: 5.7370\n",
      "Step: 220, Loss: 6.9411\n",
      "Step: 230, Loss: 6.4263\n",
      "Step: 240, Loss: 6.4935\n",
      "Step: 250, Loss: 6.3181\n",
      "Step: 260, Loss: 6.4933\n",
      "Step: 270, Loss: 5.8911\n",
      "Step: 280, Loss: 6.8380\n",
      "Step: 290, Loss: 7.1255\n",
      "Step: 300, Loss: 6.5911\n",
      "Step: 310, Loss: 5.8859\n",
      "Step: 320, Loss: 6.9126\n",
      "Step: 330, Loss: 6.5967\n",
      "Step: 340, Loss: 6.7064\n",
      "Step: 350, Loss: 6.1962\n",
      "Step: 360, Loss: 5.7562\n",
      "Step: 370, Loss: 5.9538\n",
      "Step: 380, Loss: 6.9621\n",
      "Step: 390, Loss: 7.1926\n",
      "Step: 400, Loss: 6.2406\n",
      "Step: 410, Loss: 6.8793\n",
      "Step: 420, Loss: 6.7920\n",
      "Step: 430, Loss: 5.6676\n",
      "Step: 440, Loss: 6.4836\n",
      "Step: 450, Loss: 5.8379\n",
      "Step: 460, Loss: 5.0805\n",
      "Step: 470, Loss: 5.3195\n",
      "Step: 480, Loss: 5.8577\n",
      "Step: 490, Loss: 6.0510\n",
      "Step: 500, Loss: 5.3045\n",
      "Step: 510, Loss: 5.6220\n",
      "Step: 520, Loss: 5.6595\n",
      "Step: 530, Loss: 5.1026\n",
      "Step: 540, Loss: 6.2747\n",
      "Step: 550, Loss: 6.5637\n",
      "Step: 560, Loss: 5.9919\n",
      "Step: 570, Loss: 4.4590\n",
      "Step: 580, Loss: 5.7116\n",
      "Step: 590, Loss: 6.4498\n",
      "Step: 600, Loss: 6.2833\n",
      "Step: 610, Loss: 6.0663\n",
      "Step: 620, Loss: 6.1303\n",
      "Step: 630, Loss: 6.0391\n",
      "Step: 640, Loss: 5.9534\n",
      "Step: 650, Loss: 5.8069\n",
      "Step: 660, Loss: 5.7637\n",
      "Step: 670, Loss: 6.2510\n",
      "Step: 680, Loss: 6.7757\n",
      "Step: 690, Loss: 5.8833\n",
      "Step: 700, Loss: 5.6237\n",
      "Step: 710, Loss: 5.9463\n",
      "Step: 720, Loss: 5.9818\n",
      "Step: 730, Loss: 3.7111\n",
      "Step: 740, Loss: 6.2097\n",
      "Step: 750, Loss: 6.3188\n",
      "Step: 760, Loss: 5.4198\n",
      "Step: 770, Loss: 3.9056\n",
      "Step: 780, Loss: 5.8297\n",
      "Step: 790, Loss: 6.2319\n",
      "Step: 800, Loss: 5.4723\n",
      "Step: 810, Loss: 5.5568\n",
      "Step: 820, Loss: 6.2107\n",
      "Step: 830, Loss: 6.5033\n",
      "Step: 840, Loss: 5.5697\n",
      "Step: 850, Loss: 5.6075\n",
      "Step: 860, Loss: 5.4451\n",
      "Step: 870, Loss: 6.1277\n",
      "Step: 880, Loss: 6.1290\n",
      "Step: 890, Loss: 5.2920\n",
      "Step: 900, Loss: 3.9140\n",
      "Step: 910, Loss: 6.3699\n",
      "Step: 920, Loss: 4.8495\n",
      "Step: 930, Loss: 5.8845\n",
      "Step: 940, Loss: 3.8847\n",
      "Step: 950, Loss: 6.1746\n",
      "Step: 960, Loss: 6.6594\n",
      "Step: 970, Loss: 4.5531\n",
      "Step: 980, Loss: 5.2382\n",
      "Step: 990, Loss: 6.6860\n",
      "Step: 1000, Loss: 5.7797\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "print(\"Number of batches:\", len(data_loader))\n",
    "for epoch in range(num_epochs):\n",
    "    for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "        tokens = batch['tokens'].cuda()\n",
    "        logits = model(tokens)\n",
    "        loss = lm_cross_entropy_loss(logits, tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if c % log_every == 0:\n",
    "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "        if c > max_steps:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a loss curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Tokens=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          0,
          2048,
          4096,
          6144,
          8192,
          10240,
          12288,
          14336,
          16384,
          18432,
          20480,
          22528,
          24576,
          26624,
          28672,
          30720,
          32768,
          34816,
          36864,
          38912,
          40960,
          43008,
          45056,
          47104,
          49152,
          51200,
          53248,
          55296,
          57344,
          59392,
          61440,
          63488,
          65536,
          67584,
          69632,
          71680,
          73728,
          75776,
          77824,
          79872,
          81920,
          83968,
          86016,
          88064,
          90112,
          92160,
          94208,
          96256,
          98304,
          100352,
          102400,
          104448,
          106496,
          108544,
          110592,
          112640,
          114688,
          116736,
          118784,
          120832,
          122880,
          124928,
          126976,
          129024,
          131072,
          133120,
          135168,
          137216,
          139264,
          141312,
          143360,
          145408,
          147456,
          149504,
          151552,
          153600,
          155648,
          157696,
          159744,
          161792,
          163840,
          165888,
          167936,
          169984,
          172032,
          174080,
          176128,
          178176,
          180224,
          182272,
          184320,
          186368,
          188416,
          190464,
          192512,
          194560,
          196608,
          198656,
          200704,
          202752,
          204800,
          206848,
          208896,
          210944,
          212992,
          215040,
          217088,
          219136,
          221184,
          223232,
          225280,
          227328,
          229376,
          231424,
          233472,
          235520,
          237568,
          239616,
          241664,
          243712,
          245760,
          247808,
          249856,
          251904,
          253952,
          256000,
          258048,
          260096,
          262144,
          264192,
          266240,
          268288,
          270336,
          272384,
          274432,
          276480,
          278528,
          280576,
          282624,
          284672,
          286720,
          288768,
          290816,
          292864,
          294912,
          296960,
          299008,
          301056,
          303104,
          305152,
          307200,
          309248,
          311296,
          313344,
          315392,
          317440,
          319488,
          321536,
          323584,
          325632,
          327680,
          329728,
          331776,
          333824,
          335872,
          337920,
          339968,
          342016,
          344064,
          346112,
          348160,
          350208,
          352256,
          354304,
          356352,
          358400,
          360448,
          362496,
          364544,
          366592,
          368640,
          370688,
          372736,
          374784,
          376832,
          378880,
          380928,
          382976,
          385024,
          387072,
          389120,
          391168,
          393216,
          395264,
          397312,
          399360,
          401408,
          403456,
          405504,
          407552,
          409600,
          411648,
          413696,
          415744,
          417792,
          419840,
          421888,
          423936,
          425984,
          428032,
          430080,
          432128,
          434176,
          436224,
          438272,
          440320,
          442368,
          444416,
          446464,
          448512,
          450560,
          452608,
          454656,
          456704,
          458752,
          460800,
          462848,
          464896,
          466944,
          468992,
          471040,
          473088,
          475136,
          477184,
          479232,
          481280,
          483328,
          485376,
          487424,
          489472,
          491520,
          493568,
          495616,
          497664,
          499712,
          501760,
          503808,
          505856,
          507904,
          509952,
          512000,
          514048,
          516096,
          518144,
          520192,
          522240,
          524288,
          526336,
          528384,
          530432,
          532480,
          534528,
          536576,
          538624,
          540672,
          542720,
          544768,
          546816,
          548864,
          550912,
          552960,
          555008,
          557056,
          559104,
          561152,
          563200,
          565248,
          567296,
          569344,
          571392,
          573440,
          575488,
          577536,
          579584,
          581632,
          583680,
          585728,
          587776,
          589824,
          591872,
          593920,
          595968,
          598016,
          600064,
          602112,
          604160,
          606208,
          608256,
          610304,
          612352,
          614400,
          616448,
          618496,
          620544,
          622592,
          624640,
          626688,
          628736,
          630784,
          632832,
          634880,
          636928,
          638976,
          641024,
          643072,
          645120,
          647168,
          649216,
          651264,
          653312,
          655360,
          657408,
          659456,
          661504,
          663552,
          665600,
          667648,
          669696,
          671744,
          673792,
          675840,
          677888,
          679936,
          681984,
          684032,
          686080,
          688128,
          690176,
          692224,
          694272,
          696320,
          698368,
          700416,
          702464,
          704512,
          706560,
          708608,
          710656,
          712704,
          714752,
          716800,
          718848,
          720896,
          722944,
          724992,
          727040,
          729088,
          731136,
          733184,
          735232,
          737280,
          739328,
          741376,
          743424,
          745472,
          747520,
          749568,
          751616,
          753664,
          755712,
          757760,
          759808,
          761856,
          763904,
          765952,
          768000,
          770048,
          772096,
          774144,
          776192,
          778240,
          780288,
          782336,
          784384,
          786432,
          788480,
          790528,
          792576,
          794624,
          796672,
          798720,
          800768,
          802816,
          804864,
          806912,
          808960,
          811008,
          813056,
          815104,
          817152,
          819200,
          821248,
          823296,
          825344,
          827392,
          829440,
          831488,
          833536,
          835584,
          837632,
          839680,
          841728,
          843776,
          845824,
          847872,
          849920,
          851968,
          854016,
          856064,
          858112,
          860160,
          862208,
          864256,
          866304,
          868352,
          870400,
          872448,
          874496,
          876544,
          878592,
          880640,
          882688,
          884736,
          886784,
          888832,
          890880,
          892928,
          894976,
          897024,
          899072,
          901120,
          903168,
          905216,
          907264,
          909312,
          911360,
          913408,
          915456,
          917504,
          919552,
          921600,
          923648,
          925696,
          927744,
          929792,
          931840,
          933888,
          935936,
          937984,
          940032,
          942080,
          944128,
          946176,
          948224,
          950272,
          952320,
          954368,
          956416,
          958464,
          960512,
          962560,
          964608,
          966656,
          968704,
          970752,
          972800,
          974848,
          976896,
          978944,
          980992,
          983040,
          985088,
          987136,
          989184,
          991232,
          993280,
          995328,
          997376,
          999424,
          1001472,
          1003520,
          1005568,
          1007616,
          1009664,
          1011712,
          1013760,
          1015808,
          1017856,
          1019904,
          1021952,
          1024000,
          1026048,
          1028096,
          1030144,
          1032192,
          1034240,
          1036288,
          1038336,
          1040384,
          1042432,
          1044480,
          1046528,
          1048576,
          1050624,
          1052672,
          1054720,
          1056768,
          1058816,
          1060864,
          1062912,
          1064960,
          1067008,
          1069056,
          1071104,
          1073152,
          1075200,
          1077248,
          1079296,
          1081344,
          1083392,
          1085440,
          1087488,
          1089536,
          1091584,
          1093632,
          1095680,
          1097728,
          1099776,
          1101824,
          1103872,
          1105920,
          1107968,
          1110016,
          1112064,
          1114112,
          1116160,
          1118208,
          1120256,
          1122304,
          1124352,
          1126400,
          1128448,
          1130496,
          1132544,
          1134592,
          1136640,
          1138688,
          1140736,
          1142784,
          1144832,
          1146880,
          1148928,
          1150976,
          1153024,
          1155072,
          1157120,
          1159168,
          1161216,
          1163264,
          1165312,
          1167360,
          1169408,
          1171456,
          1173504,
          1175552,
          1177600,
          1179648,
          1181696,
          1183744,
          1185792,
          1187840,
          1189888,
          1191936,
          1193984,
          1196032,
          1198080,
          1200128,
          1202176,
          1204224,
          1206272,
          1208320,
          1210368,
          1212416,
          1214464,
          1216512,
          1218560,
          1220608,
          1222656,
          1224704,
          1226752,
          1228800,
          1230848,
          1232896,
          1234944,
          1236992,
          1239040,
          1241088,
          1243136,
          1245184,
          1247232,
          1249280,
          1251328,
          1253376,
          1255424,
          1257472,
          1259520,
          1261568,
          1263616,
          1265664,
          1267712,
          1269760,
          1271808,
          1273856,
          1275904,
          1277952,
          1280000,
          1282048,
          1284096,
          1286144,
          1288192,
          1290240,
          1292288,
          1294336,
          1296384,
          1298432,
          1300480,
          1302528,
          1304576,
          1306624,
          1308672,
          1310720,
          1312768,
          1314816,
          1316864,
          1318912,
          1320960,
          1323008,
          1325056,
          1327104,
          1329152,
          1331200,
          1333248,
          1335296,
          1337344,
          1339392,
          1341440,
          1343488,
          1345536,
          1347584,
          1349632,
          1351680,
          1353728,
          1355776,
          1357824,
          1359872,
          1361920,
          1363968,
          1366016,
          1368064,
          1370112,
          1372160,
          1374208,
          1376256,
          1378304,
          1380352,
          1382400,
          1384448,
          1386496,
          1388544,
          1390592,
          1392640,
          1394688,
          1396736,
          1398784,
          1400832,
          1402880,
          1404928,
          1406976,
          1409024,
          1411072,
          1413120,
          1415168,
          1417216,
          1419264,
          1421312,
          1423360,
          1425408,
          1427456,
          1429504,
          1431552,
          1433600,
          1435648,
          1437696,
          1439744,
          1441792,
          1443840,
          1445888,
          1447936,
          1449984,
          1452032,
          1454080,
          1456128,
          1458176,
          1460224,
          1462272,
          1464320,
          1466368,
          1468416,
          1470464,
          1472512,
          1474560,
          1476608,
          1478656,
          1480704,
          1482752,
          1484800,
          1486848,
          1488896,
          1490944,
          1492992,
          1495040,
          1497088,
          1499136,
          1501184,
          1503232,
          1505280,
          1507328,
          1509376,
          1511424,
          1513472,
          1515520,
          1517568,
          1519616,
          1521664,
          1523712,
          1525760,
          1527808,
          1529856,
          1531904,
          1533952,
          1536000,
          1538048,
          1540096,
          1542144,
          1544192,
          1546240,
          1548288,
          1550336,
          1552384,
          1554432,
          1556480,
          1558528,
          1560576,
          1562624,
          1564672,
          1566720,
          1568768,
          1570816,
          1572864,
          1574912,
          1576960,
          1579008,
          1581056,
          1583104,
          1585152,
          1587200,
          1589248,
          1591296,
          1593344,
          1595392,
          1597440,
          1599488,
          1601536,
          1603584,
          1605632,
          1607680,
          1609728,
          1611776,
          1613824,
          1615872,
          1617920,
          1619968,
          1622016,
          1624064,
          1626112,
          1628160,
          1630208,
          1632256,
          1634304,
          1636352,
          1638400,
          1640448,
          1642496,
          1644544,
          1646592,
          1648640,
          1650688,
          1652736,
          1654784,
          1656832,
          1658880,
          1660928,
          1662976,
          1665024,
          1667072,
          1669120,
          1671168,
          1673216,
          1675264,
          1677312,
          1679360,
          1681408,
          1683456,
          1685504,
          1687552,
          1689600,
          1691648,
          1693696,
          1695744,
          1697792,
          1699840,
          1701888,
          1703936,
          1705984,
          1708032,
          1710080,
          1712128,
          1714176,
          1716224,
          1718272,
          1720320,
          1722368,
          1724416,
          1726464,
          1728512,
          1730560,
          1732608,
          1734656,
          1736704,
          1738752,
          1740800,
          1742848,
          1744896,
          1746944,
          1748992,
          1751040,
          1753088,
          1755136,
          1757184,
          1759232,
          1761280,
          1763328,
          1765376,
          1767424,
          1769472,
          1771520,
          1773568,
          1775616,
          1777664,
          1779712,
          1781760,
          1783808,
          1785856,
          1787904,
          1789952,
          1792000,
          1794048,
          1796096,
          1798144,
          1800192,
          1802240,
          1804288,
          1806336,
          1808384,
          1810432,
          1812480,
          1814528,
          1816576,
          1818624,
          1820672,
          1822720,
          1824768,
          1826816,
          1828864,
          1830912,
          1832960,
          1835008,
          1837056,
          1839104,
          1841152,
          1843200,
          1845248,
          1847296,
          1849344,
          1851392,
          1853440,
          1855488,
          1857536,
          1859584,
          1861632,
          1863680,
          1865728,
          1867776,
          1869824,
          1871872,
          1873920,
          1875968,
          1878016,
          1880064,
          1882112,
          1884160,
          1886208,
          1888256,
          1890304,
          1892352,
          1894400,
          1896448,
          1898496,
          1900544,
          1902592,
          1904640,
          1906688,
          1908736,
          1910784,
          1912832,
          1914880,
          1916928,
          1918976,
          1921024,
          1923072,
          1925120,
          1927168,
          1929216,
          1931264,
          1933312,
          1935360,
          1937408,
          1939456,
          1941504,
          1943552,
          1945600,
          1947648,
          1949696,
          1951744,
          1953792,
          1955840,
          1957888,
          1959936,
          1961984,
          1964032,
          1966080,
          1968128,
          1970176,
          1972224,
          1974272,
          1976320,
          1978368,
          1980416,
          1982464,
          1984512,
          1986560,
          1988608,
          1990656,
          1992704,
          1994752,
          1996800,
          1998848,
          2000896,
          2002944,
          2004992,
          2007040,
          2009088,
          2011136,
          2013184,
          2015232,
          2017280,
          2019328,
          2021376,
          2023424,
          2025472,
          2027520,
          2029568,
          2031616,
          2033664,
          2035712,
          2037760,
          2039808,
          2041856,
          2043904,
          2045952,
          2048000,
          2050048
         ],
         "xaxis": "x",
         "y": [
          10.893806457519531,
          10.427045822143555,
          9.634830474853516,
          10.264103889465332,
          9.31908893585205,
          9.114545822143555,
          8.989713668823242,
          9.26657485961914,
          9.180978775024414,
          9.055776596069336,
          8.61603832244873,
          8.559957504272461,
          7.807898998260498,
          7.91976261138916,
          8.070566177368164,
          8.144721031188965,
          6.628560543060303,
          7.808859348297119,
          7.625852584838867,
          8.305569648742676,
          7.4169535636901855,
          8.14185905456543,
          7.313915252685547,
          7.433940410614014,
          7.547981262207031,
          7.983862400054932,
          7.127997875213623,
          8.078697204589844,
          7.674922943115234,
          7.675109386444092,
          7.434235095977783,
          6.662626266479492,
          7.124334335327148,
          7.280995845794678,
          8.049904823303223,
          7.849207878112793,
          7.362364292144775,
          7.178154945373535,
          7.463449954986572,
          7.016193866729736,
          7.165964603424072,
          7.815927982330322,
          7.132492542266846,
          7.43367862701416,
          6.771230220794678,
          7.732711315155029,
          7.033417224884033,
          7.152134895324707,
          7.452767848968506,
          5.829916477203369,
          5.914529323577881,
          7.554685115814209,
          6.6695756912231445,
          7.587604522705078,
          6.977630138397217,
          5.184032440185547,
          6.905055522918701,
          6.271007061004639,
          7.990111827850342,
          7.912749767303467,
          7.301448822021484,
          6.28422212600708,
          6.436372756958008,
          7.807494640350342,
          5.948660850524902,
          7.79941987991333,
          6.602100849151611,
          6.598696231842041,
          7.384997844696045,
          5.916327953338623,
          7.766610622406006,
          6.166258335113525,
          6.354918003082275,
          6.029428482055664,
          5.2543721199035645,
          6.982370376586914,
          6.40958833694458,
          7.218695640563965,
          5.283820629119873,
          7.403279781341553,
          7.551144599914551,
          7.717860221862793,
          7.281604766845703,
          6.9039106369018555,
          6.831348896026611,
          7.377828598022461,
          7.305034637451172,
          6.493904113769531,
          7.502947807312012,
          7.475921630859375,
          6.264326095581055,
          7.900688171386719,
          6.392958641052246,
          5.9810099601745605,
          6.367247104644775,
          6.246963977813721,
          5.692319393157959,
          7.359583377838135,
          7.038933753967285,
          7.6092376708984375,
          7.183945655822754,
          5.9209113121032715,
          6.992689609527588,
          7.349926948547363,
          6.442592620849609,
          6.999311923980713,
          6.716317176818848,
          7.594215393066406,
          6.622709274291992,
          6.699754238128662,
          5.393760681152344,
          7.043684482574463,
          6.514898300170898,
          6.696376323699951,
          6.69824743270874,
          6.460135459899902,
          7.281935214996338,
          7.212729454040527,
          7.594583511352539,
          7.138926982879639,
          7.3348188400268555,
          7.189274311065674,
          6.738702774047852,
          6.5225324630737305,
          6.949594497680664,
          7.422399520874023,
          7.410975456237793,
          6.666720867156982,
          6.22360372543335,
          7.085495948791504,
          5.923482418060303,
          7.522467136383057,
          6.361387729644775,
          5.82284688949585,
          6.317162036895752,
          6.981962203979492,
          7.379582405090332,
          7.258055686950684,
          6.434112071990967,
          7.474478244781494,
          6.024131774902344,
          6.669892311096191,
          6.059913158416748,
          6.535154342651367,
          7.081905364990234,
          6.6076836585998535,
          7.319371223449707,
          5.877340316772461,
          5.5880937576293945,
          6.722639083862305,
          6.517719745635986,
          6.36610221862793,
          7.354548454284668,
          6.7593159675598145,
          6.2848405838012695,
          7.1035966873168945,
          6.770442008972168,
          6.092446327209473,
          6.611698150634766,
          7.249154090881348,
          7.035548210144043,
          7.410935878753662,
          6.4828386306762695,
          7.404731750488281,
          7.463119983673096,
          7.264775276184082,
          7.030026912689209,
          6.849971771240234,
          7.444589138031006,
          5.312416076660156,
          6.035064220428467,
          7.059691429138184,
          6.277189254760742,
          5.8443074226379395,
          6.416632652282715,
          7.28755521774292,
          6.559230327606201,
          6.728107929229736,
          6.1987409591674805,
          7.212503910064697,
          6.751069068908691,
          7.469141960144043,
          7.459601879119873,
          7.349392414093018,
          6.170513153076172,
          7.395143032073975,
          7.279488563537598,
          6.692648410797119,
          7.316223621368408,
          6.771309852600098,
          5.905599594116211,
          6.544557094573975,
          5.2595744132995605,
          6.352523326873779,
          6.099898815155029,
          6.16761589050293,
          7.125868797302246,
          6.7148237228393555,
          5.997227668762207,
          6.733247756958008,
          6.214476585388184,
          6.407529830932617,
          7.264333724975586,
          6.977560520172119,
          6.015833854675293,
          5.969151020050049,
          7.000478267669678,
          7.178333759307861,
          6.263494968414307,
          7.6479105949401855,
          5.736992835998535,
          7.1781744956970215,
          6.309168338775635,
          6.672365188598633,
          5.365495204925537,
          7.181882381439209,
          7.348421573638916,
          6.74544620513916,
          7.177699565887451,
          6.935952186584473,
          6.941099643707275,
          6.934011936187744,
          5.753427982330322,
          6.674023628234863,
          5.459040641784668,
          6.063246250152588,
          6.395949363708496,
          7.373455047607422,
          6.879143238067627,
          7.1031951904296875,
          6.42634916305542,
          7.1864447593688965,
          6.8405632972717285,
          3.5708701610565186,
          7.0937581062316895,
          6.570587635040283,
          6.882235050201416,
          7.279861927032471,
          6.178062915802002,
          7.118537902832031,
          6.493494033813477,
          5.847645282745361,
          5.426514148712158,
          7.372856616973877,
          6.965512275695801,
          5.870194911956787,
          6.197558403015137,
          6.458340644836426,
          6.9095778465271,
          6.297845363616943,
          6.318068504333496,
          6.843086242675781,
          6.492095947265625,
          6.620027542114258,
          6.984341144561768,
          6.305624961853027,
          6.843216896057129,
          6.46617317199707,
          6.985827922821045,
          6.762999057769775,
          6.4933013916015625,
          6.540491104125977,
          6.23700475692749,
          6.200334072113037,
          5.087399005889893,
          6.061723709106445,
          6.203706741333008,
          5.4813008308410645,
          6.1341447830200195,
          6.042251110076904,
          5.891076564788818,
          6.780318260192871,
          6.889206886291504,
          7.024605751037598,
          6.477628707885742,
          5.8595051765441895,
          5.17009973526001,
          6.039463520050049,
          6.932980060577393,
          6.82629919052124,
          6.837954044342041,
          7.210803985595703,
          7.130180358886719,
          7.339724540710449,
          5.242691516876221,
          7.033135890960693,
          6.220620155334473,
          6.3562774658203125,
          6.419381618499756,
          7.239526748657227,
          7.125497341156006,
          6.268826961517334,
          6.306741714477539,
          6.266530990600586,
          6.408555030822754,
          6.608877658843994,
          7.197091579437256,
          7.300697326660156,
          6.535238742828369,
          6.545780181884766,
          6.591123104095459,
          6.527737617492676,
          5.379087448120117,
          5.816311836242676,
          6.979439735412598,
          6.299715042114258,
          6.738414764404297,
          6.83601713180542,
          6.863978862762451,
          6.356145858764648,
          5.8859100341796875,
          6.475433349609375,
          7.122377872467041,
          6.07142448425293,
          6.694138526916504,
          5.902716636657715,
          5.306896686553955,
          6.100412845611572,
          6.454117774963379,
          6.785376071929932,
          6.912567138671875,
          5.604643821716309,
          7.153719425201416,
          6.470905780792236,
          6.952639102935791,
          5.539736270904541,
          6.790310382843018,
          6.9054856300354,
          6.2068681716918945,
          6.471749782562256,
          6.596714019775391,
          6.819143295288086,
          5.786790370941162,
          5.1841535568237305,
          5.967902183532715,
          6.736931800842285,
          5.688403129577637,
          6.72953987121582,
          6.925650119781494,
          5.920834541320801,
          6.706357479095459,
          6.801548480987549,
          6.692095756530762,
          6.381126880645752,
          5.92860221862793,
          7.283708095550537,
          6.068110466003418,
          6.476272106170654,
          5.462843894958496,
          5.8082685470581055,
          6.196161270141602,
          5.988715648651123,
          7.031335830688477,
          5.655581474304199,
          6.302164077758789,
          6.990185260772705,
          6.657060146331787,
          5.792215824127197,
          5.712269306182861,
          5.634867191314697,
          5.756242752075195,
          6.833441734313965,
          6.653871059417725,
          6.184776782989502,
          6.141414642333984,
          6.0818586349487305,
          6.62661600112915,
          6.63861083984375,
          6.139286994934082,
          5.828361988067627,
          5.95382833480835,
          6.8002848625183105,
          6.97227668762207,
          6.524081230163574,
          6.48838996887207,
          4.758155822753906,
          6.398387908935547,
          6.102293968200684,
          6.068881511688232,
          5.9436821937561035,
          6.9621195793151855,
          5.715623378753662,
          7.000760555267334,
          5.76843786239624,
          5.78002405166626,
          5.549404621124268,
          5.251672267913818,
          6.037641525268555,
          5.882079601287842,
          5.322067737579346,
          7.192586421966553,
          5.651959419250488,
          6.422826290130615,
          6.721909046173096,
          6.212947845458984,
          6.5321269035339355,
          6.26345157623291,
          5.321763038635254,
          6.853144645690918,
          5.9502129554748535,
          6.240609169006348,
          7.064380168914795,
          7.060173511505127,
          5.434298515319824,
          5.713833808898926,
          5.35958194732666,
          7.389200687408447,
          6.6098551750183105,
          5.971768379211426,
          6.044561862945557,
          6.879286766052246,
          5.353652477264404,
          7.192244529724121,
          5.155861854553223,
          6.459859848022461,
          6.823322772979736,
          5.7928643226623535,
          6.195549964904785,
          6.129689693450928,
          5.382623195648193,
          6.791993618011475,
          7.066790580749512,
          6.2079668045043945,
          5.869408130645752,
          4.974715709686279,
          3.9946670532226562,
          7.120031356811523,
          6.764587879180908,
          5.174720764160156,
          5.589199542999268,
          5.667579650878906,
          5.988559246063232,
          6.569380283355713,
          6.821842670440674,
          6.315308094024658,
          6.189703464508057,
          6.124805450439453,
          7.062331199645996,
          4.86233377456665,
          6.8343024253845215,
          6.483643531799316,
          6.55932092666626,
          5.6199116706848145,
          4.8574137687683105,
          6.9304399490356445,
          6.953507900238037,
          6.775815010070801,
          6.817715167999268,
          6.5624260902404785,
          5.681647777557373,
          5.837902545928955,
          5.0996479988098145,
          6.8481268882751465,
          6.1209235191345215,
          6.820996284484863,
          6.056341648101807,
          6.095166206359863,
          6.61842679977417,
          5.438260078430176,
          5.6815290451049805,
          5.080475807189941,
          6.436587333679199,
          5.884406089782715,
          5.9520978927612305,
          5.5276665687561035,
          6.096118927001953,
          6.399137020111084,
          4.965343952178955,
          6.128511428833008,
          7.015021324157715,
          5.319487571716309,
          6.856664180755615,
          6.083608150482178,
          6.367321968078613,
          5.843657493591309,
          6.4260478019714355,
          6.378108978271484,
          6.459695339202881,
          6.6272759437561035,
          6.059246063232422,
          5.8577470779418945,
          6.326718807220459,
          5.700711727142334,
          6.2530927658081055,
          6.536418914794922,
          5.880959987640381,
          6.49962043762207,
          6.309074401855469,
          5.8287248611450195,
          5.960597991943359,
          6.050961494445801,
          5.545186519622803,
          6.5997138023376465,
          5.92969274520874,
          6.413270950317383,
          6.187255382537842,
          6.180765151977539,
          6.33779239654541,
          6.12028694152832,
          6.397641181945801,
          5.3044657707214355,
          6.83604621887207,
          6.510043621063232,
          6.346192836761475,
          6.65885066986084,
          6.221997261047363,
          6.662997245788574,
          5.459427833557129,
          6.85530948638916,
          5.3961591720581055,
          5.622014045715332,
          5.986109733581543,
          6.281040191650391,
          5.967301845550537,
          6.529374599456787,
          6.527423858642578,
          5.974690914154053,
          6.404294490814209,
          5.865479469299316,
          5.839770317077637,
          5.659456729888916,
          6.732898235321045,
          5.857100009918213,
          6.750272750854492,
          6.411985397338867,
          6.792856216430664,
          6.249256134033203,
          6.093013286590576,
          6.611140727996826,
          5.683406829833984,
          5.102623462677002,
          6.719038963317871,
          6.895438194274902,
          6.569279670715332,
          6.300320148468018,
          6.2968339920043945,
          6.411684036254883,
          6.303232669830322,
          6.670129299163818,
          6.057904243469238,
          6.274672031402588,
          5.103349208831787,
          6.7471022605896,
          6.320355415344238,
          6.9138264656066895,
          4.943398952484131,
          5.276199817657471,
          6.158751487731934,
          6.529575824737549,
          6.511270999908447,
          6.563686847686768,
          6.825718402862549,
          6.0076751708984375,
          5.515554428100586,
          6.78159236907959,
          4.525760650634766,
          5.7860331535339355,
          6.884456634521484,
          4.664862155914307,
          6.475404262542725,
          5.991855621337891,
          5.9356369972229,
          6.793413162231445,
          6.159652233123779,
          6.055816650390625,
          6.205516815185547,
          5.20932149887085,
          6.274753570556641,
          5.284594535827637,
          4.9419779777526855,
          4.458980560302734,
          6.209834098815918,
          5.361738681793213,
          5.023915767669678,
          5.331613540649414,
          6.29410982131958,
          6.466154098510742,
          5.626280784606934,
          6.35620641708374,
          5.478240489959717,
          5.7116475105285645,
          5.892956256866455,
          6.420624256134033,
          5.794150352478027,
          5.921229839324951,
          6.869009017944336,
          6.1308183670043945,
          5.915657043457031,
          6.974363327026367,
          5.450153350830078,
          6.449761867523193,
          6.045516014099121,
          5.421005725860596,
          6.253221035003662,
          5.2270307540893555,
          6.376247406005859,
          6.1876349449157715,
          6.06260871887207,
          6.720747470855713,
          5.627060890197754,
          6.283331394195557,
          6.351734161376953,
          5.523952960968018,
          6.370999336242676,
          6.451564311981201,
          6.202211856842041,
          4.996163845062256,
          5.659505367279053,
          5.903824329376221,
          5.46745491027832,
          6.066342830657959,
          6.6164679527282715,
          4.496311187744141,
          6.918656349182129,
          6.374454021453857,
          6.624735355377197,
          6.1405181884765625,
          6.419637680053711,
          5.368797779083252,
          5.63762092590332,
          6.130258083343506,
          5.883955001831055,
          5.349919319152832,
          5.981701850891113,
          6.619306564331055,
          5.355196952819824,
          6.703455924987793,
          6.215742111206055,
          6.538422107696533,
          6.278171539306641,
          6.039126396179199,
          5.870230674743652,
          6.178444862365723,
          6.194089412689209,
          6.146444797515869,
          6.731659889221191,
          5.233891010284424,
          5.954592704772949,
          5.619664669036865,
          6.739044666290283,
          5.9534220695495605,
          6.210587978363037,
          6.765751361846924,
          6.739131450653076,
          5.395712852478027,
          6.435729503631592,
          6.559391021728516,
          6.6493916511535645,
          5.664209365844727,
          5.9365363121032715,
          5.80687952041626,
          6.157029628753662,
          6.582067012786865,
          5.272243976593018,
          6.387413024902344,
          6.019307613372803,
          5.77635383605957,
          5.32295036315918,
          5.941738128662109,
          6.576267242431641,
          5.763748645782471,
          5.9041008949279785,
          5.74242639541626,
          6.283062934875488,
          6.257469177246094,
          6.562018394470215,
          6.809732437133789,
          6.094371795654297,
          6.236048221588135,
          4.181057929992676,
          6.251044273376465,
          6.477835178375244,
          6.046085357666016,
          5.837491512298584,
          6.458290100097656,
          5.670943260192871,
          5.2971978187561035,
          6.108110427856445,
          5.750730991363525,
          5.690359115600586,
          6.7756781578063965,
          5.08757209777832,
          6.263877868652344,
          6.261489391326904,
          5.932858467102051,
          6.66238260269165,
          5.9046430587768555,
          6.724463939666748,
          5.962268829345703,
          6.093897342681885,
          5.88328742980957,
          6.031794548034668,
          5.620194911956787,
          6.442348957061768,
          6.230268955230713,
          5.84187650680542,
          6.334973335266113,
          6.030936241149902,
          6.146600246429443,
          5.107158660888672,
          5.6236724853515625,
          6.439016342163086,
          6.273532390594482,
          6.079730033874512,
          5.274417877197266,
          5.807642459869385,
          5.274596214294434,
          5.956326961517334,
          6.173469543457031,
          6.205816745758057,
          5.946310997009277,
          5.619250297546387,
          7.0108642578125,
          6.019609451293945,
          6.104399681091309,
          5.485321998596191,
          6.42507791519165,
          6.118117332458496,
          5.677605628967285,
          6.27752685546875,
          5.981806755065918,
          5.951730251312256,
          6.593650817871094,
          5.710947036743164,
          5.852570056915283,
          5.670546054840088,
          6.290445327758789,
          5.928374290466309,
          6.097687244415283,
          6.00021505355835,
          3.711148977279663,
          6.25396728515625,
          5.271996021270752,
          5.282454967498779,
          6.487525463104248,
          6.3335957527160645,
          5.35158109664917,
          5.694026470184326,
          6.299812316894531,
          6.349177837371826,
          6.209733486175537,
          6.428217887878418,
          5.592421531677246,
          5.482235431671143,
          5.584145545959473,
          5.41577672958374,
          6.766429424285889,
          5.789606094360352,
          5.375753402709961,
          6.307466506958008,
          6.318812370300293,
          6.481020450592041,
          5.817994594573975,
          6.273177623748779,
          6.722898006439209,
          5.338503837585449,
          6.670080661773682,
          5.980781078338623,
          5.287014007568359,
          5.842097282409668,
          5.419844627380371,
          5.756831645965576,
          5.9169745445251465,
          5.530550479888916,
          6.213921070098877,
          6.109903812408447,
          5.853615760803223,
          6.207830429077148,
          5.492738246917725,
          6.175370216369629,
          3.9055612087249756,
          6.336564064025879,
          4.95462703704834,
          5.901712417602539,
          5.375865936279297,
          5.625036716461182,
          4.983633518218994,
          6.198032855987549,
          5.082873344421387,
          4.950484752655029,
          5.829664707183838,
          5.235795497894287,
          6.1329569816589355,
          5.456839084625244,
          5.522766590118408,
          5.183165073394775,
          5.625036716461182,
          5.867244720458984,
          5.701903820037842,
          6.661999702453613,
          6.231913089752197,
          6.1269636154174805,
          4.888288974761963,
          5.385567665100098,
          5.985903263092041,
          6.738123893737793,
          5.537220478057861,
          5.86268424987793,
          6.744391441345215,
          6.640661716461182,
          5.472293853759766,
          5.546179294586182,
          5.534297943115234,
          6.367386341094971,
          5.67014741897583,
          6.480928897857666,
          5.720998287200928,
          6.059472560882568,
          5.823733806610107,
          4.560082912445068,
          5.55682373046875,
          4.636996746063232,
          5.792925834655762,
          5.952547550201416,
          5.223880290985107,
          6.596568584442139,
          6.045695781707764,
          5.875568866729736,
          6.208745956420898,
          6.532783508300781,
          6.210654258728027,
          6.346561908721924,
          6.338615894317627,
          5.9337358474731445,
          6.433508396148682,
          6.319293975830078,
          5.811450004577637,
          5.49641752243042,
          5.907059192657471,
          5.746159553527832,
          6.503262519836426,
          5.756721496582031,
          6.488044738769531,
          6.117894649505615,
          5.86445426940918,
          5.835223197937012,
          5.549563884735107,
          5.330730438232422,
          4.362559795379639,
          5.012577056884766,
          5.569671630859375,
          6.176525592803955,
          5.664442539215088,
          6.331360340118408,
          5.243968486785889,
          5.100625514984131,
          6.374338150024414,
          6.497174263000488,
          5.70150899887085,
          5.64119291305542,
          5.607527732849121,
          5.71267557144165,
          6.280437469482422,
          5.915352821350098,
          5.677097320556641,
          6.360349178314209,
          6.265502452850342,
          6.178948402404785,
          5.341156482696533,
          5.870170593261719,
          5.4451117515563965,
          6.241472244262695,
          5.314521312713623,
          4.9461517333984375,
          6.680211544036865,
          6.917564868927002,
          6.49563455581665,
          6.11849308013916,
          5.478568077087402,
          6.257815837860107,
          6.127716064453125,
          5.675842761993408,
          6.210302829742432,
          6.548689842224121,
          5.86232852935791,
          5.740886688232422,
          6.508115768432617,
          5.909424304962158,
          6.129854679107666,
          5.92615270614624,
          6.129012107849121,
          5.991880416870117,
          6.622316837310791,
          5.540594577789307,
          6.060328006744385,
          5.931400775909424,
          6.792439937591553,
          5.400372505187988,
          5.405616283416748,
          5.465909481048584,
          5.291983127593994,
          6.062725067138672,
          5.695817470550537,
          6.294777870178223,
          6.29988431930542,
          6.702127933502197,
          5.836272239685059,
          6.2906880378723145,
          5.82119083404541,
          6.386631965637207,
          3.914015293121338,
          4.600915431976318,
          5.786273956298828,
          5.965373516082764,
          5.688570022583008,
          5.874922752380371,
          5.608105659484863,
          6.473889350891113,
          6.068475246429443,
          5.559708118438721,
          6.369912147521973,
          5.660721302032471,
          6.10959529876709,
          5.171573162078857,
          5.885856628417969,
          6.098738670349121,
          5.7309136390686035,
          6.036271095275879,
          5.380069732666016,
          6.102173805236816,
          4.849461078643799,
          5.115664958953857,
          6.431756019592285,
          6.017018795013428,
          6.432150363922119,
          5.532230377197266,
          5.253816604614258,
          6.474475860595703,
          3.9770805835723877,
          6.072547912597656,
          5.884472370147705,
          6.253187656402588,
          5.945555686950684,
          6.096212863922119,
          6.4258246421813965,
          5.689128875732422,
          6.403471946716309,
          5.640111446380615,
          6.316356658935547,
          5.253259658813477,
          3.884716033935547,
          5.994566440582275,
          6.261377334594727,
          6.544004917144775,
          5.812011241912842,
          5.568344593048096,
          6.187015056610107,
          6.731874465942383,
          4.954038619995117,
          5.797821044921875,
          6.174631595611572,
          4.538426399230957,
          6.5726213455200195,
          4.4626264572143555,
          5.83879280090332,
          6.313039302825928,
          5.421074867248535,
          5.935083389282227,
          4.370193004608154,
          6.679178714752197,
          6.659379959106445,
          6.706349849700928,
          5.316075325012207,
          5.927857875823975,
          5.195620536804199,
          5.5007004737854,
          6.650441646575928,
          6.306372165679932,
          6.594603538513184,
          4.7035698890686035,
          4.553125381469727,
          6.798477649688721,
          5.679795265197754,
          5.9527788162231445,
          4.551840305328369,
          5.673043251037598,
          6.304561614990234,
          6.020262241363525,
          6.0588297843933105,
          5.522863388061523,
          5.2381768226623535,
          6.00363302230835,
          6.389254570007324,
          5.4175825119018555,
          6.422732830047607,
          5.610039234161377,
          5.911554336547852,
          5.2098002433776855,
          5.918560028076172,
          5.055022239685059,
          6.686042308807373,
          5.942429542541504,
          5.942678928375244,
          5.244865894317627,
          6.026693344116211,
          6.240598678588867,
          4.710692405700684,
          5.621710777282715,
          5.577942371368408,
          5.0781660079956055,
          5.779727458953857,
          4.423628330230713
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training curve for my tiny demo model!"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"d657c504-b9c0-4f64-b88a-1888977a57e1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d657c504-b9c0-4f64-b88a-1888977a57e1\")) {                    Plotly.newPlot(                        \"d657c504-b9c0-4f64-b88a-1888977a57e1\",                        [{\"hovertemplate\":\"Tokens=%{x}<br>Loss=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[0,2048,4096,6144,8192,10240,12288,14336,16384,18432,20480,22528,24576,26624,28672,30720,32768,34816,36864,38912,40960,43008,45056,47104,49152,51200,53248,55296,57344,59392,61440,63488,65536,67584,69632,71680,73728,75776,77824,79872,81920,83968,86016,88064,90112,92160,94208,96256,98304,100352,102400,104448,106496,108544,110592,112640,114688,116736,118784,120832,122880,124928,126976,129024,131072,133120,135168,137216,139264,141312,143360,145408,147456,149504,151552,153600,155648,157696,159744,161792,163840,165888,167936,169984,172032,174080,176128,178176,180224,182272,184320,186368,188416,190464,192512,194560,196608,198656,200704,202752,204800,206848,208896,210944,212992,215040,217088,219136,221184,223232,225280,227328,229376,231424,233472,235520,237568,239616,241664,243712,245760,247808,249856,251904,253952,256000,258048,260096,262144,264192,266240,268288,270336,272384,274432,276480,278528,280576,282624,284672,286720,288768,290816,292864,294912,296960,299008,301056,303104,305152,307200,309248,311296,313344,315392,317440,319488,321536,323584,325632,327680,329728,331776,333824,335872,337920,339968,342016,344064,346112,348160,350208,352256,354304,356352,358400,360448,362496,364544,366592,368640,370688,372736,374784,376832,378880,380928,382976,385024,387072,389120,391168,393216,395264,397312,399360,401408,403456,405504,407552,409600,411648,413696,415744,417792,419840,421888,423936,425984,428032,430080,432128,434176,436224,438272,440320,442368,444416,446464,448512,450560,452608,454656,456704,458752,460800,462848,464896,466944,468992,471040,473088,475136,477184,479232,481280,483328,485376,487424,489472,491520,493568,495616,497664,499712,501760,503808,505856,507904,509952,512000,514048,516096,518144,520192,522240,524288,526336,528384,530432,532480,534528,536576,538624,540672,542720,544768,546816,548864,550912,552960,555008,557056,559104,561152,563200,565248,567296,569344,571392,573440,575488,577536,579584,581632,583680,585728,587776,589824,591872,593920,595968,598016,600064,602112,604160,606208,608256,610304,612352,614400,616448,618496,620544,622592,624640,626688,628736,630784,632832,634880,636928,638976,641024,643072,645120,647168,649216,651264,653312,655360,657408,659456,661504,663552,665600,667648,669696,671744,673792,675840,677888,679936,681984,684032,686080,688128,690176,692224,694272,696320,698368,700416,702464,704512,706560,708608,710656,712704,714752,716800,718848,720896,722944,724992,727040,729088,731136,733184,735232,737280,739328,741376,743424,745472,747520,749568,751616,753664,755712,757760,759808,761856,763904,765952,768000,770048,772096,774144,776192,778240,780288,782336,784384,786432,788480,790528,792576,794624,796672,798720,800768,802816,804864,806912,808960,811008,813056,815104,817152,819200,821248,823296,825344,827392,829440,831488,833536,835584,837632,839680,841728,843776,845824,847872,849920,851968,854016,856064,858112,860160,862208,864256,866304,868352,870400,872448,874496,876544,878592,880640,882688,884736,886784,888832,890880,892928,894976,897024,899072,901120,903168,905216,907264,909312,911360,913408,915456,917504,919552,921600,923648,925696,927744,929792,931840,933888,935936,937984,940032,942080,944128,946176,948224,950272,952320,954368,956416,958464,960512,962560,964608,966656,968704,970752,972800,974848,976896,978944,980992,983040,985088,987136,989184,991232,993280,995328,997376,999424,1001472,1003520,1005568,1007616,1009664,1011712,1013760,1015808,1017856,1019904,1021952,1024000,1026048,1028096,1030144,1032192,1034240,1036288,1038336,1040384,1042432,1044480,1046528,1048576,1050624,1052672,1054720,1056768,1058816,1060864,1062912,1064960,1067008,1069056,1071104,1073152,1075200,1077248,1079296,1081344,1083392,1085440,1087488,1089536,1091584,1093632,1095680,1097728,1099776,1101824,1103872,1105920,1107968,1110016,1112064,1114112,1116160,1118208,1120256,1122304,1124352,1126400,1128448,1130496,1132544,1134592,1136640,1138688,1140736,1142784,1144832,1146880,1148928,1150976,1153024,1155072,1157120,1159168,1161216,1163264,1165312,1167360,1169408,1171456,1173504,1175552,1177600,1179648,1181696,1183744,1185792,1187840,1189888,1191936,1193984,1196032,1198080,1200128,1202176,1204224,1206272,1208320,1210368,1212416,1214464,1216512,1218560,1220608,1222656,1224704,1226752,1228800,1230848,1232896,1234944,1236992,1239040,1241088,1243136,1245184,1247232,1249280,1251328,1253376,1255424,1257472,1259520,1261568,1263616,1265664,1267712,1269760,1271808,1273856,1275904,1277952,1280000,1282048,1284096,1286144,1288192,1290240,1292288,1294336,1296384,1298432,1300480,1302528,1304576,1306624,1308672,1310720,1312768,1314816,1316864,1318912,1320960,1323008,1325056,1327104,1329152,1331200,1333248,1335296,1337344,1339392,1341440,1343488,1345536,1347584,1349632,1351680,1353728,1355776,1357824,1359872,1361920,1363968,1366016,1368064,1370112,1372160,1374208,1376256,1378304,1380352,1382400,1384448,1386496,1388544,1390592,1392640,1394688,1396736,1398784,1400832,1402880,1404928,1406976,1409024,1411072,1413120,1415168,1417216,1419264,1421312,1423360,1425408,1427456,1429504,1431552,1433600,1435648,1437696,1439744,1441792,1443840,1445888,1447936,1449984,1452032,1454080,1456128,1458176,1460224,1462272,1464320,1466368,1468416,1470464,1472512,1474560,1476608,1478656,1480704,1482752,1484800,1486848,1488896,1490944,1492992,1495040,1497088,1499136,1501184,1503232,1505280,1507328,1509376,1511424,1513472,1515520,1517568,1519616,1521664,1523712,1525760,1527808,1529856,1531904,1533952,1536000,1538048,1540096,1542144,1544192,1546240,1548288,1550336,1552384,1554432,1556480,1558528,1560576,1562624,1564672,1566720,1568768,1570816,1572864,1574912,1576960,1579008,1581056,1583104,1585152,1587200,1589248,1591296,1593344,1595392,1597440,1599488,1601536,1603584,1605632,1607680,1609728,1611776,1613824,1615872,1617920,1619968,1622016,1624064,1626112,1628160,1630208,1632256,1634304,1636352,1638400,1640448,1642496,1644544,1646592,1648640,1650688,1652736,1654784,1656832,1658880,1660928,1662976,1665024,1667072,1669120,1671168,1673216,1675264,1677312,1679360,1681408,1683456,1685504,1687552,1689600,1691648,1693696,1695744,1697792,1699840,1701888,1703936,1705984,1708032,1710080,1712128,1714176,1716224,1718272,1720320,1722368,1724416,1726464,1728512,1730560,1732608,1734656,1736704,1738752,1740800,1742848,1744896,1746944,1748992,1751040,1753088,1755136,1757184,1759232,1761280,1763328,1765376,1767424,1769472,1771520,1773568,1775616,1777664,1779712,1781760,1783808,1785856,1787904,1789952,1792000,1794048,1796096,1798144,1800192,1802240,1804288,1806336,1808384,1810432,1812480,1814528,1816576,1818624,1820672,1822720,1824768,1826816,1828864,1830912,1832960,1835008,1837056,1839104,1841152,1843200,1845248,1847296,1849344,1851392,1853440,1855488,1857536,1859584,1861632,1863680,1865728,1867776,1869824,1871872,1873920,1875968,1878016,1880064,1882112,1884160,1886208,1888256,1890304,1892352,1894400,1896448,1898496,1900544,1902592,1904640,1906688,1908736,1910784,1912832,1914880,1916928,1918976,1921024,1923072,1925120,1927168,1929216,1931264,1933312,1935360,1937408,1939456,1941504,1943552,1945600,1947648,1949696,1951744,1953792,1955840,1957888,1959936,1961984,1964032,1966080,1968128,1970176,1972224,1974272,1976320,1978368,1980416,1982464,1984512,1986560,1988608,1990656,1992704,1994752,1996800,1998848,2000896,2002944,2004992,2007040,2009088,2011136,2013184,2015232,2017280,2019328,2021376,2023424,2025472,2027520,2029568,2031616,2033664,2035712,2037760,2039808,2041856,2043904,2045952,2048000,2050048],\"xaxis\":\"x\",\"y\":[10.893806457519531,10.427045822143555,9.634830474853516,10.264103889465332,9.31908893585205,9.114545822143555,8.989713668823242,9.26657485961914,9.180978775024414,9.055776596069336,8.61603832244873,8.559957504272461,7.807898998260498,7.91976261138916,8.070566177368164,8.144721031188965,6.628560543060303,7.808859348297119,7.625852584838867,8.305569648742676,7.4169535636901855,8.14185905456543,7.313915252685547,7.433940410614014,7.547981262207031,7.983862400054932,7.127997875213623,8.078697204589844,7.674922943115234,7.675109386444092,7.434235095977783,6.662626266479492,7.124334335327148,7.280995845794678,8.049904823303223,7.849207878112793,7.362364292144775,7.178154945373535,7.463449954986572,7.016193866729736,7.165964603424072,7.815927982330322,7.132492542266846,7.43367862701416,6.771230220794678,7.732711315155029,7.033417224884033,7.152134895324707,7.452767848968506,5.829916477203369,5.914529323577881,7.554685115814209,6.6695756912231445,7.587604522705078,6.977630138397217,5.184032440185547,6.905055522918701,6.271007061004639,7.990111827850342,7.912749767303467,7.301448822021484,6.28422212600708,6.436372756958008,7.807494640350342,5.948660850524902,7.79941987991333,6.602100849151611,6.598696231842041,7.384997844696045,5.916327953338623,7.766610622406006,6.166258335113525,6.354918003082275,6.029428482055664,5.2543721199035645,6.982370376586914,6.40958833694458,7.218695640563965,5.283820629119873,7.403279781341553,7.551144599914551,7.717860221862793,7.281604766845703,6.9039106369018555,6.831348896026611,7.377828598022461,7.305034637451172,6.493904113769531,7.502947807312012,7.475921630859375,6.264326095581055,7.900688171386719,6.392958641052246,5.9810099601745605,6.367247104644775,6.246963977813721,5.692319393157959,7.359583377838135,7.038933753967285,7.6092376708984375,7.183945655822754,5.9209113121032715,6.992689609527588,7.349926948547363,6.442592620849609,6.999311923980713,6.716317176818848,7.594215393066406,6.622709274291992,6.699754238128662,5.393760681152344,7.043684482574463,6.514898300170898,6.696376323699951,6.69824743270874,6.460135459899902,7.281935214996338,7.212729454040527,7.594583511352539,7.138926982879639,7.3348188400268555,7.189274311065674,6.738702774047852,6.5225324630737305,6.949594497680664,7.422399520874023,7.410975456237793,6.666720867156982,6.22360372543335,7.085495948791504,5.923482418060303,7.522467136383057,6.361387729644775,5.82284688949585,6.317162036895752,6.981962203979492,7.379582405090332,7.258055686950684,6.434112071990967,7.474478244781494,6.024131774902344,6.669892311096191,6.059913158416748,6.535154342651367,7.081905364990234,6.6076836585998535,7.319371223449707,5.877340316772461,5.5880937576293945,6.722639083862305,6.517719745635986,6.36610221862793,7.354548454284668,6.7593159675598145,6.2848405838012695,7.1035966873168945,6.770442008972168,6.092446327209473,6.611698150634766,7.249154090881348,7.035548210144043,7.410935878753662,6.4828386306762695,7.404731750488281,7.463119983673096,7.264775276184082,7.030026912689209,6.849971771240234,7.444589138031006,5.312416076660156,6.035064220428467,7.059691429138184,6.277189254760742,5.8443074226379395,6.416632652282715,7.28755521774292,6.559230327606201,6.728107929229736,6.1987409591674805,7.212503910064697,6.751069068908691,7.469141960144043,7.459601879119873,7.349392414093018,6.170513153076172,7.395143032073975,7.279488563537598,6.692648410797119,7.316223621368408,6.771309852600098,5.905599594116211,6.544557094573975,5.2595744132995605,6.352523326873779,6.099898815155029,6.16761589050293,7.125868797302246,6.7148237228393555,5.997227668762207,6.733247756958008,6.214476585388184,6.407529830932617,7.264333724975586,6.977560520172119,6.015833854675293,5.969151020050049,7.000478267669678,7.178333759307861,6.263494968414307,7.6479105949401855,5.736992835998535,7.1781744956970215,6.309168338775635,6.672365188598633,5.365495204925537,7.181882381439209,7.348421573638916,6.74544620513916,7.177699565887451,6.935952186584473,6.941099643707275,6.934011936187744,5.753427982330322,6.674023628234863,5.459040641784668,6.063246250152588,6.395949363708496,7.373455047607422,6.879143238067627,7.1031951904296875,6.42634916305542,7.1864447593688965,6.8405632972717285,3.5708701610565186,7.0937581062316895,6.570587635040283,6.882235050201416,7.279861927032471,6.178062915802002,7.118537902832031,6.493494033813477,5.847645282745361,5.426514148712158,7.372856616973877,6.965512275695801,5.870194911956787,6.197558403015137,6.458340644836426,6.9095778465271,6.297845363616943,6.318068504333496,6.843086242675781,6.492095947265625,6.620027542114258,6.984341144561768,6.305624961853027,6.843216896057129,6.46617317199707,6.985827922821045,6.762999057769775,6.4933013916015625,6.540491104125977,6.23700475692749,6.200334072113037,5.087399005889893,6.061723709106445,6.203706741333008,5.4813008308410645,6.1341447830200195,6.042251110076904,5.891076564788818,6.780318260192871,6.889206886291504,7.024605751037598,6.477628707885742,5.8595051765441895,5.17009973526001,6.039463520050049,6.932980060577393,6.82629919052124,6.837954044342041,7.210803985595703,7.130180358886719,7.339724540710449,5.242691516876221,7.033135890960693,6.220620155334473,6.3562774658203125,6.419381618499756,7.239526748657227,7.125497341156006,6.268826961517334,6.306741714477539,6.266530990600586,6.408555030822754,6.608877658843994,7.197091579437256,7.300697326660156,6.535238742828369,6.545780181884766,6.591123104095459,6.527737617492676,5.379087448120117,5.816311836242676,6.979439735412598,6.299715042114258,6.738414764404297,6.83601713180542,6.863978862762451,6.356145858764648,5.8859100341796875,6.475433349609375,7.122377872467041,6.07142448425293,6.694138526916504,5.902716636657715,5.306896686553955,6.100412845611572,6.454117774963379,6.785376071929932,6.912567138671875,5.604643821716309,7.153719425201416,6.470905780792236,6.952639102935791,5.539736270904541,6.790310382843018,6.9054856300354,6.2068681716918945,6.471749782562256,6.596714019775391,6.819143295288086,5.786790370941162,5.1841535568237305,5.967902183532715,6.736931800842285,5.688403129577637,6.72953987121582,6.925650119781494,5.920834541320801,6.706357479095459,6.801548480987549,6.692095756530762,6.381126880645752,5.92860221862793,7.283708095550537,6.068110466003418,6.476272106170654,5.462843894958496,5.8082685470581055,6.196161270141602,5.988715648651123,7.031335830688477,5.655581474304199,6.302164077758789,6.990185260772705,6.657060146331787,5.792215824127197,5.712269306182861,5.634867191314697,5.756242752075195,6.833441734313965,6.653871059417725,6.184776782989502,6.141414642333984,6.0818586349487305,6.62661600112915,6.63861083984375,6.139286994934082,5.828361988067627,5.95382833480835,6.8002848625183105,6.97227668762207,6.524081230163574,6.48838996887207,4.758155822753906,6.398387908935547,6.102293968200684,6.068881511688232,5.9436821937561035,6.9621195793151855,5.715623378753662,7.000760555267334,5.76843786239624,5.78002405166626,5.549404621124268,5.251672267913818,6.037641525268555,5.882079601287842,5.322067737579346,7.192586421966553,5.651959419250488,6.422826290130615,6.721909046173096,6.212947845458984,6.5321269035339355,6.26345157623291,5.321763038635254,6.853144645690918,5.9502129554748535,6.240609169006348,7.064380168914795,7.060173511505127,5.434298515319824,5.713833808898926,5.35958194732666,7.389200687408447,6.6098551750183105,5.971768379211426,6.044561862945557,6.879286766052246,5.353652477264404,7.192244529724121,5.155861854553223,6.459859848022461,6.823322772979736,5.7928643226623535,6.195549964904785,6.129689693450928,5.382623195648193,6.791993618011475,7.066790580749512,6.2079668045043945,5.869408130645752,4.974715709686279,3.9946670532226562,7.120031356811523,6.764587879180908,5.174720764160156,5.589199542999268,5.667579650878906,5.988559246063232,6.569380283355713,6.821842670440674,6.315308094024658,6.189703464508057,6.124805450439453,7.062331199645996,4.86233377456665,6.8343024253845215,6.483643531799316,6.55932092666626,5.6199116706848145,4.8574137687683105,6.9304399490356445,6.953507900238037,6.775815010070801,6.817715167999268,6.5624260902404785,5.681647777557373,5.837902545928955,5.0996479988098145,6.8481268882751465,6.1209235191345215,6.820996284484863,6.056341648101807,6.095166206359863,6.61842679977417,5.438260078430176,5.6815290451049805,5.080475807189941,6.436587333679199,5.884406089782715,5.9520978927612305,5.5276665687561035,6.096118927001953,6.399137020111084,4.965343952178955,6.128511428833008,7.015021324157715,5.319487571716309,6.856664180755615,6.083608150482178,6.367321968078613,5.843657493591309,6.4260478019714355,6.378108978271484,6.459695339202881,6.6272759437561035,6.059246063232422,5.8577470779418945,6.326718807220459,5.700711727142334,6.2530927658081055,6.536418914794922,5.880959987640381,6.49962043762207,6.309074401855469,5.8287248611450195,5.960597991943359,6.050961494445801,5.545186519622803,6.5997138023376465,5.92969274520874,6.413270950317383,6.187255382537842,6.180765151977539,6.33779239654541,6.12028694152832,6.397641181945801,5.3044657707214355,6.83604621887207,6.510043621063232,6.346192836761475,6.65885066986084,6.221997261047363,6.662997245788574,5.459427833557129,6.85530948638916,5.3961591720581055,5.622014045715332,5.986109733581543,6.281040191650391,5.967301845550537,6.529374599456787,6.527423858642578,5.974690914154053,6.404294490814209,5.865479469299316,5.839770317077637,5.659456729888916,6.732898235321045,5.857100009918213,6.750272750854492,6.411985397338867,6.792856216430664,6.249256134033203,6.093013286590576,6.611140727996826,5.683406829833984,5.102623462677002,6.719038963317871,6.895438194274902,6.569279670715332,6.300320148468018,6.2968339920043945,6.411684036254883,6.303232669830322,6.670129299163818,6.057904243469238,6.274672031402588,5.103349208831787,6.7471022605896,6.320355415344238,6.9138264656066895,4.943398952484131,5.276199817657471,6.158751487731934,6.529575824737549,6.511270999908447,6.563686847686768,6.825718402862549,6.0076751708984375,5.515554428100586,6.78159236907959,4.525760650634766,5.7860331535339355,6.884456634521484,4.664862155914307,6.475404262542725,5.991855621337891,5.9356369972229,6.793413162231445,6.159652233123779,6.055816650390625,6.205516815185547,5.20932149887085,6.274753570556641,5.284594535827637,4.9419779777526855,4.458980560302734,6.209834098815918,5.361738681793213,5.023915767669678,5.331613540649414,6.29410982131958,6.466154098510742,5.626280784606934,6.35620641708374,5.478240489959717,5.7116475105285645,5.892956256866455,6.420624256134033,5.794150352478027,5.921229839324951,6.869009017944336,6.1308183670043945,5.915657043457031,6.974363327026367,5.450153350830078,6.449761867523193,6.045516014099121,5.421005725860596,6.253221035003662,5.2270307540893555,6.376247406005859,6.1876349449157715,6.06260871887207,6.720747470855713,5.627060890197754,6.283331394195557,6.351734161376953,5.523952960968018,6.370999336242676,6.451564311981201,6.202211856842041,4.996163845062256,5.659505367279053,5.903824329376221,5.46745491027832,6.066342830657959,6.6164679527282715,4.496311187744141,6.918656349182129,6.374454021453857,6.624735355377197,6.1405181884765625,6.419637680053711,5.368797779083252,5.63762092590332,6.130258083343506,5.883955001831055,5.349919319152832,5.981701850891113,6.619306564331055,5.355196952819824,6.703455924987793,6.215742111206055,6.538422107696533,6.278171539306641,6.039126396179199,5.870230674743652,6.178444862365723,6.194089412689209,6.146444797515869,6.731659889221191,5.233891010284424,5.954592704772949,5.619664669036865,6.739044666290283,5.9534220695495605,6.210587978363037,6.765751361846924,6.739131450653076,5.395712852478027,6.435729503631592,6.559391021728516,6.6493916511535645,5.664209365844727,5.9365363121032715,5.80687952041626,6.157029628753662,6.582067012786865,5.272243976593018,6.387413024902344,6.019307613372803,5.77635383605957,5.32295036315918,5.941738128662109,6.576267242431641,5.763748645782471,5.9041008949279785,5.74242639541626,6.283062934875488,6.257469177246094,6.562018394470215,6.809732437133789,6.094371795654297,6.236048221588135,4.181057929992676,6.251044273376465,6.477835178375244,6.046085357666016,5.837491512298584,6.458290100097656,5.670943260192871,5.2971978187561035,6.108110427856445,5.750730991363525,5.690359115600586,6.7756781578063965,5.08757209777832,6.263877868652344,6.261489391326904,5.932858467102051,6.66238260269165,5.9046430587768555,6.724463939666748,5.962268829345703,6.093897342681885,5.88328742980957,6.031794548034668,5.620194911956787,6.442348957061768,6.230268955230713,5.84187650680542,6.334973335266113,6.030936241149902,6.146600246429443,5.107158660888672,5.6236724853515625,6.439016342163086,6.273532390594482,6.079730033874512,5.274417877197266,5.807642459869385,5.274596214294434,5.956326961517334,6.173469543457031,6.205816745758057,5.946310997009277,5.619250297546387,7.0108642578125,6.019609451293945,6.104399681091309,5.485321998596191,6.42507791519165,6.118117332458496,5.677605628967285,6.27752685546875,5.981806755065918,5.951730251312256,6.593650817871094,5.710947036743164,5.852570056915283,5.670546054840088,6.290445327758789,5.928374290466309,6.097687244415283,6.00021505355835,3.711148977279663,6.25396728515625,5.271996021270752,5.282454967498779,6.487525463104248,6.3335957527160645,5.35158109664917,5.694026470184326,6.299812316894531,6.349177837371826,6.209733486175537,6.428217887878418,5.592421531677246,5.482235431671143,5.584145545959473,5.41577672958374,6.766429424285889,5.789606094360352,5.375753402709961,6.307466506958008,6.318812370300293,6.481020450592041,5.817994594573975,6.273177623748779,6.722898006439209,5.338503837585449,6.670080661773682,5.980781078338623,5.287014007568359,5.842097282409668,5.419844627380371,5.756831645965576,5.9169745445251465,5.530550479888916,6.213921070098877,6.109903812408447,5.853615760803223,6.207830429077148,5.492738246917725,6.175370216369629,3.9055612087249756,6.336564064025879,4.95462703704834,5.901712417602539,5.375865936279297,5.625036716461182,4.983633518218994,6.198032855987549,5.082873344421387,4.950484752655029,5.829664707183838,5.235795497894287,6.1329569816589355,5.456839084625244,5.522766590118408,5.183165073394775,5.625036716461182,5.867244720458984,5.701903820037842,6.661999702453613,6.231913089752197,6.1269636154174805,4.888288974761963,5.385567665100098,5.985903263092041,6.738123893737793,5.537220478057861,5.86268424987793,6.744391441345215,6.640661716461182,5.472293853759766,5.546179294586182,5.534297943115234,6.367386341094971,5.67014741897583,6.480928897857666,5.720998287200928,6.059472560882568,5.823733806610107,4.560082912445068,5.55682373046875,4.636996746063232,5.792925834655762,5.952547550201416,5.223880290985107,6.596568584442139,6.045695781707764,5.875568866729736,6.208745956420898,6.532783508300781,6.210654258728027,6.346561908721924,6.338615894317627,5.9337358474731445,6.433508396148682,6.319293975830078,5.811450004577637,5.49641752243042,5.907059192657471,5.746159553527832,6.503262519836426,5.756721496582031,6.488044738769531,6.117894649505615,5.86445426940918,5.835223197937012,5.549563884735107,5.330730438232422,4.362559795379639,5.012577056884766,5.569671630859375,6.176525592803955,5.664442539215088,6.331360340118408,5.243968486785889,5.100625514984131,6.374338150024414,6.497174263000488,5.70150899887085,5.64119291305542,5.607527732849121,5.71267557144165,6.280437469482422,5.915352821350098,5.677097320556641,6.360349178314209,6.265502452850342,6.178948402404785,5.341156482696533,5.870170593261719,5.4451117515563965,6.241472244262695,5.314521312713623,4.9461517333984375,6.680211544036865,6.917564868927002,6.49563455581665,6.11849308013916,5.478568077087402,6.257815837860107,6.127716064453125,5.675842761993408,6.210302829742432,6.548689842224121,5.86232852935791,5.740886688232422,6.508115768432617,5.909424304962158,6.129854679107666,5.92615270614624,6.129012107849121,5.991880416870117,6.622316837310791,5.540594577789307,6.060328006744385,5.931400775909424,6.792439937591553,5.400372505187988,5.405616283416748,5.465909481048584,5.291983127593994,6.062725067138672,5.695817470550537,6.294777870178223,6.29988431930542,6.702127933502197,5.836272239685059,6.2906880378723145,5.82119083404541,6.386631965637207,3.914015293121338,4.600915431976318,5.786273956298828,5.965373516082764,5.688570022583008,5.874922752380371,5.608105659484863,6.473889350891113,6.068475246429443,5.559708118438721,6.369912147521973,5.660721302032471,6.10959529876709,5.171573162078857,5.885856628417969,6.098738670349121,5.7309136390686035,6.036271095275879,5.380069732666016,6.102173805236816,4.849461078643799,5.115664958953857,6.431756019592285,6.017018795013428,6.432150363922119,5.532230377197266,5.253816604614258,6.474475860595703,3.9770805835723877,6.072547912597656,5.884472370147705,6.253187656402588,5.945555686950684,6.096212863922119,6.4258246421813965,5.689128875732422,6.403471946716309,5.640111446380615,6.316356658935547,5.253259658813477,3.884716033935547,5.994566440582275,6.261377334594727,6.544004917144775,5.812011241912842,5.568344593048096,6.187015056610107,6.731874465942383,4.954038619995117,5.797821044921875,6.174631595611572,4.538426399230957,6.5726213455200195,4.4626264572143555,5.83879280090332,6.313039302825928,5.421074867248535,5.935083389282227,4.370193004608154,6.679178714752197,6.659379959106445,6.706349849700928,5.316075325012207,5.927857875823975,5.195620536804199,5.5007004737854,6.650441646575928,6.306372165679932,6.594603538513184,4.7035698890686035,4.553125381469727,6.798477649688721,5.679795265197754,5.9527788162231445,4.551840305328369,5.673043251037598,6.304561614990234,6.020262241363525,6.0588297843933105,5.522863388061523,5.2381768226623535,6.00363302230835,6.389254570007324,5.4175825119018555,6.422732830047607,5.610039234161377,5.911554336547852,5.2098002433776855,5.918560028076172,5.055022239685059,6.686042308807373,5.942429542541504,5.942678928375244,5.244865894317627,6.026693344116211,6.240598678588867,4.710692405700684,5.621710777282715,5.577942371368408,5.0781660079956055,5.779727458953857,4.423628330230713],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Training curve for my tiny demo model!\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d657c504-b9c0-4f64-b88a-1888977a57e1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(y=losses, x=np.arange(len(losses))*(model_cfg.n_ctx * batch_size), labels={\"y\":\"Loss\", \"x\":\"Tokens\"}, title=\"Training curve for my tiny demo model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
