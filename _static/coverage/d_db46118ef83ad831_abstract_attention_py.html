<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Coverage for transformer_lens/components/abstract_attention.py: 79%</title>
    <link rel="icon" sizes="32x32" href="favicon_32.png">
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="coverage_html.js" defer></script>
</head>
<body class="pyfile">
<header>
    <div class="content">
        <h1>
            <span class="text">Coverage for </span><b>transformer_lens/components/abstract_attention.py</b>:
            <span class="pc_cov">79%</span>
        </h1>
        <aside id="help_panel_wrapper">
            <input id="help_panel_state" type="checkbox">
            <label for="help_panel_state">
                <img id="keyboard_icon" src="keybd_closed.png" alt="Show/hide keyboard shortcuts" />
            </label>
            <div id="help_panel">
                <p class="legend">Shortcuts on this page</p>
                <div class="keyhelp">
                    <p>
                        <kbd>r</kbd>
                        <kbd>m</kbd>
                        <kbd>x</kbd>
                        <kbd>p</kbd>
                        &nbsp; toggle line displays
                    </p>
                    <p>
                        <kbd>j</kbd>
                        <kbd>k</kbd>
                        &nbsp; next/prev highlighted chunk
                    </p>
                    <p>
                        <kbd>0</kbd> &nbsp; (zero) top of page
                    </p>
                    <p>
                        <kbd>1</kbd> &nbsp; (one) first highlighted chunk
                    </p>
                    <p>
                        <kbd>[</kbd>
                        <kbd>]</kbd>
                        &nbsp; prev/next file
                    </p>
                    <p>
                        <kbd>u</kbd> &nbsp; up to the index
                    </p>
                    <p>
                        <kbd>?</kbd> &nbsp; show/hide this help
                    </p>
                </div>
            </div>
        </aside>
        <h2>
            <span class="text">246 statements &nbsp;</span>
            <button type="button" class="run button_toggle_run" value="run" data-shortcut="r" title="Toggle lines run">202<span class="text"> run</span></button>
            <button type="button" class="mis show_mis button_toggle_mis" value="mis" data-shortcut="m" title="Toggle lines missing">44<span class="text"> missing</span></button>
            <button type="button" class="exc show_exc button_toggle_exc" value="exc" data-shortcut="x" title="Toggle lines excluded">0<span class="text"> excluded</span></button>
            <button type="button" class="par run show_par button_toggle_par" value="par" data-shortcut="p" title="Toggle lines partially run">22<span class="text"> partial</span></button>
        </h2>
        <p class="text">
            <a id="prevFileLink" class="nav" href="d_db46118ef83ad831___init___py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_db46118ef83ad831_attention_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2025-05-15 21:26 +0000
        </p>
        <aside class="hidden">
            <button type="button" class="button_next_chunk" data-shortcut="j"/>
            <button type="button" class="button_prev_chunk" data-shortcut="k"/>
            <button type="button" class="button_top_of_page" data-shortcut="0"/>
            <button type="button" class="button_first_chunk" data-shortcut="1"/>
            <button type="button" class="button_prev_file" data-shortcut="["/>
            <button type="button" class="button_next_file" data-shortcut="]"/>
            <button type="button" class="button_to_index" data-shortcut="u"/>
            <button type="button" class="button_show_hide_help" data-shortcut="?"/>
        </aside>
    </div>
</header>
<main id="source">
    <p class="run"><span class="n"><a id="t1" href="#t1">1</a></span><span class="t"><span class="key">import</span> <span class="nam">math</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2" href="#t2">2</a></span><span class="t"><span class="key">from</span> <span class="nam">abc</span> <span class="key">import</span> <span class="nam">ABC</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t3" href="#t3">3</a></span><span class="t"><span class="key">from</span> <span class="nam">typing</span> <span class="key">import</span> <span class="nam">Dict</span><span class="op">,</span> <span class="nam">Optional</span><span class="op">,</span> <span class="nam">Tuple</span><span class="op">,</span> <span class="nam">Union</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t4" href="#t4">4</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t5" href="#t5">5</a></span><span class="t"><span class="key">import</span> <span class="nam">einops</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t6" href="#t6">6</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t7" href="#t7">7</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">nn</span> <span class="key">as</span> <span class="nam">nn</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t8" href="#t8">8</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">nn</span><span class="op">.</span><span class="nam">functional</span> <span class="key">as</span> <span class="nam">F</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t9" href="#t9">9</a></span><span class="t"><span class="key">from</span> <span class="nam">better_abc</span> <span class="key">import</span> <span class="nam">abstract_attribute</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t10" href="#t10">10</a></span><span class="t"><span class="key">from</span> <span class="nam">jaxtyping</span> <span class="key">import</span> <span class="nam">Float</span><span class="op">,</span> <span class="nam">Int</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t11" href="#t11">11</a></span><span class="t"><span class="key">from</span> <span class="nam">transformers</span><span class="op">.</span><span class="nam">utils</span> <span class="key">import</span> <span class="nam">is_bitsandbytes_available</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t12" href="#t12">12</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t13" href="#t13">13</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">FactoredMatrix</span> <span class="key">import</span> <span class="nam">FactoredMatrix</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t14" href="#t14">14</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">hook_points</span> <span class="key">import</span> <span class="nam">HookPoint</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t15" href="#t15">15</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">HookedTransformerConfig</span> <span class="key">import</span> <span class="nam">HookedTransformerConfig</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t16" href="#t16">16</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">past_key_value_caching</span> <span class="key">import</span> <span class="nam">HookedTransformerKeyValueCacheEntry</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t17" href="#t17">17</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utilities</span><span class="op">.</span><span class="nam">attention</span> <span class="key">import</span> <span class="nam">complex_attn_linear</span><span class="op">,</span> <span class="nam">simple_attn_linear</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t18" href="#t18">18</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utils</span> <span class="key">import</span> <span class="nam">get_offset_position_ids</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t19" href="#t19">19</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t20" href="#t20">20</a></span><span class="t"><span class="key">if</span> <span class="nam">is_bitsandbytes_available</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">20&#x202F;&#x219B;&#x202F;21</span><span class="annotate long">line 20 didn't jump to line 21, because the condition on line 20 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t21" href="#t21">21</a></span><span class="t">    <span class="key">import</span> <span class="nam">bitsandbytes</span> <span class="key">as</span> <span class="nam">bnb</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t22" href="#t22">22</a></span><span class="t">    <span class="key">from</span> <span class="nam">bitsandbytes</span><span class="op">.</span><span class="nam">nn</span><span class="op">.</span><span class="nam">modules</span> <span class="key">import</span> <span class="nam">Params4bit</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t23" href="#t23">23</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t24" href="#t24">24</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t25" href="#t25">25</a></span><span class="t"><span class="key">class</span> <span class="nam">AbstractAttention</span><span class="op">(</span><span class="nam">ABC</span><span class="op">,</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Module</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t26" href="#t26">26</a></span><span class="t">    <span class="nam">alibi</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t27" href="#t27">27</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t28" href="#t28">28</a></span><span class="t">    <span class="key">def</span> <span class="nam">__init__</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t29" href="#t29">29</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t30" href="#t30">30</a></span><span class="t">        <span class="nam">cfg</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Dict</span><span class="op">,</span> <span class="nam">HookedTransformerConfig</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t31" href="#t31">31</a></span><span class="t">        <span class="nam">attn_type</span><span class="op">:</span> <span class="nam">str</span> <span class="op">=</span> <span class="str">"global"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t32" href="#t32">32</a></span><span class="t">        <span class="nam">layer_id</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t33" href="#t33">33</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t34" href="#t34">34</a></span><span class="t">        <span class="str">"""Abstract Base Class of Attention Blocks, featuring common functionality of both Attention and GroupedQueryAttention blocks.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t35" href="#t35">35</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t36" href="#t36">36</a></span><span class="t"><span class="str">        Query and Output projections are defined in this class as they are the same for regular and grouped query attention.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t37" href="#t37">37</a></span><span class="t"><span class="str">        Attributes related to Key and Value projections are abstract as their implementations may differ. For example, in GroupedQueryAttention there are less query and key heads than value heads.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t38" href="#t38">38</a></span><span class="t"><span class="str">        To enforce implementation of W_K, W_V, b_K, and b_V by child classes, the better_abc.abstract_attribute class is used. See here for details: https://stackoverflow.com/questions/23831510/abstract-attribute-not-property.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t39" href="#t39">39</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t40" href="#t40">40</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t41" href="#t41">41</a></span><span class="t"><span class="str">            cfg (Union[Dict, HookedTransformerConfig]): Config</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t42" href="#t42">42</a></span><span class="t"><span class="str">            attn_type (str, optional): "global" or "local", used by GPT-Neo. Local attention means the model can only attend back cfg.window_size tokens (here, 256). Not used by any other model at the moment. Defaults to "global".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t43" href="#t43">43</a></span><span class="t"><span class="str">            layer_id (int, optional): The index of the current layer. Used by the Mistral models (labelled here as stanford-gpt2) to scale down attention scores pre softmax for numerical stability reasons by 1/(layer_id+1). Defaults to None.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t44" href="#t44">44</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t45" href="#t45">45</a></span><span class="t">        <span class="nam">super</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">__init__</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t46" href="#t46">46</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span> <span class="op">=</span> <span class="nam">HookedTransformerConfig</span><span class="op">.</span><span class="nam">unwrap</span><span class="op">(</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t47" href="#t47">47</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t48" href="#t48">48</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">48&#x202F;&#x219B;&#x202F;49</span><span class="annotate long">line 48 didn't jump to line 49, because the condition on line 48 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t49" href="#t49">49</a></span><span class="t">            <span class="nam">nq</span> <span class="op">=</span> <span class="nam">int</span><span class="op">(</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_model</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span> <span class="op">/</span> <span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t50" href="#t50">50</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span> <span class="op">=</span> <span class="nam">Params4bit</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">empty</span><span class="op">(</span><span class="nam">nq</span><span class="op">,</span> <span class="num">1</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">uint8</span><span class="op">)</span><span class="op">,</span> <span class="nam">requires_grad</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t51" href="#t51">51</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span> <span class="op">=</span> <span class="nam">Params4bit</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">empty</span><span class="op">(</span><span class="nam">nq</span><span class="op">,</span> <span class="num">1</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">uint8</span><span class="op">)</span><span class="op">,</span> <span class="nam">requires_grad</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t52" href="#t52">52</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t53" href="#t53">53</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span> <span class="op">=</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t54" href="#t54">54</a></span><span class="t">                <span class="nam">torch</span><span class="op">.</span><span class="nam">empty</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t55" href="#t55">55</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t56" href="#t56">56</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_model</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t57" href="#t57">57</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t58" href="#t58">58</a></span><span class="t">                    <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t59" href="#t59">59</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t60" href="#t60">60</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t61" href="#t61">61</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span> <span class="op">=</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t62" href="#t62">62</a></span><span class="t">                <span class="nam">torch</span><span class="op">.</span><span class="nam">empty</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t63" href="#t63">63</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t64" href="#t64">64</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t65" href="#t65">65</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_model</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t66" href="#t66">66</a></span><span class="t">                    <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t67" href="#t67">67</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t68" href="#t68">68</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t69" href="#t69">69</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span> <span class="op">=</span> <span class="nam">abstract_attribute</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t70" href="#t70">70</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span> <span class="op">=</span> <span class="nam">abstract_attribute</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t71" href="#t71">71</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t72" href="#t72">72</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">b_Q</span> <span class="op">=</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t73" href="#t73">73</a></span><span class="t">            <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t74" href="#t74">74</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t75" href="#t75">75</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">b_K</span><span class="op">:</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span> <span class="op">=</span> <span class="nam">abstract_attribute</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t76" href="#t76">76</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">b_V</span><span class="op">:</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span> <span class="op">=</span> <span class="nam">abstract_attribute</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t77" href="#t77">77</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span> <span class="op">=</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Parameter</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_model</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t78" href="#t78">78</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t79" href="#t79">79</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">attn_type</span> <span class="op">=</span> <span class="nam">attn_type</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t80" href="#t80">80</a></span><span class="t">        <span class="com"># Create a max_ctx x max_ctx mask, with True iff that query position</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t81" href="#t81">81</a></span><span class="t">        <span class="com"># can attend to that key position (query is first axis, key is second axis)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t82" href="#t82">82</a></span><span class="t">        <span class="nam">causal_mask</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tril</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ones</span><span class="op">(</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_ctx</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_ctx</span><span class="op">)</span><span class="op">)</span><span class="op">.</span><span class="nam">bool</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t83" href="#t83">83</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">attn_type</span> <span class="op">==</span> <span class="str">"global"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t84" href="#t84">84</a></span><span class="t">            <span class="com"># For global attention, this is a lower triangular matrix - key &lt;= query</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t85" href="#t85">85</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">register_buffer</span><span class="op">(</span><span class="str">"mask"</span><span class="op">,</span> <span class="nam">causal_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t86" href="#t86">86</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">attn_type</span> <span class="op">==</span> <span class="str">"local"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">86&#x202F;&#x219B;&#x202F;92</span><span class="annotate long">line 86 didn't jump to line 92, because the condition on line 86 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t87" href="#t87">87</a></span><span class="t">            <span class="com"># For local, this is banded, query - window_size &lt; key &lt;= query</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t88" href="#t88">88</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">window_size</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">88&#x202F;&#x219B;&#x202F;89</span><span class="annotate long">line 88 didn't jump to line 89, because the condition on line 88 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t89" href="#t89">89</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Window size must be an integer for local attention"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t90" href="#t90">90</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">register_buffer</span><span class="op">(</span><span class="str">"mask"</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">triu</span><span class="op">(</span><span class="nam">causal_mask</span><span class="op">,</span> <span class="num">1</span> <span class="op">-</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">window_size</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t91" href="#t91">91</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t92" href="#t92">92</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"Invalid attention type: {self.attn_type}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t93" href="#t93">93</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t94" href="#t94">94</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">register_buffer</span><span class="op">(</span><span class="str">"IGNORE"</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="op">-</span><span class="nam">torch</span><span class="op">.</span><span class="nam">inf</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t95" href="#t95">95</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t96" href="#t96">96</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">layer_id</span> <span class="op">=</span> <span class="nam">layer_id</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t97" href="#t97">97</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t98" href="#t98">98</a></span><span class="t">        <span class="com"># attn_scale is a constant that we divide the attention scores by pre-softmax. I'm not entirely sure why it matters, but it's probably a mix of softmax not being scale invariant and numerical stability?</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t99" href="#t99">99</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_scale</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t100" href="#t100">100</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">attn_scale</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_scale</span>  <span class="com"># Defaults to sqrt(d_head)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t101" href="#t101">101</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t102" href="#t102">102</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">attn_scale</span> <span class="op">=</span> <span class="num">1.0</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t103" href="#t103">103</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">scale_attn_by_inverse_layer_idx</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t104" href="#t104">104</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">layer_id</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"><span class="annotate short">104&#x202F;&#x219B;&#x202F;105</span><span class="annotate long">line 104 didn't jump to line 105, because the condition on line 104 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t105" href="#t105">105</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Layer ID must be provided to scale attention scores"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t106" href="#t106">106</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">attn_scale</span> <span class="op">*=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">layer_id</span> <span class="op">+</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t107" href="#t107">107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t108" href="#t108">108</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_k</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t109" href="#t109">109</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_q</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t110" href="#t110">110</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_v</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t111" href="#t111">111</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_z</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t112" href="#t112">112</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_attn_scores</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t113" href="#t113">113</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pattern</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t114" href="#t114">114</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_result</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t115" href="#t115">115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t116" href="#t116">116</a></span><span class="t">        <span class="com"># See HookedTransformerConfig for more details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t117" href="#t117">117</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"shortformer"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t118" href="#t118">118</a></span><span class="t">            <span class="com"># This tracks the input to the keys and queries, which is resid_pre + pos_embeds</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t119" href="#t119">119</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_attn_input</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t120" href="#t120">120</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t121" href="#t121">121</a></span><span class="t">            <span class="com"># Applies a rotation to each two-element chunk of keys and queries pre dot producting to bake in relative position. See HookedTransformerConfig for details</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t122" href="#t122">122</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_rot_k</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t123" href="#t123">123</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_rot_q</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t124" href="#t124">124</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_dim</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"><span class="annotate short">124&#x202F;&#x219B;&#x202F;125</span><span class="annotate long">line 124 didn't jump to line 125, because the condition on line 124 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t125" href="#t125">125</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Rotary dim must be provided for rotary positional embeddings"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t126" href="#t126">126</a></span><span class="t">            <span class="nam">sin</span><span class="op">,</span> <span class="nam">cos</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">calculate_sin_cos_rotary</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t127" href="#t127">127</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_dim</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t128" href="#t128">128</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_ctx</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t129" href="#t129">129</a></span><span class="t">                <span class="nam">base</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_base</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t130" href="#t130">130</a></span><span class="t">                <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t131" href="#t131">131</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t132" href="#t132">132</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">register_buffer</span><span class="op">(</span><span class="str">"rotary_sin"</span><span class="op">,</span> <span class="nam">sin</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t133" href="#t133">133</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">register_buffer</span><span class="op">(</span><span class="str">"rotary_cos"</span><span class="op">,</span> <span class="nam">cos</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t134" href="#t134">134</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"alibi"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t135" href="#t135">135</a></span><span class="t">            <span class="com"># ALiBi bias wil be constructed on the first forward pass.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t136" href="#t136">136</a></span><span class="t">            <span class="com"># Note: While computationally efficient, initializing an bias with max n_ctx (16, 1024, 1024) of float32 will occupy ~256MiB of contiguous GPU memory, which may not be optimal for memory usage.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t137" href="#t137">137</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">alibi</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t138" href="#t138">138</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t139" href="#t139">139</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"relative_positional_bias"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t140" href="#t140">140</a></span><span class="t">            <span class="com"># will be overwritten by the child T5Attention class</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t141" href="#t141">141</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">has_relative_attention_bias</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t142" href="#t142">142</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t143" href="#t143">143</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t144" href="#t144">144</a></span><span class="t">    <span class="key">def</span> <span class="nam">OV</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">FactoredMatrix</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t145" href="#t145">145</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t146" href="#t146">146</a></span><span class="t"><span class="str">        OV-Circuit, as defined in A Mathematical Framework. Because there's no non-linearity between the value vector and the output of the layer, the output is purely determined by the matrix W_OV = W_V @ W_O, and not W_V or W_O individually. (Mathematically, for a single head, output == pattern @ residual @ W_V @ W_O, see the glossary for more)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t147" href="#t147">147</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t148" href="#t148">148</a></span><span class="t"><span class="str">        Done in the order W_V, W_O because the paper uses left-multiplying weight matrices, and TransformerLens uses right-multiplying, sorry!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t149" href="#t149">149</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t150" href="#t150">150</a></span><span class="t"><span class="str">        Returns a FactoredMatrix, with left matrix W_V [head_index, d_model, d_head] and right matrix W_O [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model]. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the OV circuit of a head k, attn.OV[k] works.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t151" href="#t151">151</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t152" href="#t152">152</a></span><span class="t">        <span class="key">return</span> <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t153" href="#t153">153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t154" href="#t154">154</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t155" href="#t155">155</a></span><span class="t">    <span class="key">def</span> <span class="nam">QK</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">FactoredMatrix</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t156" href="#t156">156</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t157" href="#t157">157</a></span><span class="t"><span class="str">        QK-Circuit, as defined in A Mathematical Framework. Because there's no non-linearity in the key-query dot product, the output is purely determined by the matrix W_QK = W_Q.T @ W_K, and not W_Q or W_K individually. (Mathematically, for a single head, pattern = destination_residual.T @ W_Q.T @ W_K @ source-residual, see the glossary for more).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t158" href="#t158">158</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t159" href="#t159">159</a></span><span class="t"><span class="str">        Done in the order Q on the left, K on the right, because the pattern has dimensions [destination_pos, source_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t160" href="#t160">160</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t161" href="#t161">161</a></span><span class="t"><span class="str">        Returns a FactoredMatrix, with left matrix W_Q [head_index, d_model, d_head] and right matrix W_K.T [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model] matrix. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the QK circuit of a head k, attn.QK[k] works.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t162" href="#t162">162</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t163" href="#t163">163</a></span><span class="t">        <span class="nam">W_K_transpose</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t164" href="#t164">164</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">,</span> <span class="str">"head_index d_model d_head -> head_index d_head d_model"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t165" href="#t165">165</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t166" href="#t166">166</a></span><span class="t">        <span class="key">return</span> <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span><span class="op">,</span> <span class="nam">W_K_transpose</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t167" href="#t167">167</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t168" href="#t168">168</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t169" href="#t169">169</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t170" href="#t170">170</a></span><span class="t">        <span class="nam">query_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t171" href="#t171">171</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t172" href="#t172">172</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t173" href="#t173">173</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t174" href="#t174">174</a></span><span class="t">        <span class="nam">key_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t175" href="#t175">175</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t176" href="#t176">176</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t177" href="#t177">177</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos kv_head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t178" href="#t178">178</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t179" href="#t179">179</a></span><span class="t">        <span class="nam">value_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t180" href="#t180">180</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t181" href="#t181">181</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t182" href="#t182">182</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos kv_head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t183" href="#t183">183</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t184" href="#t184">184</a></span><span class="t">        <span class="nam">past_kv_cache_entry</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCacheEntry</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t185" href="#t185">185</a></span><span class="t">        <span class="nam">additive_attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch 1 1 kv_pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t186" href="#t186">186</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch offset_pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t187" href="#t187">187</a></span><span class="t">        <span class="nam">position_bias</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"1 head_index pos kv_pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t188" href="#t188">188</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t189" href="#t189">189</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t190" href="#t190">190</a></span><span class="t"><span class="str">        shortformer_pos_embed is only used if self.cfg.positional_embedding_type == "shortformer", else defaults to None and is irrelevant. See HookedTransformerConfig for more details</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t191" href="#t191">191</a></span><span class="t"><span class="str">        past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t192" href="#t192">192</a></span><span class="t"><span class="str">        additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t193" href="#t193">193</a></span><span class="t"><span class="str">        attention_mask is the attention mask for padded tokens. Defaults to None.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t194" href="#t194">194</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t195" href="#t195">195</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t196" href="#t196">196</a></span><span class="t">        <span class="nam">q</span><span class="op">,</span> <span class="nam">k</span><span class="op">,</span> <span class="nam">v</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">calculate_qkv_matrices</span><span class="op">(</span><span class="nam">query_input</span><span class="op">,</span> <span class="nam">key_input</span><span class="op">,</span> <span class="nam">value_input</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t197" href="#t197">197</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t198" href="#t198">198</a></span><span class="t">        <span class="key">if</span> <span class="nam">past_kv_cache_entry</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t199" href="#t199">199</a></span><span class="t">            <span class="com"># Appends the new keys and values to the cached values, and automatically updates the cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t200" href="#t200">200</a></span><span class="t">            <span class="nam">kv_cache_pos_offset</span> <span class="op">=</span> <span class="nam">past_kv_cache_entry</span><span class="op">.</span><span class="nam">past_keys</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t201" href="#t201">201</a></span><span class="t">            <span class="nam">k</span><span class="op">,</span> <span class="nam">v</span> <span class="op">=</span> <span class="nam">past_kv_cache_entry</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">k</span><span class="op">,</span> <span class="nam">v</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t202" href="#t202">202</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t203" href="#t203">203</a></span><span class="t">            <span class="com"># Not using a cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t204" href="#t204">204</a></span><span class="t">            <span class="nam">kv_cache_pos_offset</span> <span class="op">=</span> <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t205" href="#t205">205</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t206" href="#t206">206</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t207" href="#t207">207</a></span><span class="t">            <span class="nam">q</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_rot_q</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">apply_rotary</span><span class="op">(</span><span class="nam">q</span><span class="op">,</span> <span class="nam">kv_cache_pos_offset</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t208" href="#t208">208</a></span><span class="t">            <span class="nam">k</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_rot_k</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t209" href="#t209">209</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">apply_rotary</span><span class="op">(</span><span class="nam">k</span><span class="op">,</span> <span class="num">0</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t210" href="#t210">210</a></span><span class="t">            <span class="op">)</span>  <span class="com"># keys are cached so no offset</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t211" href="#t211">211</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t212" href="#t212">212</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">not</span> <span class="key">in</span> <span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float64</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">212&#x202F;&#x219B;&#x202F;214</span><span class="annotate long">line 212 didn't jump to line 214, because the condition on line 212 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t213" href="#t213">213</a></span><span class="t">            <span class="com"># If using 16 bits, increase the precision to avoid numerical instabilities</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t214" href="#t214">214</a></span><span class="t">            <span class="nam">q</span> <span class="op">=</span> <span class="nam">q</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t215" href="#t215">215</a></span><span class="t">            <span class="nam">k</span> <span class="op">=</span> <span class="nam">k</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t216" href="#t216">216</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t217" href="#t217">217</a></span><span class="t">        <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">calculate_attention_scores</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t218" href="#t218">218</a></span><span class="t">            <span class="nam">q</span><span class="op">,</span> <span class="nam">k</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t219" href="#t219">219</a></span><span class="t">        <span class="op">)</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t220" href="#t220">220</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t221" href="#t221">221</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"alibi"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t222" href="#t222">222</a></span><span class="t">            <span class="nam">query_ctx</span> <span class="op">=</span> <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t223" href="#t223">223</a></span><span class="t">            <span class="com"># The key context length is the number of positions in the past - this includes all positions in the cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t224" href="#t224">224</a></span><span class="t">            <span class="nam">key_ctx</span> <span class="op">=</span> <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t225" href="#t225">225</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t226" href="#t226">226</a></span><span class="t">            <span class="com"># only recompute when necessary to increase efficiency.</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t227" href="#t227">227</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">alibi</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">key_ctx</span> <span class="op">></span> <span class="nam">self</span><span class="op">.</span><span class="nam">alibi</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">227&#x202F;&#x219B;&#x202F;233</span><span class="annotate long">line 227 didn't jump to line 233, because the condition on line 227 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t228" href="#t228">228</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">alibi</span> <span class="op">=</span> <span class="nam">AbstractAttention</span><span class="op">.</span><span class="nam">create_alibi_bias</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t229" href="#t229">229</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span> <span class="nam">key_ctx</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t230" href="#t230">230</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t231" href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t232" href="#t232">232</a></span><span class="t">            <span class="com"># Take the last query_ctx positions so it also works with past_kv_cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t233" href="#t233">233</a></span><span class="t">            <span class="nam">attn_scores</span> <span class="op">+=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">alibi</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t234" href="#t234">234</a></span><span class="t">                <span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="nam">query_ctx</span><span class="op">:</span><span class="op">,</span> <span class="op">:</span><span class="nam">key_ctx</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t235" href="#t235">235</a></span><span class="t">            <span class="op">]</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t236" href="#t236">236</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"relative_positional_bias"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t237" href="#t237">237</a></span><span class="t">            <span class="key">if</span> <span class="nam">position_bias</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t238" href="#t238">238</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_relative_attention_bias</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">238&#x202F;&#x219B;&#x202F;239</span><span class="annotate long">line 238 didn't jump to line 239, because the condition on line 238 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t239" href="#t239">239</a></span><span class="t">                    <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Positional bias is required for relative_positional_bias"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t240" href="#t240">240</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t241" href="#t241">241</a></span><span class="t">                    <span class="nam">position_bias</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t242" href="#t242">242</a></span><span class="t">                        <span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t243" href="#t243">243</a></span><span class="t">                        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t244" href="#t244">244</a></span><span class="t">                        <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">2</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t245" href="#t245">245</a></span><span class="t">                        <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">3</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t246" href="#t246">246</a></span><span class="t">                        <span class="nam">device</span><span class="op">=</span><span class="nam">attn_scores</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t247" href="#t247">247</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t248" href="#t248">248</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t249" href="#t249">249</a></span><span class="t">            <span class="nam">attn_scores</span> <span class="op">+=</span> <span class="nam">position_bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t250" href="#t250">250</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attention_dir</span> <span class="op">==</span> <span class="str">"causal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t251" href="#t251">251</a></span><span class="t">            <span class="com"># If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t252" href="#t252">252</a></span><span class="t">            <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_causal_mask</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t253" href="#t253">253</a></span><span class="t">                <span class="nam">attn_scores</span><span class="op">,</span> <span class="nam">kv_cache_pos_offset</span><span class="op">,</span> <span class="nam">attention_mask</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t254" href="#t254">254</a></span><span class="t">            <span class="op">)</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t255" href="#t255">255</a></span><span class="t">        <span class="key">if</span> <span class="nam">additive_attention_mask</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t256" href="#t256">256</a></span><span class="t">            <span class="nam">attn_scores</span> <span class="op">+=</span> <span class="nam">additive_attention_mask</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t257" href="#t257">257</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t258" href="#t258">258</a></span><span class="t">        <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_attn_scores</span><span class="op">(</span><span class="nam">attn_scores</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t259" href="#t259">259</a></span><span class="t">        <span class="nam">pattern</span> <span class="op">=</span> <span class="nam">F</span><span class="op">.</span><span class="nam">softmax</span><span class="op">(</span><span class="nam">attn_scores</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t260" href="#t260">260</a></span><span class="t">        <span class="nam">pattern</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">isnan</span><span class="op">(</span><span class="nam">pattern</span><span class="op">)</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">pattern</span><span class="op">)</span><span class="op">,</span> <span class="nam">pattern</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t261" href="#t261">261</a></span><span class="t">        <span class="nam">pattern</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pattern</span><span class="op">(</span><span class="nam">pattern</span><span class="op">)</span>  <span class="com"># [batch, head_index, query_pos, key_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t262" href="#t262">262</a></span><span class="t">        <span class="nam">pattern</span> <span class="op">=</span> <span class="nam">pattern</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t263" href="#t263">263</a></span><span class="t">        <span class="nam">pattern</span> <span class="op">=</span> <span class="nam">pattern</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">v</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t264" href="#t264">264</a></span><span class="t">        <span class="nam">z</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">calculate_z_scores</span><span class="op">(</span><span class="nam">v</span><span class="op">,</span> <span class="nam">pattern</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t265" href="#t265">265</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_result</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t266" href="#t266">266</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">266&#x202F;&#x219B;&#x202F;268</span><span class="annotate long">line 266 didn't jump to line 268</span></span></p>
    <p class="pln"><span class="n"><a id="t267" href="#t267">267</a></span><span class="t">                <span class="com"># call bitsandbytes method to dequantize and multiply</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t268" href="#t268">268</a></span><span class="t">                <span class="nam">out</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t269" href="#t269">269</a></span><span class="t">                    <span class="nam">bnb</span><span class="op">.</span><span class="nam">matmul_4bit</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t270" href="#t270">270</a></span><span class="t">                        <span class="nam">z</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t271" href="#t271">271</a></span><span class="t">                        <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">.</span><span class="nam">t</span><span class="op">(</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t272" href="#t272">272</a></span><span class="t">                        <span class="com"># bias=self.W_O.t(),</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t273" href="#t273">273</a></span><span class="t">                        <span class="nam">bias</span><span class="op">=</span><span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t274" href="#t274">274</a></span><span class="t">                        <span class="nam">quant_state</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">.</span><span class="nam">quant_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t275" href="#t275">275</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t276" href="#t276">276</a></span><span class="t">                    <span class="op">+</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t277" href="#t277">277</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t278" href="#t278">278</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t279" href="#t279">279</a></span><span class="t">                <span class="nam">w</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t280" href="#t280">280</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">,</span> <span class="str">"head_index d_head d_model -> d_model (head_index d_head)"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t281" href="#t281">281</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t282" href="#t282">282</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t283" href="#t283">283</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span><span class="op">.</span><span class="nam">device</span> <span class="op">!=</span> <span class="nam">w</span><span class="op">.</span><span class="nam">device</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">283&#x202F;&#x219B;&#x202F;284</span><span class="annotate long">line 283 didn't jump to line 284, because the condition on line 283 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t284" href="#t284">284</a></span><span class="t">                    <span class="nam">w</span> <span class="op">=</span> <span class="nam">w</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t285" href="#t285">285</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span><span class="op">.</span><span class="nam">device</span> <span class="op">!=</span> <span class="nam">z</span><span class="op">.</span><span class="nam">device</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">285&#x202F;&#x219B;&#x202F;286</span><span class="annotate long">line 285 didn't jump to line 286, because the condition on line 285 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t286" href="#t286">286</a></span><span class="t">                    <span class="nam">z</span> <span class="op">=</span> <span class="nam">z</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t287" href="#t287">287</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t288" href="#t288">288</a></span><span class="t">                <span class="nam">out</span> <span class="op">=</span> <span class="nam">F</span><span class="op">.</span><span class="nam">linear</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t289" href="#t289">289</a></span><span class="t">                    <span class="nam">z</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t290" href="#t290">290</a></span><span class="t">                    <span class="nam">w</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t291" href="#t291">291</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t292" href="#t292">292</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t293" href="#t293">293</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t294" href="#t294">294</a></span><span class="t">            <span class="com"># Explicitly calculate the attention result so it can be accessed by a hook</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t295" href="#t295">295</a></span><span class="t">            <span class="com"># This is off by default because it can easily eat through your GPU memory.</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t296" href="#t296">296</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">296&#x202F;&#x219B;&#x202F;297</span><span class="annotate long">line 296 didn't jump to line 297, because the condition on line 296 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t297" href="#t297">297</a></span><span class="t">                <span class="nam">result</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_result</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t298" href="#t298">298</a></span><span class="t">                    <span class="nam">bnb</span><span class="op">.</span><span class="nam">matmul_4bit</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t299" href="#t299">299</a></span><span class="t">                        <span class="nam">z</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">z</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t300" href="#t300">300</a></span><span class="t">                        <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">.</span><span class="nam">t</span><span class="op">(</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t301" href="#t301">301</a></span><span class="t">                        <span class="nam">bias</span><span class="op">=</span><span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t302" href="#t302">302</a></span><span class="t">                        <span class="nam">quant_state</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">.</span><span class="nam">quant_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t303" href="#t303">303</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t304" href="#t304">304</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t305" href="#t305">305</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t306" href="#t306">306</a></span><span class="t">                <span class="com"># Add singleton dimensions to make shapes compatible for broadcasting:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t307" href="#t307">307</a></span><span class="t">                <span class="nam">w</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t308" href="#t308">308</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t309" href="#t309">309</a></span><span class="t">                    <span class="str">"head_index d_head d_model -> 1 1 head_index d_head d_model"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t310" href="#t310">310</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t311" href="#t311">311</a></span><span class="t">                <span class="nam">z</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t312" href="#t312">312</a></span><span class="t">                    <span class="nam">z</span><span class="op">,</span> <span class="str">"batch pos head_index d_head -> batch pos head_index d_head 1"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t313" href="#t313">313</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t314" href="#t314">314</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t315" href="#t315">315</a></span><span class="t">                <span class="com"># Multiply the z tensor by the W_O tensor, summing over the d_head dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t316" href="#t316">316</a></span><span class="t">                <span class="nam">unhooked_result</span> <span class="op">=</span> <span class="op">(</span><span class="nam">z</span> <span class="op">*</span> <span class="nam">w</span><span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t317" href="#t317">317</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t318" href="#t318">318</a></span><span class="t">                <span class="nam">result</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_result</span><span class="op">(</span><span class="nam">unhooked_result</span><span class="op">)</span>  <span class="com"># [batch, pos, head_index, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t319" href="#t319">319</a></span><span class="t">            <span class="nam">out</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t320" href="#t320">320</a></span><span class="t">                <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span><span class="nam">result</span><span class="op">,</span> <span class="str">"batch position index model->batch position model"</span><span class="op">,</span> <span class="str">"sum"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t321" href="#t321">321</a></span><span class="t">                <span class="op">+</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t322" href="#t322">322</a></span><span class="t">            <span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t323" href="#t323">323</a></span><span class="t">        <span class="key">return</span> <span class="nam">out</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t324" href="#t324">324</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t325" href="#t325">325</a></span><span class="t">    <span class="key">def</span> <span class="nam">calculate_qkv_matrices</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t326" href="#t326">326</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t327" href="#t327">327</a></span><span class="t">        <span class="nam">query_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t328" href="#t328">328</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t329" href="#t329">329</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t330" href="#t330">330</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t331" href="#t331">331</a></span><span class="t">        <span class="nam">key_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t332" href="#t332">332</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t333" href="#t333">333</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t334" href="#t334">334</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t335" href="#t335">335</a></span><span class="t">        <span class="nam">value_input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t336" href="#t336">336</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t337" href="#t337">337</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t338" href="#t338">338</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t339" href="#t339">339</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t340" href="#t340">340</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t341" href="#t341">341</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t342" href="#t342">342</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch kv_pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t343" href="#t343">343</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t344" href="#t344">344</a></span><span class="t">        <span class="nam">attn_fn</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t345" href="#t345">345</a></span><span class="t">            <span class="nam">complex_attn_linear</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t346" href="#t346">346</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_split_qkv_input</span> <span class="key">or</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t347" href="#t347">347</a></span><span class="t">            <span class="key">else</span> <span class="nam">simple_attn_linear</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t348" href="#t348">348</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t349" href="#t349">349</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">349&#x202F;&#x219B;&#x202F;350</span><span class="annotate long">line 349 didn't jump to line 350, because the condition on line 349 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t350" href="#t350">350</a></span><span class="t">            <span class="nam">q</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_q</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t351" href="#t351">351</a></span><span class="t">                <span class="com"># call bitsandbytes method to dequantize and multiply</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t352" href="#t352">352</a></span><span class="t">                <span class="nam">bnb</span><span class="op">.</span><span class="nam">matmul_4bit</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t353" href="#t353">353</a></span><span class="t">                    <span class="nam">query_input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t354" href="#t354">354</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span><span class="op">.</span><span class="nam">t</span><span class="op">(</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t355" href="#t355">355</a></span><span class="t">                    <span class="nam">bias</span><span class="op">=</span><span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t356" href="#t356">356</a></span><span class="t">                    <span class="nam">quant_state</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span><span class="op">.</span><span class="nam">quant_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t357" href="#t357">357</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t358" href="#t358">358</a></span><span class="t">                    <span class="nam">query_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t359" href="#t359">359</a></span><span class="t">                    <span class="nam">query_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t360" href="#t360">360</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t361" href="#t361">361</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t362" href="#t362">362</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t363" href="#t363">363</a></span><span class="t">                <span class="op">+</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_Q</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t364" href="#t364">364</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t365" href="#t365">365</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t366" href="#t366">366</a></span><span class="t">            <span class="nam">q</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_q</span><span class="op">(</span><span class="nam">attn_fn</span><span class="op">(</span><span class="nam">query_input</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_Q</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t367" href="#t367">367</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">367&#x202F;&#x219B;&#x202F;368</span><span class="annotate long">line 367 didn't jump to line 368, because the condition on line 367 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t368" href="#t368">368</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">,</span> <span class="nam">Params4bit</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t369" href="#t369">369</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"W_K must be a Params4bit object if load_in_4bit is True"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t370" href="#t370">370</a></span><span class="t">            <span class="nam">k</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_k</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t371" href="#t371">371</a></span><span class="t">                <span class="com"># call bitsandbytes method to dequantize and multiply</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t372" href="#t372">372</a></span><span class="t">                <span class="nam">bnb</span><span class="op">.</span><span class="nam">matmul_4bit</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t373" href="#t373">373</a></span><span class="t">                    <span class="nam">key_input</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">.</span><span class="nam">t</span><span class="op">(</span><span class="op">)</span><span class="op">,</span> <span class="nam">bias</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">quant_state</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">.</span><span class="nam">quant_state</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t374" href="#t374">374</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t375" href="#t375">375</a></span><span class="t">                    <span class="nam">key_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t376" href="#t376">376</a></span><span class="t">                    <span class="nam">key_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t377" href="#t377">377</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t378" href="#t378">378</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t379" href="#t379">379</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t380" href="#t380">380</a></span><span class="t">                <span class="op">+</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_K</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t381" href="#t381">381</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t382" href="#t382">382</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t383" href="#t383">383</a></span><span class="t">            <span class="nam">k</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_k</span><span class="op">(</span><span class="nam">attn_fn</span><span class="op">(</span><span class="nam">key_input</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_K</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t384" href="#t384">384</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t385" href="#t385">385</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">385&#x202F;&#x219B;&#x202F;386</span><span class="annotate long">line 385 didn't jump to line 386, because the condition on line 385 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t386" href="#t386">386</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">,</span> <span class="nam">Params4bit</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t387" href="#t387">387</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"W_V must be a Params4bit object if load_in_4bit is True"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t388" href="#t388">388</a></span><span class="t">            <span class="nam">v</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_v</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t389" href="#t389">389</a></span><span class="t">                <span class="com"># call bitsandbytes method to dequantize and multiply</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t390" href="#t390">390</a></span><span class="t">                <span class="nam">bnb</span><span class="op">.</span><span class="nam">matmul_4bit</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t391" href="#t391">391</a></span><span class="t">                    <span class="nam">value_input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t392" href="#t392">392</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">.</span><span class="nam">t</span><span class="op">(</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t393" href="#t393">393</a></span><span class="t">                    <span class="nam">bias</span><span class="op">=</span><span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t394" href="#t394">394</a></span><span class="t">                    <span class="nam">quant_state</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">.</span><span class="nam">quant_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t395" href="#t395">395</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t396" href="#t396">396</a></span><span class="t">                    <span class="nam">value_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t397" href="#t397">397</a></span><span class="t">                    <span class="nam">value_input</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t398" href="#t398">398</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t399" href="#t399">399</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t400" href="#t400">400</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t401" href="#t401">401</a></span><span class="t">                <span class="op">+</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_V</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t402" href="#t402">402</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t403" href="#t403">403</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t404" href="#t404">404</a></span><span class="t">            <span class="nam">v</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_v</span><span class="op">(</span><span class="nam">attn_fn</span><span class="op">(</span><span class="nam">value_input</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">b_V</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t405" href="#t405">405</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t406" href="#t406">406</a></span><span class="t">        <span class="key">return</span> <span class="nam">q</span><span class="op">,</span> <span class="nam">k</span><span class="op">,</span> <span class="nam">v</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t407" href="#t407">407</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t408" href="#t408">408</a></span><span class="t">    <span class="key">def</span> <span class="nam">calculate_attention_scores</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t409" href="#t409">409</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t410" href="#t410">410</a></span><span class="t">        <span class="nam">q</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch query_pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t411" href="#t411">411</a></span><span class="t">        <span class="nam">k</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch key_pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t412" href="#t412">412</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch head_index query_pos key_pos"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t413" href="#t413">413</a></span><span class="t">        <span class="nam">q_</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t414" href="#t414">414</a></span><span class="t">            <span class="nam">q</span><span class="op">,</span> <span class="str">"batch query_pos head_index d_head -> batch head_index query_pos d_head"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t415" href="#t415">415</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t416" href="#t416">416</a></span><span class="t">        <span class="nam">k_</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t417" href="#t417">417</a></span><span class="t">            <span class="nam">k</span><span class="op">,</span> <span class="str">"batch key_pos head_index d_head -> batch head_index d_head key_pos"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t418" href="#t418">418</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t419" href="#t419">419</a></span><span class="t">        <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">q_</span> <span class="op">@</span> <span class="nam">k_</span> <span class="op">/</span> <span class="nam">self</span><span class="op">.</span><span class="nam">attn_scale</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t420" href="#t420">420</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_scores_soft_cap</span> <span class="op">></span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">420&#x202F;&#x219B;&#x202F;421</span><span class="annotate long">line 420 didn't jump to line 421, because the condition on line 420 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t421" href="#t421">421</a></span><span class="t">            <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_scores_soft_cap</span> <span class="op">*</span> <span class="nam">F</span><span class="op">.</span><span class="nam">tanh</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t422" href="#t422">422</a></span><span class="t">                <span class="nam">attn_scores</span> <span class="op">/</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_scores_soft_cap</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t423" href="#t423">423</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t424" href="#t424">424</a></span><span class="t">        <span class="key">return</span> <span class="nam">attn_scores</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t425" href="#t425">425</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t426" href="#t426">426</a></span><span class="t">    <span class="key">def</span> <span class="nam">calculate_z_scores</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t427" href="#t427">427</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t428" href="#t428">428</a></span><span class="t">        <span class="nam">v</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch key_pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t429" href="#t429">429</a></span><span class="t">        <span class="nam">pattern</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch head_index query_pos key_pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t430" href="#t430">430</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch query_pos head_index d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t431" href="#t431">431</a></span><span class="t">        <span class="nam">v_</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t432" href="#t432">432</a></span><span class="t">            <span class="nam">v</span><span class="op">,</span> <span class="str">"batch key_pos head_index d_head -> batch head_index key_pos d_head"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t433" href="#t433">433</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t434" href="#t434">434</a></span><span class="t">        <span class="nam">pattern_</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t435" href="#t435">435</a></span><span class="t">            <span class="nam">pattern</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t436" href="#t436">436</a></span><span class="t">            <span class="str">"batch head_index query_pos key_pos -> batch head_index query_pos key_pos"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t437" href="#t437">437</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t438" href="#t438">438</a></span><span class="t">        <span class="nam">z</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_z</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t439" href="#t439">439</a></span><span class="t">            <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t440" href="#t440">440</a></span><span class="t">                <span class="nam">pattern_</span> <span class="op">@</span> <span class="nam">v_</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t441" href="#t441">441</a></span><span class="t">                <span class="str">"batch head_index query_pos d_head -> batch query_pos head_index d_head"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t442" href="#t442">442</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t443" href="#t443">443</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t444" href="#t444">444</a></span><span class="t">        <span class="key">return</span> <span class="nam">z</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t445" href="#t445">445</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t446" href="#t446">446</a></span><span class="t">    <span class="key">def</span> <span class="nam">apply_causal_mask</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t447" href="#t447">447</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t448" href="#t448">448</a></span><span class="t">        <span class="nam">attn_scores</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch head_index pos pos_plus_past_kv_pos_offset"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t449" href="#t449">449</a></span><span class="t">        <span class="nam">past_kv_pos_offset</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="num">0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t450" href="#t450">450</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch offset_pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t451" href="#t451">451</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t452" href="#t452">452</a></span><span class="t">        <span class="com"># The query context length is the number of positions we take queries from - if not using a past_kv_cache this is just the context length (for the current prompt), but if we're caching it can be different.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t453" href="#t453">453</a></span><span class="t">        <span class="nam">query_ctx_length</span> <span class="op">=</span> <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t454" href="#t454">454</a></span><span class="t">        <span class="com"># The key context length is the number of positions in the past - this includes all positions in the cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t455" href="#t455">455</a></span><span class="t">        <span class="com"># If not caching, query_ctx_length == key_ctx_length</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t456" href="#t456">456</a></span><span class="t">        <span class="nam">key_ctx_length</span> <span class="op">=</span> <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t457" href="#t457">457</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t458" href="#t458">458</a></span><span class="t">        <span class="key">if</span> <span class="nam">query_ctx_length</span> <span class="op">+</span> <span class="nam">past_kv_pos_offset</span> <span class="op">!=</span> <span class="nam">key_ctx_length</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">458&#x202F;&#x219B;&#x202F;459</span><span class="annotate long">line 458 didn't jump to line 459, because the condition on line 458 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t459" href="#t459">459</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t460" href="#t460">460</a></span><span class="t">                <span class="str">f"query_ctx_length {query_ctx_length} + past_kv_pos_offset {past_kv_pos_offset} != key_ctx_length {key_ctx_length} - you likely have a bug."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t461" href="#t461">461</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t462" href="#t462">462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t463" href="#t463">463</a></span><span class="t">        <span class="com"># Index back to front to ensure local attention works</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t464" href="#t464">464</a></span><span class="t">        <span class="nam">final_mask</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">mask</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">-</span><span class="nam">query_ctx_length</span><span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="nam">key_ctx_length</span><span class="op">:</span><span class="op">]</span>  <span class="com"># [1, 1, pos, pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t465" href="#t465">465</a></span><span class="t">        <span class="key">if</span> <span class="nam">attention_mask</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t466" href="#t466">466</a></span><span class="t">            <span class="com"># Apply a causal mask to the attention scores considering the padding</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t467" href="#t467">467</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t468" href="#t468">468</a></span><span class="t">            <span class="com"># Add singleton dimensions to the attention mask to match the shape of the final mask</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t469" href="#t469">469</a></span><span class="t">            <span class="nam">attention_mask</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t470" href="#t470">470</a></span><span class="t">                <span class="nam">attention_mask</span><span class="op">,</span> <span class="str">"batch offset_pos -> batch 1 1 offset_pos"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t471" href="#t471">471</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t472" href="#t472">472</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t473" href="#t473">473</a></span><span class="t">            <span class="nam">final_mask</span> <span class="op">=</span> <span class="nam">final_mask</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">attention_mask</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t474" href="#t474">474</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t475" href="#t475">475</a></span><span class="t">            <span class="com"># Element-wise multiplication of the final mask and the attention mask and cast to boolean</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t476" href="#t476">476</a></span><span class="t">            <span class="nam">final_mask</span> <span class="op">=</span> <span class="op">(</span><span class="nam">final_mask</span> <span class="op">*</span> <span class="nam">attention_mask</span><span class="op">)</span><span class="op">.</span><span class="nam">bool</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, head, pos, offset_pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t477" href="#t477">477</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t478" href="#t478">478</a></span><span class="t">        <span class="nam">attn_scores</span> <span class="op">=</span> <span class="nam">attn_scores</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">final_mask</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t479" href="#t479">479</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">final_mask</span><span class="op">,</span> <span class="nam">attn_scores</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">IGNORE</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t480" href="#t480">480</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t481" href="#t481">481</a></span><span class="t">    <span class="key">def</span> <span class="nam">calculate_sin_cos_rotary</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t482" href="#t482">482</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t483" href="#t483">483</a></span><span class="t">        <span class="nam">rotary_dim</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t484" href="#t484">484</a></span><span class="t">        <span class="nam">n_ctx</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t485" href="#t485">485</a></span><span class="t">        <span class="nam">base</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="num">10000</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t486" href="#t486">486</a></span><span class="t">        <span class="nam">dtype</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t487" href="#t487">487</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_ctx rotary_dim"</span><span class="op">]</span><span class="op">,</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_ctx rotary_dim"</span><span class="op">]</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t488" href="#t488">488</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t489" href="#t489">489</a></span><span class="t"><span class="str">        Calculate the sine and cosine waves to use in a rotary embedding. See https://blog.eleuther.ai/rotary-embeddings/ for details</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t490" href="#t490">490</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t491" href="#t491">491</a></span><span class="t"><span class="str">        Note: For some inexplicable reason, in GPT-J each ADJACENT pair of elements in k and q are rotated, in GPT-NeoX the pair of elements at k and k+n//2 are rotated (ie folding the full length in half, and then looking at pairs accordingly). I have absolutely no clue why, it should be completely equivalent.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t492" href="#t492">492</a></span><span class="t"><span class="str">        To resolve this, I've coded it to default to the GPT-J mode, but to explicitly check whether it's GPT-NeoX and then do the GPT-NeoX thing if it is.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t493" href="#t493">493</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t494" href="#t494">494</a></span><span class="t">        <span class="nam">high_precision</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span> <span class="key">if</span> <span class="nam">dtype</span> <span class="op">!=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float64</span> <span class="key">else</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float64</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t495" href="#t495">495</a></span><span class="t">        <span class="nam">pos</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">n_ctx</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">high_precision</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t496" href="#t496">496</a></span><span class="t">        <span class="nam">dim</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">rotary_dim</span> <span class="op">//</span> <span class="num">2</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">high_precision</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t497" href="#t497">497</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t498" href="#t498">498</a></span><span class="t">        <span class="com"># Llama-3.1 uses NTK-by-Parts Rotary Embedding introduced in Section 3.2 in https://arxiv.org/pdf/2309.00071</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t499" href="#t499">499</a></span><span class="t">        <span class="com"># Implementation copied from https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/modeling_rope_utils.py#L310</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t500" href="#t500">500</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_NTK_by_parts_rope</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">500&#x202F;&#x219B;&#x202F;501</span><span class="annotate long">line 500 didn't jump to line 501, because the condition on line 500 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t501" href="#t501">501</a></span><span class="t">            <span class="nam">inv_freq</span> <span class="op">=</span> <span class="num">1.0</span> <span class="op">/</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t502" href="#t502">502</a></span><span class="t">                <span class="nam">base</span> <span class="op">**</span> <span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">rotary_dim</span><span class="op">,</span> <span class="num">2</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">int64</span><span class="op">)</span><span class="op">.</span><span class="nam">float</span><span class="op">(</span><span class="op">)</span> <span class="op">/</span> <span class="nam">rotary_dim</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t503" href="#t503">503</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t504" href="#t504">504</a></span><span class="t">            <span class="nam">factor</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">NTK_by_parts_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t505" href="#t505">505</a></span><span class="t">            <span class="nam">low_freq_factor</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">NTK_by_parts_low_freq_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t506" href="#t506">506</a></span><span class="t">            <span class="nam">high_freq_factor</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">NTK_by_parts_high_freq_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t507" href="#t507">507</a></span><span class="t">            <span class="nam">old_context_len</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">NTK_original_ctx_len</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t508" href="#t508">508</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t509" href="#t509">509</a></span><span class="t">            <span class="nam">low_freq_wavelen</span> <span class="op">=</span> <span class="nam">old_context_len</span> <span class="op">/</span> <span class="nam">low_freq_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t510" href="#t510">510</a></span><span class="t">            <span class="nam">high_freq_wavelen</span> <span class="op">=</span> <span class="nam">old_context_len</span> <span class="op">/</span> <span class="nam">high_freq_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t511" href="#t511">511</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t512" href="#t512">512</a></span><span class="t">            <span class="nam">wavelen</span> <span class="op">=</span> <span class="num">2</span> <span class="op">*</span> <span class="nam">math</span><span class="op">.</span><span class="nam">pi</span> <span class="op">/</span> <span class="nam">inv_freq</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t513" href="#t513">513</a></span><span class="t">            <span class="nam">inv_freq_llama</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">wavelen</span> <span class="op">></span> <span class="nam">low_freq_wavelen</span><span class="op">,</span> <span class="nam">inv_freq</span> <span class="op">/</span> <span class="nam">factor</span><span class="op">,</span> <span class="nam">inv_freq</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t514" href="#t514">514</a></span><span class="t">            <span class="nam">smooth_factor</span> <span class="op">=</span> <span class="op">(</span><span class="nam">old_context_len</span> <span class="op">/</span> <span class="nam">wavelen</span> <span class="op">-</span> <span class="nam">low_freq_factor</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t515" href="#t515">515</a></span><span class="t">                <span class="nam">high_freq_factor</span> <span class="op">-</span> <span class="nam">low_freq_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t516" href="#t516">516</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t517" href="#t517">517</a></span><span class="t">            <span class="nam">smoothed_inv_freq</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t518" href="#t518">518</a></span><span class="t">                <span class="num">1</span> <span class="op">-</span> <span class="nam">smooth_factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t519" href="#t519">519</a></span><span class="t">            <span class="op">)</span> <span class="op">*</span> <span class="nam">inv_freq_llama</span> <span class="op">/</span> <span class="nam">factor</span> <span class="op">+</span> <span class="nam">smooth_factor</span> <span class="op">*</span> <span class="nam">inv_freq_llama</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t520" href="#t520">520</a></span><span class="t">            <span class="nam">is_medium_freq</span> <span class="op">=</span> <span class="op">~</span><span class="op">(</span><span class="nam">wavelen</span> <span class="op">&lt;</span> <span class="nam">high_freq_wavelen</span><span class="op">)</span> <span class="op">*</span> <span class="op">~</span><span class="op">(</span><span class="nam">wavelen</span> <span class="op">></span> <span class="nam">low_freq_wavelen</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t521" href="#t521">521</a></span><span class="t">            <span class="nam">inv_freq_llama</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">is_medium_freq</span><span class="op">,</span> <span class="nam">smoothed_inv_freq</span><span class="op">,</span> <span class="nam">inv_freq_llama</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t522" href="#t522">522</a></span><span class="t">            <span class="nam">freq</span> <span class="op">=</span> <span class="num">1</span> <span class="op">/</span> <span class="nam">inv_freq_llama</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t523" href="#t523">523</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t524" href="#t524">524</a></span><span class="t">            <span class="nam">freq</span> <span class="op">=</span> <span class="nam">base</span> <span class="op">**</span> <span class="op">(</span><span class="nam">dim</span> <span class="op">/</span> <span class="op">(</span><span class="nam">rotary_dim</span> <span class="op">/</span> <span class="num">2</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t525" href="#t525">525</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_adjacent_pairs</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">525&#x202F;&#x219B;&#x202F;526</span><span class="annotate long">line 525 didn't jump to line 526, because the condition on line 525 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t526" href="#t526">526</a></span><span class="t">            <span class="nam">freq</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">repeat</span><span class="op">(</span><span class="nam">freq</span><span class="op">,</span> <span class="str">"d -> (d 2)"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t527" href="#t527">527</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t528" href="#t528">528</a></span><span class="t">            <span class="nam">freq</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">repeat</span><span class="op">(</span><span class="nam">freq</span><span class="op">,</span> <span class="str">"d -> (2 d)"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t529" href="#t529">529</a></span><span class="t">        <span class="com"># Create a n_ctx x rotary_dim tensor, where each column is an arithmetic sequence of angles in that frequency</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t530" href="#t530">530</a></span><span class="t">        <span class="nam">angles</span> <span class="op">=</span> <span class="nam">pos</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span> <span class="op">/</span> <span class="nam">freq</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t531" href="#t531">531</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">sin</span><span class="op">(</span><span class="nam">angles</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">)</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cos</span><span class="op">(</span><span class="nam">angles</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t532" href="#t532">532</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t533" href="#t533">533</a></span><span class="t">    <span class="key">def</span> <span class="nam">rotate_every_two</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t534" href="#t534">534</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="nam">x</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"... rotary_dim"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t535" href="#t535">535</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"... rotary_dim"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t536" href="#t536">536</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t537" href="#t537">537</a></span><span class="t"><span class="str">        Rotary helper function, splits x into blocks of size 2 along the final axis and maps [x0, x1] to [-x1, x0]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t538" href="#t538">538</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t539" href="#t539">539</a></span><span class="t"><span class="str">        The final axis of x must have even length.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t540" href="#t540">540</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t541" href="#t541">541</a></span><span class="t"><span class="str">        GPT-NeoX and GPT-J do rotary subtly differently, see calculate_sin_cos_rotary for details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t542" href="#t542">542</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t543" href="#t543">543</a></span><span class="t">        <span class="nam">rot_x</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">clone</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t544" href="#t544">544</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_adjacent_pairs</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">544&#x202F;&#x219B;&#x202F;545</span><span class="annotate long">line 544 didn't jump to line 545, because the condition on line 544 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t545" href="#t545">545</a></span><span class="t">            <span class="nam">rot_x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="op">:</span><span class="op">:</span><span class="num">2</span><span class="op">]</span> <span class="op">=</span> <span class="op">-</span><span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="num">1</span><span class="op">:</span><span class="op">:</span><span class="num">2</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t546" href="#t546">546</a></span><span class="t">            <span class="nam">rot_x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="num">1</span><span class="op">:</span><span class="op">:</span><span class="num">2</span><span class="op">]</span> <span class="op">=</span> <span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="op">:</span><span class="op">:</span><span class="num">2</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t547" href="#t547">547</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t548" href="#t548">548</a></span><span class="t">            <span class="nam">n</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span> <span class="op">//</span> <span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t549" href="#t549">549</a></span><span class="t">            <span class="nam">rot_x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="op">:</span><span class="nam">n</span><span class="op">]</span> <span class="op">=</span> <span class="op">-</span><span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="nam">n</span><span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t550" href="#t550">550</a></span><span class="t">            <span class="nam">rot_x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="nam">n</span><span class="op">:</span><span class="op">]</span> <span class="op">=</span> <span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="op">:</span><span class="nam">n</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t551" href="#t551">551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t552" href="#t552">552</a></span><span class="t">        <span class="key">return</span> <span class="nam">rot_x</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t553" href="#t553">553</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t554" href="#t554">554</a></span><span class="t">    <span class="key">def</span> <span class="nam">apply_rotary</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t555" href="#t555">555</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t556" href="#t556">556</a></span><span class="t">        <span class="nam">x</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos head_index d_head"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t557" href="#t557">557</a></span><span class="t">        <span class="nam">past_kv_pos_offset</span><span class="op">=</span><span class="num">0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t558" href="#t558">558</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch offset_pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t559" href="#t559">559</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos head_index d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t560" href="#t560">560</a></span><span class="t">        <span class="com"># Only apply rotary to first rotary_dim dimensions (eg, if rotary_dim=64 and d_head=256, only apply to first 1/4 of dimensions)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t561" href="#t561">561</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t562" href="#t562">562</a></span><span class="t">        <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">device</span> <span class="op">!=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotary_sin</span><span class="op">.</span><span class="nam">device</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">562&#x202F;&#x219B;&#x202F;563</span><span class="annotate long">line 562 didn't jump to line 563, because the condition on line 562 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t563" href="#t563">563</a></span><span class="t">            <span class="nam">x</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">rotary_sin</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t564" href="#t564">564</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t565" href="#t565">565</a></span><span class="t">        <span class="nam">x_pos</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t566" href="#t566">566</a></span><span class="t">        <span class="nam">x_rot</span> <span class="op">=</span> <span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="op">:</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_dim</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t567" href="#t567">567</a></span><span class="t">        <span class="nam">x_pass</span> <span class="op">=</span> <span class="nam">x</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">rotary_dim</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t568" href="#t568">568</a></span><span class="t">        <span class="nam">x_flip</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotate_every_two</span><span class="op">(</span><span class="nam">x_rot</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t569" href="#t569">569</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t570" href="#t570">570</a></span><span class="t">        <span class="key">if</span> <span class="nam">attention_mask</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t571" href="#t571">571</a></span><span class="t">            <span class="nam">rotary_cos</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotary_cos</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t572" href="#t572">572</a></span><span class="t">                <span class="key">None</span><span class="op">,</span> <span class="nam">past_kv_pos_offset</span> <span class="op">:</span> <span class="nam">past_kv_pos_offset</span> <span class="op">+</span> <span class="nam">x_pos</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t573" href="#t573">573</a></span><span class="t">            <span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t574" href="#t574">574</a></span><span class="t">            <span class="nam">rotary_sin</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotary_sin</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t575" href="#t575">575</a></span><span class="t">                <span class="key">None</span><span class="op">,</span> <span class="nam">past_kv_pos_offset</span> <span class="op">:</span> <span class="nam">past_kv_pos_offset</span> <span class="op">+</span> <span class="nam">x_pos</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t576" href="#t576">576</a></span><span class="t">            <span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t577" href="#t577">577</a></span><span class="t">            <span class="nam">x_rotated</span> <span class="op">=</span> <span class="nam">x_rot</span> <span class="op">*</span> <span class="nam">rotary_cos</span> <span class="op">+</span> <span class="nam">x_flip</span> <span class="op">*</span> <span class="nam">rotary_sin</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t578" href="#t578">578</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t579" href="#t579">579</a></span><span class="t">            <span class="nam">offset_position_ids</span> <span class="op">=</span> <span class="nam">get_offset_position_ids</span><span class="op">(</span><span class="nam">past_kv_pos_offset</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t580" href="#t580">580</a></span><span class="t">            <span class="nam">offset_position_ids</span> <span class="op">=</span> <span class="nam">offset_position_ids</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">rotary_cos</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t581" href="#t581">581</a></span><span class="t">            <span class="nam">mask_rotary_cos</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotary_cos</span><span class="op">[</span><span class="nam">offset_position_ids</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t582" href="#t582">582</a></span><span class="t">            <span class="nam">mask_rotary_sin</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">rotary_sin</span><span class="op">[</span><span class="nam">offset_position_ids</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t583" href="#t583">583</a></span><span class="t">            <span class="nam">x_rotated</span> <span class="op">=</span> <span class="nam">x_rot</span> <span class="op">*</span> <span class="nam">mask_rotary_cos</span> <span class="op">+</span> <span class="nam">x_flip</span> <span class="op">*</span> <span class="nam">mask_rotary_sin</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t584" href="#t584">584</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t585" href="#t585">585</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="op">[</span><span class="nam">x_rotated</span><span class="op">,</span> <span class="nam">x_pass</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t586" href="#t586">586</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t587" href="#t587">587</a></span><span class="t">    <span class="op">@</span><span class="nam">staticmethod</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t588" href="#t588">588</a></span><span class="t">    <span class="key">def</span> <span class="nam">create_alibi_slope</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t589" href="#t589">589</a></span><span class="t">        <span class="nam">n_ctx</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span> <span class="nam">device</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t590" href="#t590">590</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"query key"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t591" href="#t591">591</a></span><span class="t">        <span class="str">"""Create an ALiBi Slope Matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t592" href="#t592">592</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t593" href="#t593">593</a></span><span class="t"><span class="str">        Create the slope matrix used in ALiBi, before it is multiplied by the head-specific scalar.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t594" href="#t594">594</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t595" href="#t595">595</a></span><span class="t"><span class="str">        See :meth:`create_alibi_bias` for the full ALiBi bias calculation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t596" href="#t596">596</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t597" href="#t597">597</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t598" href="#t598">598</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t599" href="#t599">599</a></span><span class="t"><span class="str">        >>> AbstractAttention.create_alibi_slope(3)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t600" href="#t600">600</a></span><span class="t"><span class="str">        tensor([[ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t601" href="#t601">601</a></span><span class="t"><span class="str">                [-1.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t602" href="#t602">602</a></span><span class="t"><span class="str">                [-2., -1.,  0.]])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t603" href="#t603">603</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t604" href="#t604">604</a></span><span class="t"><span class="str">        >>> AbstractAttention.create_alibi_slope(4)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t605" href="#t605">605</a></span><span class="t"><span class="str">        tensor([[ 0.,  0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t606" href="#t606">606</a></span><span class="t"><span class="str">                [-1.,  0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t607" href="#t607">607</a></span><span class="t"><span class="str">                [-2., -1.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t608" href="#t608">608</a></span><span class="t"><span class="str">                [-3., -2., -1.,  0.]])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t609" href="#t609">609</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t610" href="#t610">610</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t611" href="#t611">611</a></span><span class="t"><span class="str">            n_ctx: The maximum number of tokens in a prompt.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t612" href="#t612">612</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t613" href="#t613">613</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t614" href="#t614">614</a></span><span class="t"><span class="str">            A tensor of shape (n_ctx, n_ctx), where the upper triangle is zero and the lower</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t615" href="#t615">615</a></span><span class="t"><span class="str">            triangle is decreasing by a constant slope of 1 (towards the bottom left corner).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t616" href="#t616">616</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t617" href="#t617">617</a></span><span class="t">        <span class="com"># set rows as [[0,1,2...]]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t618" href="#t618">618</a></span><span class="t">        <span class="nam">rows</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">n_ctx</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">device</span><span class="op">)</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">(</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t619" href="#t619">619</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t620" href="#t620">620</a></span><span class="t">        <span class="com"># Set cols as [[0],[1],[2]...]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t621" href="#t621">621</a></span><span class="t">        <span class="nam">cols</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">n_ctx</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">device</span><span class="op">)</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">(</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t622" href="#t622">622</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t623" href="#t623">623</a></span><span class="t">        <span class="com"># Use broadcasting to create the desired lower triangular part of the matrix</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t624" href="#t624">624</a></span><span class="t">        <span class="nam">slope_matrix</span> <span class="op">=</span> <span class="nam">rows</span> <span class="op">-</span> <span class="nam">cols</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t625" href="#t625">625</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t626" href="#t626">626</a></span><span class="t">        <span class="com"># Use the clamp method to set all positive values (upper right triangle) to</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t627" href="#t627">627</a></span><span class="t">        <span class="key">return</span> <span class="nam">slope_matrix</span><span class="op">.</span><span class="nam">clamp</span><span class="op">(</span><span class="nam">max</span><span class="op">=</span><span class="num">0</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t628" href="#t628">628</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t629" href="#t629">629</a></span><span class="t">    <span class="op">@</span><span class="nam">staticmethod</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t630" href="#t630">630</a></span><span class="t">    <span class="key">def</span> <span class="nam">create_alibi_multipliers</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t631" href="#t631">631</a></span><span class="t">        <span class="nam">n_heads</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span> <span class="nam">device</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t632" href="#t632">632</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"head_idx"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t633" href="#t633">633</a></span><span class="t">        <span class="str">"""Create the ALiBi Scalar Multipliers for each Head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t634" href="#t634">634</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t635" href="#t635">635</a></span><span class="t"><span class="str">        For n heads, the set of multipliers (m) is the geometric sequence that starts at 2^(-8/n), and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t636" href="#t636">636</a></span><span class="t"><span class="str">        uses that same value as its ratio. For example, with 8 heads the values would be [1/(2^1),</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t637" href="#t637">637</a></span><span class="t"><span class="str">        1/(2^2), ... , 1/(2^8)]. With 16 heads the values would be [1/(2^0.5), 1/(2^1), ... , 1/(2^8)].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t638" href="#t638">638</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t639" href="#t639">639</a></span><span class="t"><span class="str">        See :meth:`create_alibi_bias` for the full ALiBi bias calculation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t640" href="#t640">640</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t641" href="#t641">641</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t642" href="#t642">642</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t643" href="#t643">643</a></span><span class="t"><span class="str">        >>> AbstractAttention.create_alibi_multipliers(8)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t644" href="#t644">644</a></span><span class="t"><span class="str">        tensor([0.5000, 0.2500, 0.1250, 0.0625, 0.0312, 0.0156, 0.0078, 0.0039])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t645" href="#t645">645</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t646" href="#t646">646</a></span><span class="t"><span class="str">        >>> AbstractAttention.create_alibi_multipliers(16)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t647" href="#t647">647</a></span><span class="t"><span class="str">        tensor([0.7071, 0.5000, 0.3536, 0.2500, 0.1768, 0.1250, 0.0884, 0.0625, 0.0442, 0.0312,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t648" href="#t648">648</a></span><span class="t"><span class="str">                0.0221, 0.0156, 0.0110, 0.0078, 0.0055, 0.0039])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t649" href="#t649">649</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t650" href="#t650">650</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t651" href="#t651">651</a></span><span class="t"><span class="str">            n_heads: The number of heads in a layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t652" href="#t652">652</a></span><span class="t"><span class="str">            device: The device to create the tensor on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t653" href="#t653">653</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t654" href="#t654">654</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t655" href="#t655">655</a></span><span class="t"><span class="str">            A tensor of shape (n_heads,) containing the scalar multiplier for each head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t656" href="#t656">656</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t657" href="#t657">657</a></span><span class="t">        <span class="com"># Calculate the starting value</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t658" href="#t658">658</a></span><span class="t">        <span class="nam">start</span> <span class="op">=</span> <span class="num">2</span> <span class="op">**</span> <span class="op">(</span><span class="op">-</span><span class="num">8</span> <span class="op">/</span> <span class="nam">n_heads</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t659" href="#t659">659</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t660" href="#t660">660</a></span><span class="t">        <span class="com"># Generate the indices [0, 1, ..., n_heads-1]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t661" href="#t661">661</a></span><span class="t">        <span class="nam">indices</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">n_heads</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t662" href="#t662">662</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t663" href="#t663">663</a></span><span class="t">        <span class="com"># Compute the multipliers, with the starting value being the same as the ratio</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t664" href="#t664">664</a></span><span class="t">        <span class="nam">multipliers</span> <span class="op">=</span> <span class="nam">start</span> <span class="op">*</span> <span class="op">(</span><span class="nam">start</span><span class="op">**</span><span class="nam">indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t665" href="#t665">665</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t666" href="#t666">666</a></span><span class="t">        <span class="key">return</span> <span class="nam">multipliers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t667" href="#t667">667</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t668" href="#t668">668</a></span><span class="t">    <span class="op">@</span><span class="nam">staticmethod</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t669" href="#t669">669</a></span><span class="t">    <span class="key">def</span> <span class="nam">create_alibi_bias</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t670" href="#t670">670</a></span><span class="t">        <span class="nam">n_heads</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span> <span class="nam">n_ctx</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span> <span class="nam">device</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span> <span class="nam">str</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t671" href="#t671">671</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"head_idx query key"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t672" href="#t672">672</a></span><span class="t">        <span class="str">"""Create the ALiBi Bias for all Heads.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t673" href="#t673">673</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t674" href="#t674">674</a></span><span class="t"><span class="str">        Calculate the ALiBi bias (https://arxiv.org/pdf/2108.12409.pdf) for all heads in a layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t675" href="#t675">675</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t676" href="#t676">676</a></span><span class="t"><span class="str">        The broad idea behind ALiBi is to remove the positional encoding from the original transformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t677" href="#t677">677</a></span><span class="t"><span class="str">        model, and instead apply a bias to each attention score. This bias is proportional to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t678" href="#t678">678</a></span><span class="t"><span class="str">        distance between the query and key (i.e. it encourage paying less attention to more distant</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t679" href="#t679">679</a></span><span class="t"><span class="str">        tokens), and is added to the attention scores before the softmax. It is used in models such as</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t680" href="#t680">680</a></span><span class="t"><span class="str">        Bloom.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t681" href="#t681">681</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t682" href="#t682">682</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t683" href="#t683">683</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t684" href="#t684">684</a></span><span class="t"><span class="str">        >>> AbstractAttention.create_alibi_bias(2, 4, torch.device('cpu'))</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t685" href="#t685">685</a></span><span class="t"><span class="str">        tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t686" href="#t686">686</a></span><span class="t"><span class="str">            [-0.0625,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t687" href="#t687">687</a></span><span class="t"><span class="str">            [-0.1250, -0.0625,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t688" href="#t688">688</a></span><span class="t"><span class="str">            [-0.1875, -0.1250, -0.0625,  0.0000]],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t689" href="#t689">689</a></span><span class="t"><span class="str">            [[ 0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t690" href="#t690">690</a></span><span class="t"><span class="str">            [-0.0039,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t691" href="#t691">691</a></span><span class="t"><span class="str">            [-0.0078, -0.0039,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t692" href="#t692">692</a></span><span class="t"><span class="str">            [-0.0117, -0.0078, -0.0039,  0.0000]]])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t693" href="#t693">693</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t694" href="#t694">694</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t695" href="#t695">695</a></span><span class="t"><span class="str">            n_heads: The number of heads in a layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t696" href="#t696">696</a></span><span class="t"><span class="str">            n_ctx: The maximum number of tokens in a prompt.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t697" href="#t697">697</a></span><span class="t"><span class="str">            device: The device to create the tensor on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t698" href="#t698">698</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t699" href="#t699">699</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t700" href="#t700">700</a></span><span class="t"><span class="str">            The ALiBi bias that should be added to the attention scores before the softmax.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t701" href="#t701">701</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t702" href="#t702">702</a></span><span class="t">        <span class="com"># Create the slope matrix</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t703" href="#t703">703</a></span><span class="t">        <span class="nam">slope</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"query key"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">AbstractAttention</span><span class="op">.</span><span class="nam">create_alibi_slope</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t704" href="#t704">704</a></span><span class="t">            <span class="nam">n_ctx</span><span class="op">,</span> <span class="nam">device</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t705" href="#t705">705</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t706" href="#t706">706</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t707" href="#t707">707</a></span><span class="t">        <span class="com"># Create the scalar multiplier for each head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t708" href="#t708">708</a></span><span class="t">        <span class="nam">multipliers</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"head_idx"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">AbstractAttention</span><span class="op">.</span><span class="nam">create_alibi_multipliers</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t709" href="#t709">709</a></span><span class="t">            <span class="nam">n_heads</span><span class="op">,</span> <span class="nam">device</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t710" href="#t710">710</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t711" href="#t711">711</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t712" href="#t712">712</a></span><span class="t">        <span class="com"># Add singleton dimensions to make shapes compatible for broadcasting:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t713" href="#t713">713</a></span><span class="t">        <span class="nam">slope</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span><span class="nam">slope</span><span class="op">,</span> <span class="str">"query key -> 1 query key"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t714" href="#t714">714</a></span><span class="t">        <span class="nam">multipliers</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span><span class="nam">multipliers</span><span class="op">,</span> <span class="str">"head_idx -> head_idx 1 1"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t715" href="#t715">715</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t716" href="#t716">716</a></span><span class="t">        <span class="com"># Element-wise multiplication of the slope and multipliers</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t717" href="#t717">717</a></span><span class="t">        <span class="nam">alibi_bias</span> <span class="op">=</span> <span class="nam">multipliers</span> <span class="op">*</span> <span class="nam">slope</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t718" href="#t718">718</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t719" href="#t719">719</a></span><span class="t">        <span class="key">return</span> <span class="nam">alibi_bias</span>&nbsp;</span><span class="r"></span></p>
</main>
<footer>
    <div class="content">
        <p>
            <a id="prevFileLink" class="nav" href="d_db46118ef83ad831___init___py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_db46118ef83ad831_attention_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2025-05-15 21:26 +0000
        </p>
    </div>
</footer>
</body>
</html>
