<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Coverage for transformer_lens/HookedTransformer.py: 77%</title>
    <link rel="icon" sizes="32x32" href="favicon_32.png">
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="coverage_html.js" defer></script>
</head>
<body class="pyfile">
<header>
    <div class="content">
        <h1>
            <span class="text">Coverage for </span><b>transformer_lens/HookedTransformer.py</b>:
            <span class="pc_cov">77%</span>
        </h1>
        <aside id="help_panel_wrapper">
            <input id="help_panel_state" type="checkbox">
            <label for="help_panel_state">
                <img id="keyboard_icon" src="keybd_closed.png" alt="Show/hide keyboard shortcuts" />
            </label>
            <div id="help_panel">
                <p class="legend">Shortcuts on this page</p>
                <div class="keyhelp">
                    <p>
                        <kbd>r</kbd>
                        <kbd>m</kbd>
                        <kbd>x</kbd>
                        <kbd>p</kbd>
                        &nbsp; toggle line displays
                    </p>
                    <p>
                        <kbd>j</kbd>
                        <kbd>k</kbd>
                        &nbsp; next/prev highlighted chunk
                    </p>
                    <p>
                        <kbd>0</kbd> &nbsp; (zero) top of page
                    </p>
                    <p>
                        <kbd>1</kbd> &nbsp; (one) first highlighted chunk
                    </p>
                    <p>
                        <kbd>[</kbd>
                        <kbd>]</kbd>
                        &nbsp; prev/next file
                    </p>
                    <p>
                        <kbd>u</kbd> &nbsp; up to the index
                    </p>
                    <p>
                        <kbd>?</kbd> &nbsp; show/hide this help
                    </p>
                </div>
            </div>
        </aside>
        <h2>
            <span class="text">741 statements &nbsp;</span>
            <button type="button" class="run button_toggle_run" value="run" data-shortcut="r" title="Toggle lines run">609<span class="text"> run</span></button>
            <button type="button" class="mis show_mis button_toggle_mis" value="mis" data-shortcut="m" title="Toggle lines missing">132<span class="text"> missing</span></button>
            <button type="button" class="exc show_exc button_toggle_exc" value="exc" data-shortcut="x" title="Toggle lines excluded">0<span class="text"> excluded</span></button>
            <button type="button" class="par run show_par button_toggle_par" value="par" data-shortcut="p" title="Toggle lines partially run">79<span class="text"> partial</span></button>
        </h2>
        <p class="text">
            <a id="prevFileLink" class="nav" href="d_af97b5493da09a14_HookedEncoderDecoder_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_af97b5493da09a14_HookedTransformerConfig_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2024-12-31 02:13 +0000
        </p>
        <aside class="hidden">
            <button type="button" class="button_next_chunk" data-shortcut="j"/>
            <button type="button" class="button_prev_chunk" data-shortcut="k"/>
            <button type="button" class="button_top_of_page" data-shortcut="0"/>
            <button type="button" class="button_first_chunk" data-shortcut="1"/>
            <button type="button" class="button_prev_file" data-shortcut="["/>
            <button type="button" class="button_next_file" data-shortcut="]"/>
            <button type="button" class="button_to_index" data-shortcut="u"/>
            <button type="button" class="button_show_hide_help" data-shortcut="?"/>
        </aside>
    </div>
</header>
<main id="source">
    <p class="pln"><span class="n"><a id="t1" href="#t1">1</a></span><span class="t"><span class="str">"""Hooked Transformer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2" href="#t2">2</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t3" href="#t3">3</a></span><span class="t"><span class="str">The Hooked Transformer is the core part of TransformerLens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t4" href="#t4">4</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t5" href="#t5">5</a></span><span class="t"><span class="str">In common PyTorch model implementations (e.g. ones from HuggingFace) it's fairly easy to extract</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t6" href="#t6">6</a></span><span class="t"><span class="str">model weights, but much harder to extract activations. TransformerLens aims to simplify this task by</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t7" href="#t7">7</a></span><span class="t"><span class="str">attaching hooks to every notable activation within the model. This enables the inspection and/or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t8" href="#t8">8</a></span><span class="t"><span class="str">alteration of activations in individual components like attention heads and MLP layers, facilitating</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t9" href="#t9">9</a></span><span class="t"><span class="str">a deeper understanding of the internal workings of transformers like GPT-2.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t10" href="#t10">10</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t11" href="#t11">11</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t12" href="#t12">12</a></span><span class="t"><span class="key">import</span> <span class="nam">logging</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t13" href="#t13">13</a></span><span class="t"><span class="key">import</span> <span class="nam">os</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t14" href="#t14">14</a></span><span class="t"><span class="key">from</span> <span class="nam">typing</span> <span class="key">import</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t15" href="#t15">15</a></span><span class="t">    <span class="nam">Dict</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t16" href="#t16">16</a></span><span class="t">    <span class="nam">List</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t17" href="#t17">17</a></span><span class="t">    <span class="nam">NamedTuple</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t18" href="#t18">18</a></span><span class="t">    <span class="nam">Optional</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t19" href="#t19">19</a></span><span class="t">    <span class="nam">Tuple</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t20" href="#t20">20</a></span><span class="t">    <span class="nam">Type</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t21" href="#t21">21</a></span><span class="t">    <span class="nam">TypeVar</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t22" href="#t22">22</a></span><span class="t">    <span class="nam">Union</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t23" href="#t23">23</a></span><span class="t">    <span class="nam">cast</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t24" href="#t24">24</a></span><span class="t">    <span class="nam">overload</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t25" href="#t25">25</a></span><span class="t"><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t26" href="#t26">26</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t27" href="#t27">27</a></span><span class="t"><span class="key">import</span> <span class="nam">einops</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t28" href="#t28">28</a></span><span class="t"><span class="key">import</span> <span class="nam">numpy</span> <span class="key">as</span> <span class="nam">np</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t29" href="#t29">29</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t30" href="#t30">30</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">nn</span> <span class="key">as</span> <span class="nam">nn</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t31" href="#t31">31</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">nn</span><span class="op">.</span><span class="nam">functional</span> <span class="key">as</span> <span class="nam">F</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t32" href="#t32">32</a></span><span class="t"><span class="key">import</span> <span class="nam">tqdm</span><span class="op">.</span><span class="nam">auto</span> <span class="key">as</span> <span class="nam">tqdm</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t33" href="#t33">33</a></span><span class="t"><span class="key">from</span> <span class="nam">jaxtyping</span> <span class="key">import</span> <span class="nam">Float</span><span class="op">,</span> <span class="nam">Int</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t34" href="#t34">34</a></span><span class="t"><span class="key">from</span> <span class="nam">packaging</span> <span class="key">import</span> <span class="nam">version</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t35" href="#t35">35</a></span><span class="t"><span class="key">from</span> <span class="nam">transformers</span> <span class="key">import</span> <span class="nam">AutoModelForCausalLM</span><span class="op">,</span> <span class="nam">AutoTokenizer</span><span class="op">,</span> <span class="nam">PreTrainedTokenizerBase</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t36" href="#t36">36</a></span><span class="t"><span class="key">from</span> <span class="nam">typing_extensions</span> <span class="key">import</span> <span class="nam">Literal</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t37" href="#t37">37</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t38" href="#t38">38</a></span><span class="t"><span class="key">import</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">loading_from_pretrained</span> <span class="key">as</span> <span class="nam">loading</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t39" href="#t39">39</a></span><span class="t"><span class="key">import</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utils</span> <span class="key">as</span> <span class="nam">utils</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t40" href="#t40">40</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">ActivationCache</span> <span class="key">import</span> <span class="nam">ActivationCache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t41" href="#t41">41</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">components</span> <span class="key">import</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t42" href="#t42">42</a></span><span class="t">    <span class="nam">Embed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t43" href="#t43">43</a></span><span class="t">    <span class="nam">LayerNorm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t44" href="#t44">44</a></span><span class="t">    <span class="nam">LayerNormPre</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t45" href="#t45">45</a></span><span class="t">    <span class="nam">PosEmbed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t46" href="#t46">46</a></span><span class="t">    <span class="nam">RMSNorm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t47" href="#t47">47</a></span><span class="t">    <span class="nam">RMSNormPre</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t48" href="#t48">48</a></span><span class="t">    <span class="nam">TransformerBlock</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t49" href="#t49">49</a></span><span class="t">    <span class="nam">Unembed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t50" href="#t50">50</a></span><span class="t"><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t51" href="#t51">51</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">FactoredMatrix</span> <span class="key">import</span> <span class="nam">FactoredMatrix</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t52" href="#t52">52</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">hook_points</span> <span class="key">import</span> <span class="nam">HookedRootModule</span><span class="op">,</span> <span class="nam">HookPoint</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t53" href="#t53">53</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">HookedTransformerConfig</span> <span class="key">import</span> <span class="nam">HookedTransformerConfig</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t54" href="#t54">54</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">loading_from_pretrained</span> <span class="key">import</span> <span class="nam">NON_HF_HOSTED_MODEL_NAMES</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t55" href="#t55">55</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t56" href="#t56">56</a></span><span class="t"><span class="com"># Note - activation cache is used with run_with_cache, past_key_value_caching is used for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t57" href="#t57">57</a></span><span class="t"><span class="com"># generation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t58" href="#t58">58</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">past_key_value_caching</span> <span class="key">import</span> <span class="nam">HookedTransformerKeyValueCache</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t59" href="#t59">59</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utilities</span> <span class="key">import</span> <span class="nam">devices</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t60" href="#t60">60</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utils</span> <span class="key">import</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t61" href="#t61">61</a></span><span class="t">    <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t62" href="#t62">62</a></span><span class="t">    <span class="nam">init_kaiming_normal_</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t63" href="#t63">63</a></span><span class="t">    <span class="nam">init_kaiming_uniform_</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t64" href="#t64">64</a></span><span class="t">    <span class="nam">init_xavier_normal_</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t65" href="#t65">65</a></span><span class="t">    <span class="nam">init_xavier_uniform_</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t66" href="#t66">66</a></span><span class="t"><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t67" href="#t67">67</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t68" href="#t68">68</a></span><span class="t"><span class="nam">SingleLoss</span> <span class="op">=</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">""</span><span class="op">]</span>  <span class="com"># Type alias for a single element tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t69" href="#t69">69</a></span><span class="t"><span class="nam">LossPerToken</span> <span class="op">=</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos-1"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t70" href="#t70">70</a></span><span class="t"><span class="nam">Loss</span> <span class="op">=</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">SingleLoss</span><span class="op">,</span> <span class="nam">LossPerToken</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t71" href="#t71">71</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t72" href="#t72">72</a></span><span class="t"><span class="nam">DTYPE_FROM_STRING</span> <span class="op">=</span> <span class="op">{</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t73" href="#t73">73</a></span><span class="t">    <span class="str">"float32"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t74" href="#t74">74</a></span><span class="t">    <span class="str">"fp32"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t75" href="#t75">75</a></span><span class="t">    <span class="str">"float16"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float16</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t76" href="#t76">76</a></span><span class="t">    <span class="str">"fp16"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float16</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t77" href="#t77">77</a></span><span class="t">    <span class="str">"bfloat16"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t78" href="#t78">78</a></span><span class="t">    <span class="str">"bf16"</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t79" href="#t79">79</a></span><span class="t"><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t80" href="#t80">80</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t81" href="#t81">81</a></span><span class="t"><span class="nam">T</span> <span class="op">=</span> <span class="nam">TypeVar</span><span class="op">(</span><span class="str">"T"</span><span class="op">,</span> <span class="nam">bound</span><span class="op">=</span><span class="str">"HookedTransformer"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t82" href="#t82">82</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t83" href="#t83">83</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t84" href="#t84">84</a></span><span class="t"><span class="key">class</span> <span class="nam">Output</span><span class="op">(</span><span class="nam">NamedTuple</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t85" href="#t85">85</a></span><span class="t">    <span class="str">"""Output Named Tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t86" href="#t86">86</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t87" href="#t87">87</a></span><span class="t"><span class="str">    Named tuple object for if we want to output both logits and loss.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t88" href="#t88">88</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t89" href="#t89">89</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t90" href="#t90">90</a></span><span class="t">    <span class="nam">logits</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t91" href="#t91">91</a></span><span class="t">    <span class="nam">loss</span><span class="op">:</span> <span class="nam">Loss</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t92" href="#t92">92</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t93" href="#t93">93</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t94" href="#t94">94</a></span><span class="t"><span class="key">class</span> <span class="nam">HookedTransformer</span><span class="op">(</span><span class="nam">HookedRootModule</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t95" href="#t95">95</a></span><span class="t">    <span class="str">"""Hooked Transformer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t96" href="#t96">96</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t97" href="#t97">97</a></span><span class="t"><span class="str">    Implements a full Transformer using the components :doc:`here &lt;transformer_lens.components>`,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t98" href="#t98">98</a></span><span class="t"><span class="str">    with a :class:`transformer_lens.hook_points.HookPoint` on every interesting activation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t99" href="#t99">99</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t100" href="#t100">100</a></span><span class="t"><span class="str">    TransformerLens comes loaded with >50 GPT-style models. Typically you initialise it with one of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t101" href="#t101">101</a></span><span class="t"><span class="str">    these via :meth:`from_pretrained`, although it can also be instantiated with randomly</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t102" href="#t102">102</a></span><span class="t"><span class="str">    initialized weights via :meth:`__init__`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t103" href="#t103">103</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t104" href="#t104">104</a></span><span class="t"><span class="str">    Once you've initialized the model, a common next step is to test it can do the task you're</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t105" href="#t105">105</a></span><span class="t"><span class="str">    investigating. This can be done with :func:`transformer_lens.utils.test_prompt`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t106" href="#t106">106</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t107" href="#t107">107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t108" href="#t108">108</a></span><span class="t">    <span class="nam">ln_final</span><span class="op">:</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">Module</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t109" href="#t109">109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t110" href="#t110">110</a></span><span class="t">    <span class="key">def</span> <span class="nam">__init__</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t111" href="#t111">111</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t112" href="#t112">112</a></span><span class="t">        <span class="nam">cfg</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">HookedTransformerConfig</span><span class="op">,</span> <span class="nam">Dict</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t113" href="#t113">113</a></span><span class="t">        <span class="nam">tokenizer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">PreTrainedTokenizerBase</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t114" href="#t114">114</a></span><span class="t">        <span class="nam">move_to_device</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t115" href="#t115">115</a></span><span class="t">        <span class="nam">default_padding_side</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span> <span class="op">=</span> <span class="str">"right"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t116" href="#t116">116</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t117" href="#t117">117</a></span><span class="t">        <span class="str">"""Model initialization.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t118" href="#t118">118</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t119" href="#t119">119</a></span><span class="t"><span class="str">        Note that if you want to load the model from pretrained weights, you should use</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t120" href="#t120">120</a></span><span class="t"><span class="str">        :meth:`from_pretrained` instead.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t121" href="#t121">121</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t122" href="#t122">122</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t123" href="#t123">123</a></span><span class="t"><span class="str">            cfg: The config to use for the model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t124" href="#t124">124</a></span><span class="t"><span class="str">            tokenizer: The tokenizer to use for the model. If not provided, it is inferred from</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t125" href="#t125">125</a></span><span class="t"><span class="str">                `cfg.tokenizer_name` or initialized to `None`. If `None`, then the model cannot be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t126" href="#t126">126</a></span><span class="t"><span class="str">                passed strings, and d_vocab must be explicitly set.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t127" href="#t127">127</a></span><span class="t"><span class="str">            move_to_device: Whether to move the model to the device specified in cfg.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t128" href="#t128">128</a></span><span class="t"><span class="str">                device. Must be true if `n_devices` in the config is greater than 1, since the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t129" href="#t129">129</a></span><span class="t"><span class="str">                model's layers will be split across multiple devices.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t130" href="#t130">130</a></span><span class="t"><span class="str">            default_padding_side: Which side to pad on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t131" href="#t131">131</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t132" href="#t132">132</a></span><span class="t">        <span class="nam">super</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">__init__</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t133" href="#t133">133</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">cfg</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">133&#x202F;&#x219B;&#x202F;134</span><span class="annotate long">line 133 didn't jump to line 134, because the condition on line 133 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t134" href="#t134">134</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t135" href="#t135">135</a></span><span class="t">                <span class="str">"Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a "</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t136" href="#t136">136</a></span><span class="t">                <span class="str">"pretrained model, use HookedTransformer.from_pretrained() instead."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t137" href="#t137">137</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t138" href="#t138">138</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t139" href="#t139">139</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span> <span class="op">=</span> <span class="nam">HookedTransformerConfig</span><span class="op">.</span><span class="nam">unwrap</span><span class="op">(</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t140" href="#t140">140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t141" href="#t141">141</a></span><span class="t">        <span class="key">if</span> <span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t142" href="#t142">142</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">set_tokenizer</span><span class="op">(</span><span class="nam">tokenizer</span><span class="op">,</span> <span class="nam">default_padding_side</span><span class="op">=</span><span class="nam">default_padding_side</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t143" href="#t143">143</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_name</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t144" href="#t144">144</a></span><span class="t">            <span class="com"># If we have a tokenizer name, we can load it from HuggingFace</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t145" href="#t145">145</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_name</span> <span class="key">in</span> <span class="nam">NON_HF_HOSTED_MODEL_NAMES</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">145&#x202F;&#x219B;&#x202F;146</span><span class="annotate long">line 145 didn't jump to line 146, because the condition on line 145 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t146" href="#t146">146</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t147" href="#t147">147</a></span><span class="t">                    <span class="str">"%s tokenizer not loaded. Please load manually."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t148" href="#t148">148</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_name</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t149" href="#t149">149</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t150" href="#t150">150</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t151" href="#t151">151</a></span><span class="t">                <span class="com"># Hugging Face defaults to use_fast to True</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t152" href="#t152">152</a></span><span class="t">                <span class="nam">use_fast</span> <span class="op">=</span> <span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t153" href="#t153">153</a></span><span class="t">                <span class="com"># Phi model's fast tokenizer does not support adding a BOS token, use_fast</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t154" href="#t154">154</a></span><span class="t">                <span class="com"># should be False</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t155" href="#t155">155</a></span><span class="t">                <span class="key">if</span> <span class="str">"phi"</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_name</span><span class="op">.</span><span class="nam">lower</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">155&#x202F;&#x219B;&#x202F;156</span><span class="annotate long">line 155 didn't jump to line 156, because the condition on line 155 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t156" href="#t156">156</a></span><span class="t">                    <span class="nam">use_fast</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t157" href="#t157">157</a></span><span class="t">                <span class="nam">huggingface_token</span> <span class="op">=</span> <span class="nam">os</span><span class="op">.</span><span class="nam">environ</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"HF_TOKEN"</span><span class="op">,</span> <span class="key">None</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t158" href="#t158">158</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">set_tokenizer</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t159" href="#t159">159</a></span><span class="t">                    <span class="nam">AutoTokenizer</span><span class="op">.</span><span class="nam">from_pretrained</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t160" href="#t160">160</a></span><span class="t">                        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_name</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t161" href="#t161">161</a></span><span class="t">                        <span class="nam">add_bos_token</span><span class="op">=</span><span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t162" href="#t162">162</a></span><span class="t">                        <span class="nam">trust_remote_code</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">trust_remote_code</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t163" href="#t163">163</a></span><span class="t">                        <span class="nam">use_fast</span><span class="op">=</span><span class="nam">use_fast</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t164" href="#t164">164</a></span><span class="t">                        <span class="nam">token</span><span class="op">=</span><span class="nam">huggingface_token</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t165" href="#t165">165</a></span><span class="t">                    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t166" href="#t166">166</a></span><span class="t">                    <span class="nam">default_padding_side</span><span class="op">=</span><span class="nam">default_padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t167" href="#t167">167</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t168" href="#t168">168</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t169" href="#t169">169</a></span><span class="t">            <span class="com"># If no tokenizer name is provided, we assume we're training on an algorithmic task and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t170" href="#t170">170</a></span><span class="t">            <span class="com"># will pass in tokens directly. In this case, we don't need a tokenizer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t171" href="#t171">171</a></span><span class="t">            <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab</span> <span class="op">!=</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="str">"Must provide a tokenizer if d_vocab is not provided"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t172" href="#t172">172</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t173" href="#t173">173</a></span><span class="t">            <span class="key">if</span> <span class="nam">default_padding_side</span> <span class="op">!=</span> <span class="str">"right"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">173&#x202F;&#x219B;&#x202F;174</span><span class="annotate long">line 173 didn't jump to line 174, because the condition on line 173 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t174" href="#t174">174</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t175" href="#t175">175</a></span><span class="t">                    <span class="str">"default_padding_side is explictly given but ignored because tokenizer is not set."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t176" href="#t176">176</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t177" href="#t177">177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t178" href="#t178">178</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">embed</span> <span class="op">=</span> <span class="nam">Embed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t179" href="#t179">179</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_embed</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t180" href="#t180">180</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t181" href="#t181">181</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">!=</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t182" href="#t182">182</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">pos_embed</span> <span class="op">=</span> <span class="nam">PosEmbed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t183" href="#t183">183</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pos_embed</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos, d__dictmodel]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t184" href="#t184">184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t185" href="#t185">185</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_hook_tokens</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t186" href="#t186">186</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_tokens</span> <span class="op">=</span> <span class="nam">HookPoint</span><span class="op">(</span><span class="op">)</span>  <span class="com"># [batch, pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t187" href="#t187">187</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t188" href="#t188">188</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span> <span class="op">=</span> <span class="nam">nn</span><span class="op">.</span><span class="nam">ModuleList</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t189" href="#t189">189</a></span><span class="t">            <span class="op">[</span><span class="nam">TransformerBlock</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">,</span> <span class="nam">block_index</span><span class="op">)</span> <span class="key">for</span> <span class="nam">block_index</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t190" href="#t190">190</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t191" href="#t191">191</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t192" href="#t192">192</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"RMS"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">192&#x202F;&#x219B;&#x202F;193</span><span class="annotate long">line 192 didn't jump to line 193, because the condition on line 192 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t193" href="#t193">193</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">RMSNorm</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t194" href="#t194">194</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"RMSPre"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t195" href="#t195">195</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t196" href="#t196">196</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"LN"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t197" href="#t197">197</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">final_rms</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">197&#x202F;&#x219B;&#x202F;198</span><span class="annotate long">line 197 didn't jump to line 198, because the condition on line 197 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t198" href="#t198">198</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">RMSNorm</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t199" href="#t199">199</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t200" href="#t200">200</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">LayerNorm</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t201" href="#t201">201</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"LNPre"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t202" href="#t202">202</a></span><span class="t">            <span class="com"># We've folded in LayerNorm weights, so just need the center + scale parts</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t203" href="#t203">203</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">final_rms</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t204" href="#t204">204</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t205" href="#t205">205</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t206" href="#t206">206</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">LayerNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t207" href="#t207">207</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">207&#x202F;&#x219B;&#x202F;211</span><span class="annotate long">line 207 didn't jump to line 211, because the condition on line 207 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t208" href="#t208">208</a></span><span class="t">            <span class="com"># If it's None, don't create either layer</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t209" href="#t209">209</a></span><span class="t">            <span class="key">pass</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t210" href="#t210">210</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t211" href="#t211">211</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">"Invalid normalization_type passed in %s"</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t212" href="#t212">212</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">unembed</span> <span class="op">=</span> <span class="nam">Unembed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t213" href="#t213">213</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t214" href="#t214">214</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t215" href="#t215">215</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">init_weights</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t216" href="#t216">216</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t217" href="#t217">217</a></span><span class="t">        <span class="key">if</span> <span class="nam">move_to_device</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t218" href="#t218">218</a></span><span class="t">            <span class="com"># We load the devices in a pipeline manner - the first device gets the embed and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t219" href="#t219">219</a></span><span class="t">            <span class="com"># pos_embed layers and the first n_layers // n_devices blocks, the second gets the next</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t220" href="#t220">220</a></span><span class="t">            <span class="com"># n_layers // n_devices blocks ... the last gets the last n_layers // n_devices blocks,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t221" href="#t221">221</a></span><span class="t">            <span class="com"># the final normalization layer (if it exists) and the unembed layer</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t222" href="#t222">222</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">move_model_modules_to_device</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t223" href="#t223">223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t224" href="#t224">224</a></span><span class="t">        <span class="com"># Helper variable to store a small (10K-20K) dataset of training data. Empty by default, can</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t225" href="#t225">225</a></span><span class="t">        <span class="com"># be loaded with load_sample_training_dataset</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t226" href="#t226">226</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t227" href="#t227">227</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t228" href="#t228">228</a></span><span class="t">        <span class="com"># Gives each module a parameter with its name (relative to this root module)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t229" href="#t229">229</a></span><span class="t">        <span class="com"># Needed for HookPoints to work</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t230" href="#t230">230</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">setup</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t231" href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t232" href="#t232">232</a></span><span class="t">    <span class="key">def</span> <span class="nam">check_hooks_to_add</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t233" href="#t233">233</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t234" href="#t234">234</a></span><span class="t">        <span class="nam">hook_point</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t235" href="#t235">235</a></span><span class="t">        <span class="nam">hook_point_name</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t236" href="#t236">236</a></span><span class="t">        <span class="nam">hook</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t237" href="#t237">237</a></span><span class="t">        <span class="nam">dir</span><span class="op">=</span><span class="str">"fwd"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t238" href="#t238">238</a></span><span class="t">        <span class="nam">is_permanent</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t239" href="#t239">239</a></span><span class="t">        <span class="nam">prepend</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t240" href="#t240">240</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t241" href="#t241">241</a></span><span class="t">        <span class="key">if</span> <span class="nam">hook_point_name</span><span class="op">.</span><span class="nam">endswith</span><span class="op">(</span><span class="str">"attn.hook_result"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t242" href="#t242">242</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t243" href="#t243">243</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_result</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t244" href="#t244">244</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">f"Cannot add hook {hook_point_name} if use_attn_result_hook is False"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t245" href="#t245">245</a></span><span class="t">        <span class="key">if</span> <span class="nam">hook_point_name</span><span class="op">.</span><span class="nam">endswith</span><span class="op">(</span><span class="op">(</span><span class="str">"hook_q_input"</span><span class="op">,</span> <span class="str">"hook_k_input"</span><span class="op">,</span> <span class="str">"hook_v_input"</span><span class="op">)</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t246" href="#t246">246</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t247" href="#t247">247</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_split_qkv_input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t248" href="#t248">248</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">f"Cannot add hook {hook_point_name} if use_split_qkv_input is False"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t249" href="#t249">249</a></span><span class="t">        <span class="key">if</span> <span class="nam">hook_point_name</span><span class="op">.</span><span class="nam">endswith</span><span class="op">(</span><span class="str">"mlp_in"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t250" href="#t250">250</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t251" href="#t251">251</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_hook_mlp_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t252" href="#t252">252</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">f"Cannot add hook {hook_point_name} if use_hook_mlp_in is False"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t253" href="#t253">253</a></span><span class="t">        <span class="key">if</span> <span class="nam">hook_point_name</span><span class="op">.</span><span class="nam">endswith</span><span class="op">(</span><span class="str">"attn_in"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t254" href="#t254">254</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t255" href="#t255">255</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t256" href="#t256">256</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">f"Cannot add hook {hook_point_name} if use_attn_in is False"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t257" href="#t257">257</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t258" href="#t258">258</a></span><span class="t">    <span class="key">def</span> <span class="nam">input_to_embed</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t259" href="#t259">259</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t260" href="#t260">260</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">,</span> <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t261" href="#t261">261</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t262" href="#t262">262</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t263" href="#t263">263</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t264" href="#t264">264</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t265" href="#t265">265</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t266" href="#t266">266</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">,</span>  <span class="com"># residual</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t267" href="#t267">267</a></span><span class="t">        <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>  <span class="com"># tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t268" href="#t268">268</a></span><span class="t">        <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>  <span class="com"># shortformer_pos_embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t269" href="#t269">269</a></span><span class="t">        <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">,</span>  <span class="com"># attention_mask [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t270" href="#t270">270</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t271" href="#t271">271</a></span><span class="t">        <span class="str">"""Convert input to first residual stream.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t272" href="#t272">272</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t273" href="#t273">273</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t274" href="#t274">274</a></span><span class="t"><span class="str">            input (Union[str, List[str], Int[torch.Tensor, "batch pos"]]): The input to the model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t275" href="#t275">275</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t276" href="#t276">276</a></span><span class="t"><span class="str">                the BOS token to the input (only applies when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t277" href="#t277">277</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos which is set to True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t278" href="#t278">278</a></span><span class="t"><span class="str">                otherwise. Pass True or False to locally override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t279" href="#t279">279</a></span><span class="t"><span class="str">            padding_side ([Literal["left", "right"], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t280" href="#t280">280</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t281" href="#t281">281</a></span><span class="t"><span class="str">                multiple strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t282" href="#t282">282</a></span><span class="t"><span class="str">            past_kv_cache (HookedTransformerKeyValueCache, optional): If passed, we're doing caching</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t283" href="#t283">283</a></span><span class="t"><span class="str">                and attention_mask will be stored in the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t284" href="#t284">284</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t285" href="#t285">285</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span> <span class="key">or</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">list</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t286" href="#t286">286</a></span><span class="t">            <span class="com"># If text, convert to tokens (batch_size=1)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t287" href="#t287">287</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t288" href="#t288">288</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t289" href="#t289">289</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">"Must provide a tokenizer if passing a string to the model"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t290" href="#t290">290</a></span><span class="t">            <span class="com"># This is only intended to support passing in a single string</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t291" href="#t291">291</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t292" href="#t292">292</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t293" href="#t293">293</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t294" href="#t294">294</a></span><span class="t">        <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">294&#x202F;&#x219B;&#x202F;296</span><span class="annotate long">line 294 didn't jump to line 296, because the condition on line 294 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t295" href="#t295">295</a></span><span class="t">            <span class="com"># If tokens are a rank 1 tensor, add a dummy batch dimension to avoid things breaking.</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t296" href="#t296">296</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">[</span><span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t297" href="#t297">297</a></span><span class="t">        <span class="key">if</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">device</span><span class="op">.</span><span class="nam">type</span> <span class="op">!=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t298" href="#t298">298</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t299" href="#t299">299</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t300" href="#t300">300</a></span><span class="t">        <span class="key">if</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t301" href="#t301">301</a></span><span class="t">            <span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">padding_side</span> <span class="op">==</span> <span class="str">"left"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t302" href="#t302">302</a></span><span class="t">            <span class="key">or</span> <span class="nam">attention_mask</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t303" href="#t303">303</a></span><span class="t">            <span class="key">or</span> <span class="nam">past_kv_cache</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t304" href="#t304">304</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t305" href="#t305">305</a></span><span class="t">            <span class="com"># This means we need to have an explicit attention mask.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t306" href="#t306">306</a></span><span class="t">            <span class="key">if</span> <span class="nam">attention_mask</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t307" href="#t307">307</a></span><span class="t">                <span class="com"># If the padding side is left or we are using caching, we need to compute the attention</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t308" href="#t308">308</a></span><span class="t">                <span class="com"># mask for the adjustment of absolute positional embeddings and attention masking so</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t309" href="#t309">309</a></span><span class="t">                <span class="com"># that pad tokens are not attended.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t310" href="#t310">310</a></span><span class="t">                <span class="key">if</span> <span class="nam">prepend_bos</span> <span class="key">is</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t311" href="#t311">311</a></span><span class="t">                    <span class="nam">prepend_bos</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">default_prepend_bos</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t312" href="#t312">312</a></span><span class="t">                <span class="nam">attention_mask</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">get_attention_mask</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t313" href="#t313">313</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t314" href="#t314">314</a></span><span class="t">            <span class="key">assert</span> <span class="nam">attention_mask</span><span class="op">.</span><span class="nam">shape</span> <span class="op">==</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">,</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t315" href="#t315">315</a></span><span class="t">                <span class="str">f"Attention mask shape {attention_mask.shape} does not match tokens shape "</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t316" href="#t316">316</a></span><span class="t">                <span class="str">f"{tokens.shape}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t317" href="#t317">317</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t318" href="#t318">318</a></span><span class="t">            <span class="nam">attention_mask</span> <span class="op">=</span> <span class="nam">attention_mask</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t319" href="#t319">319</a></span><span class="t">            <span class="key">if</span> <span class="nam">past_kv_cache</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t320" href="#t320">320</a></span><span class="t">                <span class="com"># past_kv_cache is not None, so we're doing caching.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t321" href="#t321">321</a></span><span class="t">                <span class="com"># We need to extend the previous attention_mask.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t322" href="#t322">322</a></span><span class="t">                <span class="com"># Update the past_kv_cache with the new attention_mask (unless it's frozen)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t323" href="#t323">323</a></span><span class="t">                <span class="nam">attention_mask</span> <span class="op">=</span> <span class="nam">past_kv_cache</span><span class="op">.</span><span class="nam">append_attention_mask</span><span class="op">(</span><span class="nam">attention_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t324" href="#t324">324</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t325" href="#t325">325</a></span><span class="t">            <span class="com"># We separate this case from for computational efficiency.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t326" href="#t326">326</a></span><span class="t">            <span class="nam">attention_mask</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t327" href="#t327">327</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t328" href="#t328">328</a></span><span class="t">        <span class="com"># If we're doing caching, then we reuse keys and values from previous runs, as that's the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t329" href="#t329">329</a></span><span class="t">        <span class="com"># only way that past activations will affect the final logits. The cache contains those so</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t330" href="#t330">330</a></span><span class="t">        <span class="com"># we don't need to recompute them. This is useful for generating text. As we have absolute</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t331" href="#t331">331</a></span><span class="t">        <span class="com"># positional encodings, to implement this we have a `pos_offset` variable, defaulting to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t332" href="#t332">332</a></span><span class="t">        <span class="com"># zero, which says to offset which positional encodings are used (cached keys and values</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t333" href="#t333">333</a></span><span class="t">        <span class="com"># were calculated with their own positional encodings).</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t334" href="#t334">334</a></span><span class="t">        <span class="key">if</span> <span class="nam">past_kv_cache</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t335" href="#t335">335</a></span><span class="t">            <span class="nam">pos_offset</span> <span class="op">=</span> <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t336" href="#t336">336</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t337" href="#t337">337</a></span><span class="t">            <span class="nam">batch_size</span><span class="op">,</span> <span class="nam">ctx_length</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t338" href="#t338">338</a></span><span class="t">            <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t339" href="#t339">339</a></span><span class="t">                <span class="nam">cached_batch_size</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t340" href="#t340">340</a></span><span class="t">                <span class="nam">cache_ctx_length</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t341" href="#t341">341</a></span><span class="t">                <span class="nam">num_heads_in_cache</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t342" href="#t342">342</a></span><span class="t">                <span class="nam">d_head_in_cache</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t343" href="#t343">343</a></span><span class="t">            <span class="op">)</span> <span class="op">=</span> <span class="nam">past_kv_cache</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">past_keys</span><span class="op">.</span><span class="nam">shape</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t344" href="#t344">344</a></span><span class="t">            <span class="key">assert</span> <span class="nam">cached_batch_size</span> <span class="op">==</span> <span class="nam">batch_size</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t345" href="#t345">345</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">345&#x202F;&#x219B;&#x202F;348</span><span class="annotate long">line 345 didn't jump to line 348, because the condition on line 345 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t346" href="#t346">346</a></span><span class="t">                <span class="key">assert</span> <span class="nam">num_heads_in_cache</span> <span class="op">==</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t347" href="#t347">347</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t348" href="#t348">348</a></span><span class="t">                <span class="key">assert</span> <span class="nam">num_heads_in_cache</span> <span class="op">==</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t349" href="#t349">349</a></span><span class="t">            <span class="key">assert</span> <span class="nam">d_head_in_cache</span> <span class="op">==</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t350" href="#t350">350</a></span><span class="t">            <span class="nam">pos_offset</span> <span class="op">=</span> <span class="nam">cache_ctx_length</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t351" href="#t351">351</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_hook_tokens</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t352" href="#t352">352</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_tokens</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t353" href="#t353">353</a></span><span class="t">        <span class="nam">embed</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_embed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">embed</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span><span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t354" href="#t354">354</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"standard"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t355" href="#t355">355</a></span><span class="t">            <span class="nam">pos_embed</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pos_embed</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t356" href="#t356">356</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">pos_embed</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">pos_offset</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t357" href="#t357">357</a></span><span class="t">            <span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t358" href="#t358">358</a></span><span class="t">            <span class="nam">residual</span> <span class="op">=</span> <span class="nam">embed</span> <span class="op">+</span> <span class="nam">pos_embed</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t359" href="#t359">359</a></span><span class="t">            <span class="nam">shortformer_pos_embed</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t360" href="#t360">360</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"shortformer"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t361" href="#t361">361</a></span><span class="t">            <span class="com"># If we're using shortformer style attention, we don't add the positional embedding to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t362" href="#t362">362</a></span><span class="t">            <span class="com"># the residual stream. See HookedTransformerConfig for details</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t363" href="#t363">363</a></span><span class="t">            <span class="nam">pos_embed</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pos_embed</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t364" href="#t364">364</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">pos_embed</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">pos_offset</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t365" href="#t365">365</a></span><span class="t">            <span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t366" href="#t366">366</a></span><span class="t">            <span class="nam">residual</span> <span class="op">=</span> <span class="nam">embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t367" href="#t367">367</a></span><span class="t">            <span class="nam">shortformer_pos_embed</span> <span class="op">=</span> <span class="nam">pos_embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t368" href="#t368">368</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t369" href="#t369">369</a></span><span class="t">            <span class="com"># Rotary doesn't use positional embeddings, instead they're applied when dot producting</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t370" href="#t370">370</a></span><span class="t">            <span class="com"># keys and queries. See HookedTransformerConfig for details</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t371" href="#t371">371</a></span><span class="t">            <span class="nam">residual</span> <span class="op">=</span> <span class="nam">embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t372" href="#t372">372</a></span><span class="t">            <span class="nam">shortformer_pos_embed</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t373" href="#t373">373</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"alibi"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">373&#x202F;&#x219B;&#x202F;378</span><span class="annotate long">line 373 didn't jump to line 378, because the condition on line 373 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t374" href="#t374">374</a></span><span class="t">            <span class="com"># ALiBi does not add positional embeddings to word embeddings,instead it biases QK attention scores.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t375" href="#t375">375</a></span><span class="t">            <span class="nam">residual</span> <span class="op">=</span> <span class="nam">embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t376" href="#t376">376</a></span><span class="t">            <span class="nam">shortformer_pos_embed</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t377" href="#t377">377</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t378" href="#t378">378</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t379" href="#t379">379</a></span><span class="t">                <span class="str">f"Invalid positional_embedding_type passed in {self.cfg.positional_embedding_type}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t380" href="#t380">380</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t381" href="#t381">381</a></span><span class="t">        <span class="key">return</span> <span class="nam">residual</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">,</span> <span class="nam">shortformer_pos_embed</span><span class="op">,</span> <span class="nam">attention_mask</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t382" href="#t382">382</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t383" href="#t383">383</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t384" href="#t384">384</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t385" href="#t385">385</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t386" href="#t386">386</a></span><span class="t">        <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t387" href="#t387">387</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"logits"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t388" href="#t388">388</a></span><span class="t">        <span class="nam">loss_per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t389" href="#t389">389</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t390" href="#t390">390</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t391" href="#t391">391</a></span><span class="t">        <span class="nam">start_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t392" href="#t392">392</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t393" href="#t393">393</a></span><span class="t">        <span class="nam">shortformer_pos_embed</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t394" href="#t394">394</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>  <span class="com"># [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t395" href="#t395">395</a></span><span class="t">        <span class="nam">stop_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t396" href="#t396">396</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t397" href="#t397">397</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Loss</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t398" href="#t398">398</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t399" href="#t399">399</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t400" href="#t400">400</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t401" href="#t401">401</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t402" href="#t402">402</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t403" href="#t403">403</a></span><span class="t">        <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t404" href="#t404">404</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"loss"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t405" href="#t405">405</a></span><span class="t">        <span class="nam">loss_per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t406" href="#t406">406</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t407" href="#t407">407</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t408" href="#t408">408</a></span><span class="t">        <span class="nam">start_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t409" href="#t409">409</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t410" href="#t410">410</a></span><span class="t">        <span class="nam">shortformer_pos_embed</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t411" href="#t411">411</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>  <span class="com"># [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t412" href="#t412">412</a></span><span class="t">        <span class="nam">stop_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t413" href="#t413">413</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t414" href="#t414">414</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Loss</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t415" href="#t415">415</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t416" href="#t416">416</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t417" href="#t417">417</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t418" href="#t418">418</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t419" href="#t419">419</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t420" href="#t420">420</a></span><span class="t">        <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t421" href="#t421">421</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"both"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t422" href="#t422">422</a></span><span class="t">        <span class="nam">loss_per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t423" href="#t423">423</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t424" href="#t424">424</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t425" href="#t425">425</a></span><span class="t">        <span class="nam">start_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t426" href="#t426">426</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t427" href="#t427">427</a></span><span class="t">        <span class="nam">shortformer_pos_embed</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t428" href="#t428">428</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>  <span class="com"># [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t429" href="#t429">429</a></span><span class="t">        <span class="nam">stop_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t430" href="#t430">430</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t431" href="#t431">431</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span> <span class="nam">Loss</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t432" href="#t432">432</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t433" href="#t433">433</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t434" href="#t434">434</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t435" href="#t435">435</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t436" href="#t436">436</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t437" href="#t437">437</a></span><span class="t">        <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t438" href="#t438">438</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t439" href="#t439">439</a></span><span class="t">        <span class="nam">loss_per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t440" href="#t440">440</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t441" href="#t441">441</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t442" href="#t442">442</a></span><span class="t">        <span class="nam">start_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t443" href="#t443">443</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t444" href="#t444">444</a></span><span class="t">        <span class="nam">shortformer_pos_embed</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t445" href="#t445">445</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>  <span class="com"># [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t446" href="#t446">446</a></span><span class="t">        <span class="nam">stop_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t447" href="#t447">447</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t448" href="#t448">448</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t449" href="#t449">449</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t450" href="#t450">450</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t451" href="#t451">451</a></span><span class="t">    <span class="key">def</span> <span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t452" href="#t452">452</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t453" href="#t453">453</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t454" href="#t454">454</a></span><span class="t">            <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t455" href="#t455">455</a></span><span class="t">            <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t456" href="#t456">456</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t457" href="#t457">457</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t458" href="#t458">458</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t459" href="#t459">459</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span> <span class="op">=</span> <span class="str">"logits"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t460" href="#t460">460</a></span><span class="t">        <span class="nam">loss_per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t461" href="#t461">461</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t462" href="#t462">462</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t463" href="#t463">463</a></span><span class="t">        <span class="nam">start_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t464" href="#t464">464</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t465" href="#t465">465</a></span><span class="t">        <span class="nam">shortformer_pos_embed</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t466" href="#t466">466</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>  <span class="com"># [batch pos]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t467" href="#t467">467</a></span><span class="t">        <span class="nam">stop_at_layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t468" href="#t468">468</a></span><span class="t">        <span class="nam">past_kv_cache</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">HookedTransformerKeyValueCache</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t469" href="#t469">469</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t470" href="#t470">470</a></span><span class="t">        <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t471" href="#t471">471</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t472" href="#t472">472</a></span><span class="t">        <span class="nam">Loss</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t473" href="#t473">473</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span> <span class="nam">Loss</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t474" href="#t474">474</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t475" href="#t475">475</a></span><span class="t">        <span class="str">"""Forward Pass.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t476" href="#t476">476</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t477" href="#t477">477</a></span><span class="t"><span class="str">        Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t478" href="#t478">478</a></span><span class="t"><span class="str">        tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t479" href="#t479">479</a></span><span class="t"><span class="str">        text string.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t480" href="#t480">480</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t481" href="#t481">481</a></span><span class="t"><span class="str">        Note that loss is the standard "predict the next token" cross-entropy loss for GPT-2 style</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t482" href="#t482">482</a></span><span class="t"><span class="str">        language models - if you want a custom loss function, the recommended behaviour is returning</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t483" href="#t483">483</a></span><span class="t"><span class="str">        the logits and then applying your custom loss function.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t484" href="#t484">484</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t485" href="#t485">485</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t486" href="#t486">486</a></span><span class="t"><span class="str">            return_type Optional[str]: The type of output to return. Can be one of: None (return</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t487" href="#t487">487</a></span><span class="t"><span class="str">                nothing, don't calculate logits), 'logits' (return logits), 'loss' (return</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t488" href="#t488">488</a></span><span class="t"><span class="str">                cross-entropy loss), 'both' (return logits and loss).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t489" href="#t489">489</a></span><span class="t"><span class="str">            loss_per_token bool: Whether to return the (next token prediction) loss per token (True)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t490" href="#t490">490</a></span><span class="t"><span class="str">                or average (False). Average loss is a scalar (averaged over position *and* batch),</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t491" href="#t491">491</a></span><span class="t"><span class="str">                per-token loss is a tensor ([batch, position-1]) - position-1 because we're</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t492" href="#t492">492</a></span><span class="t"><span class="str">                predicting the next token, and there's no specified next token for the final token.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t493" href="#t493">493</a></span><span class="t"><span class="str">                Defaults to False.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t494" href="#t494">494</a></span><span class="t"><span class="str">            prepend_bos Optional[bool]: Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t495" href="#t495">495</a></span><span class="t"><span class="str">                the BOS token to the input (only applies when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t496" href="#t496">496</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos which is set to True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t497" href="#t497">497</a></span><span class="t"><span class="str">                otherwise. (Even for models not explicitly trained with a prepended BOS token, heads</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t498" href="#t498">498</a></span><span class="t"><span class="str">                often use the first position as a resting position and accordingly lose information</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t499" href="#t499">499</a></span><span class="t"><span class="str">                from the first token, so this empirically seems to give better results.) Pass True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t500" href="#t500">500</a></span><span class="t"><span class="str">                or False to locally override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t501" href="#t501">501</a></span><span class="t"><span class="str">            padding_side Optional[Literal["left", "right"]]: Overrides self.tokenizer.padding_side.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t502" href="#t502">502</a></span><span class="t"><span class="str">                Specifies which side to pad on when tokenizing multiple strings of different</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t503" href="#t503">503</a></span><span class="t"><span class="str">                lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t504" href="#t504">504</a></span><span class="t"><span class="str">            start_at_layer Optional[int]: If not None, start the forward pass at the specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t505" href="#t505">505</a></span><span class="t"><span class="str">                layer. Requires input to be the residual stream before the specified layer with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t506" href="#t506">506</a></span><span class="t"><span class="str">                shape [batch, pos, d_model]. Inclusive - ie, start_at_layer = 0 skips the embedding</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t507" href="#t507">507</a></span><span class="t"><span class="str">                then runs the rest of the model. Supports negative indexing. start_at_layer = -1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t508" href="#t508">508</a></span><span class="t"><span class="str">                only runs the final block and the unembedding. Defaults to None (run the full</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t509" href="#t509">509</a></span><span class="t"><span class="str">                model).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t510" href="#t510">510</a></span><span class="t"><span class="str">            tokens: Optional[Int[torch.Tensor, "batch pos"]]: Tokenized input. Only use if</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t511" href="#t511">511</a></span><span class="t"><span class="str">                start_at_layer is not None and return type is "loss" or "both".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t512" href="#t512">512</a></span><span class="t"><span class="str">            shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]]: Positional</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t513" href="#t513">513</a></span><span class="t"><span class="str">                embedding for shortformer models. Only use if start_at_layer is not None and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t514" href="#t514">514</a></span><span class="t"><span class="str">                self.cfg.positional_embedding_type == "shortformer".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t515" href="#t515">515</a></span><span class="t"><span class="str">            attention_mask: Optional[torch.Tensor]: Override the attention mask used to ignore</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t516" href="#t516">516</a></span><span class="t"><span class="str">                padded tokens. If start_at_layer is not None and (self.tokenizer.padding_side ==</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t517" href="#t517">517</a></span><span class="t"><span class="str">                "left" or past_kv_cache is not None), this should be passed as the attention mask</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t518" href="#t518">518</a></span><span class="t"><span class="str">                is not computed automatically. Defaults to None.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t519" href="#t519">519</a></span><span class="t"><span class="str">            stop_at_layer Optional[int]: If not None, stop the forward pass at the specified layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t520" href="#t520">520</a></span><span class="t"><span class="str">                Exclusive - ie, stop_at_layer = 0 will only run the embedding layer, stop_at_layer =</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t521" href="#t521">521</a></span><span class="t"><span class="str">                1 will run the embedding layer and the first transformer block, etc. Supports</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t522" href="#t522">522</a></span><span class="t"><span class="str">                negative indexing. Useful for analysis of intermediate layers, eg finding neuron</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t523" href="#t523">523</a></span><span class="t"><span class="str">                activations in layer 3 of a 24 layer model. Defaults to None (run the full model).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t524" href="#t524">524</a></span><span class="t"><span class="str">                If not None, we return the last residual stream computed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t525" href="#t525">525</a></span><span class="t"><span class="str">            past_kv_cache Optional[HookedTransformerKeyValueCache]: If not None, keys and values</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t526" href="#t526">526</a></span><span class="t"><span class="str">                will be stored for every attention head (unless the cache is frozen). If there are</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t527" href="#t527">527</a></span><span class="t"><span class="str">                keys and values already in the cache, these will be prepended to the keys and values</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t528" href="#t528">528</a></span><span class="t"><span class="str">                for the new input, so that the new tokens can pay attention to previous tokens. This</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t529" href="#t529">529</a></span><span class="t"><span class="str">                is useful for generating text, because we don't need to repeat computation for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t530" href="#t530">530</a></span><span class="t"><span class="str">                tokens that have already been through the model. Also caches attention_mask so</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t531" href="#t531">531</a></span><span class="t"><span class="str">                previous tokens are masked correctly (unless frozen). Padding should be ignored in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t532" href="#t532">532</a></span><span class="t"><span class="str">                all cases, so it's okay to eg. pass in left padded tokens twice in a row.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t533" href="#t533">533</a></span><span class="t"><span class="str">                Warning: Don't accidentally prepend_bos to the second half of a prompt.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t534" href="#t534">534</a></span><span class="t"><span class="str">                Defaults to None (don't use caching).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t535" href="#t535">535</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t536" href="#t536">536</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t537" href="#t537">537</a></span><span class="t">        <span class="key">with</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">LocallyOverridenDefaults</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t538" href="#t538">538</a></span><span class="t">            <span class="nam">self</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t539" href="#t539">539</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t540" href="#t540">540</a></span><span class="t">            <span class="key">if</span> <span class="nam">start_at_layer</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t541" href="#t541">541</a></span><span class="t">                <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t542" href="#t542">542</a></span><span class="t">                    <span class="nam">residual</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t543" href="#t543">543</a></span><span class="t">                    <span class="nam">tokens</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t544" href="#t544">544</a></span><span class="t">                    <span class="nam">shortformer_pos_embed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t545" href="#t545">545</a></span><span class="t">                    <span class="nam">attention_mask</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t546" href="#t546">546</a></span><span class="t">                <span class="op">)</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">input_to_embed</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t547" href="#t547">547</a></span><span class="t">                    <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t548" href="#t548">548</a></span><span class="t">                    <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t549" href="#t549">549</a></span><span class="t">                    <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t550" href="#t550">550</a></span><span class="t">                    <span class="nam">attention_mask</span><span class="op">=</span><span class="nam">attention_mask</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t551" href="#t551">551</a></span><span class="t">                    <span class="nam">past_kv_cache</span><span class="op">=</span><span class="nam">past_kv_cache</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t552" href="#t552">552</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t553" href="#t553">553</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t554" href="#t554">554</a></span><span class="t">                <span class="key">assert</span> <span class="nam">type</span><span class="op">(</span><span class="nam">input</span><span class="op">)</span> <span class="op">==</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t555" href="#t555">555</a></span><span class="t">                <span class="nam">residual</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t556" href="#t556">556</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t557" href="#t557">557</a></span><span class="t">            <span class="key">if</span> <span class="nam">start_at_layer</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t558" href="#t558">558</a></span><span class="t">                <span class="nam">start_at_layer</span> <span class="op">=</span> <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t559" href="#t559">559</a></span><span class="t">            <span class="com"># If we explicitly want to start or stop at a layer, we only iterate through the blocks</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t560" href="#t560">560</a></span><span class="t">            <span class="com"># between those indices. Note that start_at_layer is inclusive and stop_at_layer is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t561" href="#t561">561</a></span><span class="t">            <span class="com"># exclusive.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t562" href="#t562">562</a></span><span class="t">            <span class="com"># Eg: start_at_layer==None + stop_at_layer==0 means to only run the embed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t563" href="#t563">563</a></span><span class="t">            <span class="com"># Eg: start_at_layer==3 + stop_at_layer==-1 means to run from layer 3 until the end of the PENULTIMATE layer</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t564" href="#t564">564</a></span><span class="t">            <span class="nam">blocks_and_idxs</span> <span class="op">=</span> <span class="nam">list</span><span class="op">(</span><span class="nam">zip</span><span class="op">(</span><span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t565" href="#t565">565</a></span><span class="t">            <span class="key">for</span> <span class="nam">i</span><span class="op">,</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">blocks_and_idxs</span><span class="op">[</span><span class="nam">start_at_layer</span><span class="op">:</span><span class="nam">stop_at_layer</span><span class="op">]</span><span class="op">:</span>  <span class="com"># type: ignore</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t566" href="#t566">566</a></span><span class="t">                <span class="com"># Note that each block includes skip connections, so we don't need</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t567" href="#t567">567</a></span><span class="t">                <span class="com"># residual + block(residual)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t568" href="#t568">568</a></span><span class="t">                <span class="com"># If we're using multiple GPUs, we need to send the residual and shortformer_pos_embed to the correct GPU</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t569" href="#t569">569</a></span><span class="t">                <span class="nam">residual</span> <span class="op">=</span> <span class="nam">residual</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="nam">i</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t570" href="#t570">570</a></span><span class="t">                <span class="key">if</span> <span class="nam">shortformer_pos_embed</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t571" href="#t571">571</a></span><span class="t">                    <span class="nam">shortformer_pos_embed</span> <span class="op">=</span> <span class="nam">shortformer_pos_embed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t572" href="#t572">572</a></span><span class="t">                        <span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="nam">i</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t573" href="#t573">573</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t574" href="#t574">574</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t575" href="#t575">575</a></span><span class="t">                <span class="nam">residual</span> <span class="op">=</span> <span class="nam">block</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t576" href="#t576">576</a></span><span class="t">                    <span class="nam">residual</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t577" href="#t577">577</a></span><span class="t">                    <span class="com"># Cache contains a list of HookedTransformerKeyValueCache objects, one for each</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t578" href="#t578">578</a></span><span class="t">                    <span class="com"># block</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t579" href="#t579">579</a></span><span class="t">                    <span class="nam">past_kv_cache_entry</span><span class="op">=</span><span class="nam">past_kv_cache</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">if</span> <span class="nam">past_kv_cache</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">else</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t580" href="#t580">580</a></span><span class="t">                    <span class="nam">shortformer_pos_embed</span><span class="op">=</span><span class="nam">shortformer_pos_embed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t581" href="#t581">581</a></span><span class="t">                    <span class="nam">attention_mask</span><span class="op">=</span><span class="nam">attention_mask</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t582" href="#t582">582</a></span><span class="t">                <span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t583" href="#t583">583</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t584" href="#t584">584</a></span><span class="t">            <span class="key">if</span> <span class="nam">stop_at_layer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t585" href="#t585">585</a></span><span class="t">                <span class="com"># When we stop at an early layer, we end here rather than doing further computation</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t586" href="#t586">586</a></span><span class="t">                <span class="key">return</span> <span class="nam">residual</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t587" href="#t587">587</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t588" href="#t588">588</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t589" href="#t589">589</a></span><span class="t">                <span class="nam">residual</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span><span class="op">(</span><span class="nam">residual</span><span class="op">)</span>  <span class="com"># [batch, pos, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t590" href="#t590">590</a></span><span class="t">            <span class="key">if</span> <span class="nam">return_type</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t591" href="#t591">591</a></span><span class="t">                <span class="key">return</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t592" href="#t592">592</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t593" href="#t593">593</a></span><span class="t">                <span class="nam">logits</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">unembed</span><span class="op">(</span><span class="nam">residual</span><span class="op">)</span>  <span class="com"># [batch, pos, d_vocab]</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t594" href="#t594">594</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">output_logits_soft_cap</span> <span class="op">></span> <span class="num">0.0</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">594&#x202F;&#x219B;&#x202F;595</span><span class="annotate long">line 594 didn't jump to line 595, because the condition on line 594 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t595" href="#t595">595</a></span><span class="t">                    <span class="nam">logits</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">output_logits_soft_cap</span> <span class="op">*</span> <span class="nam">F</span><span class="op">.</span><span class="nam">tanh</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t596" href="#t596">596</a></span><span class="t">                        <span class="nam">logits</span> <span class="op">/</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">output_logits_soft_cap</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t597" href="#t597">597</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t598" href="#t598">598</a></span><span class="t">                <span class="key">if</span> <span class="nam">return_type</span> <span class="op">==</span> <span class="str">"logits"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t599" href="#t599">599</a></span><span class="t">                    <span class="key">return</span> <span class="nam">logits</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t600" href="#t600">600</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t601" href="#t601">601</a></span><span class="t">                    <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t602" href="#t602">602</a></span><span class="t">                        <span class="nam">tokens</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t603" href="#t603">603</a></span><span class="t">                    <span class="op">)</span><span class="op">,</span> <span class="str">"tokens must be passed in if return_type is 'loss' or 'both'"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t604" href="#t604">604</a></span><span class="t">                    <span class="nam">loss</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">loss_fn</span><span class="op">(</span><span class="nam">logits</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">,</span> <span class="nam">per_token</span><span class="op">=</span><span class="nam">loss_per_token</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t605" href="#t605">605</a></span><span class="t">                    <span class="key">if</span> <span class="nam">return_type</span> <span class="op">==</span> <span class="str">"loss"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">605&#x202F;&#x219B;&#x202F;607</span><span class="annotate long">line 605 didn't jump to line 607, because the condition on line 605 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t606" href="#t606">606</a></span><span class="t">                        <span class="key">return</span> <span class="nam">loss</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t607" href="#t607">607</a></span><span class="t">                    <span class="key">elif</span> <span class="nam">return_type</span> <span class="op">==</span> <span class="str">"both"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t608" href="#t608">608</a></span><span class="t">                        <span class="key">return</span> <span class="nam">Output</span><span class="op">(</span><span class="nam">logits</span><span class="op">,</span> <span class="nam">loss</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t609" href="#t609">609</a></span><span class="t">                    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t610" href="#t610">610</a></span><span class="t">                        <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">f"Invalid return_type passed in: {return_type}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t611" href="#t611">611</a></span><span class="t">                        <span class="key">return</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t612" href="#t612">612</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t613" href="#t613">613</a></span><span class="t">    <span class="key">def</span> <span class="nam">loss_fn</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t614" href="#t614">614</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t615" href="#t615">615</a></span><span class="t">        <span class="nam">logits</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t616" href="#t616">616</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t617" href="#t617">617</a></span><span class="t">        <span class="nam">attention_mask</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t618" href="#t618">618</a></span><span class="t">        <span class="nam">per_token</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t619" href="#t619">619</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t620" href="#t620">620</a></span><span class="t">        <span class="str">"""Wrapper around `utils.lm_cross_entropy_loss`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t621" href="#t621">621</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t622" href="#t622">622</a></span><span class="t"><span class="str">        Used in forward() with return_type=="loss" or "both".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t623" href="#t623">623</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t624" href="#t624">624</a></span><span class="t">        <span class="key">if</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">device</span> <span class="op">!=</span> <span class="nam">logits</span><span class="op">.</span><span class="nam">device</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">624&#x202F;&#x219B;&#x202F;625</span><span class="annotate long">line 624 didn't jump to line 625, because the condition on line 624 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t625" href="#t625">625</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">logits</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t626" href="#t626">626</a></span><span class="t">        <span class="key">return</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">lm_cross_entropy_loss</span><span class="op">(</span><span class="nam">logits</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">,</span> <span class="nam">attention_mask</span><span class="op">,</span> <span class="nam">per_token</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t627" href="#t627">627</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t628" href="#t628">628</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t629" href="#t629">629</a></span><span class="t">    <span class="key">def</span> <span class="nam">run_with_cache</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t630" href="#t630">630</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">model_args</span><span class="op">,</span> <span class="nam">return_cache_object</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="key">True</span><span class="op">]</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t631" href="#t631">631</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Output</span><span class="op">,</span> <span class="nam">ActivationCache</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t632" href="#t632">632</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t633" href="#t633">633</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t634" href="#t634">634</a></span><span class="t">    <span class="op">@</span><span class="nam">overload</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t635" href="#t635">635</a></span><span class="t">    <span class="key">def</span> <span class="nam">run_with_cache</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t636" href="#t636">636</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">model_args</span><span class="op">,</span> <span class="nam">return_cache_object</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="key">False</span><span class="op">]</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t637" href="#t637">637</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Output</span><span class="op">,</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t638" href="#t638">638</a></span><span class="t">        <span class="op">...</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t639" href="#t639">639</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t640" href="#t640">640</a></span><span class="t">    <span class="key">def</span> <span class="nam">run_with_cache</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t641" href="#t641">641</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">model_args</span><span class="op">,</span> <span class="nam">return_cache_object</span><span class="op">=</span><span class="key">True</span><span class="op">,</span> <span class="nam">remove_batch_dim</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t642" href="#t642">642</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Tuple</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t643" href="#t643">643</a></span><span class="t">        <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t644" href="#t644">644</a></span><span class="t">            <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t645" href="#t645">645</a></span><span class="t">            <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t646" href="#t646">646</a></span><span class="t">            <span class="nam">Loss</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t647" href="#t647">647</a></span><span class="t">            <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_vocab"</span><span class="op">]</span><span class="op">,</span> <span class="nam">Loss</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t648" href="#t648">648</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t649" href="#t649">649</a></span><span class="t">        <span class="nam">Union</span><span class="op">[</span><span class="nam">ActivationCache</span><span class="op">,</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t650" href="#t650">650</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t651" href="#t651">651</a></span><span class="t">        <span class="str">"""Wrapper around `run_with_cache` in HookedRootModule.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t652" href="#t652">652</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t653" href="#t653">653</a></span><span class="t"><span class="str">        If return_cache_object is True, this will return an ActivationCache object, with a bunch of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t654" href="#t654">654</a></span><span class="t"><span class="str">        useful HookedTransformer specific methods, otherwise it will return a dictionary of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t655" href="#t655">655</a></span><span class="t"><span class="str">        activations as in HookedRootModule.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t656" href="#t656">656</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t657" href="#t657">657</a></span><span class="t">        <span class="nam">out</span><span class="op">,</span> <span class="nam">cache_dict</span> <span class="op">=</span> <span class="nam">super</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">run_with_cache</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t658" href="#t658">658</a></span><span class="t">            <span class="op">*</span><span class="nam">model_args</span><span class="op">,</span> <span class="nam">remove_batch_dim</span><span class="op">=</span><span class="nam">remove_batch_dim</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t659" href="#t659">659</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t660" href="#t660">660</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_cache_object</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">660&#x202F;&#x219B;&#x202F;664</span><span class="annotate long">line 660 didn't jump to line 664, because the condition on line 660 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t661" href="#t661">661</a></span><span class="t">            <span class="nam">cache</span> <span class="op">=</span> <span class="nam">ActivationCache</span><span class="op">(</span><span class="nam">cache_dict</span><span class="op">,</span> <span class="nam">self</span><span class="op">,</span> <span class="nam">has_batch_dim</span><span class="op">=</span><span class="key">not</span> <span class="nam">remove_batch_dim</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t662" href="#t662">662</a></span><span class="t">            <span class="key">return</span> <span class="nam">out</span><span class="op">,</span> <span class="nam">cache</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t663" href="#t663">663</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t664" href="#t664">664</a></span><span class="t">            <span class="key">return</span> <span class="nam">out</span><span class="op">,</span> <span class="nam">cache_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t665" href="#t665">665</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t666" href="#t666">666</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_tokenizer</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t667" href="#t667">667</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t668" href="#t668">668</a></span><span class="t">        <span class="nam">tokenizer</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t669" href="#t669">669</a></span><span class="t">        <span class="nam">default_padding_side</span><span class="op">=</span><span class="str">"right"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t670" href="#t670">670</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t671" href="#t671">671</a></span><span class="t">        <span class="str">"""Set the tokenizer to use for this model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t672" href="#t672">672</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t673" href="#t673">673</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t674" href="#t674">674</a></span><span class="t"><span class="str">            tokenizer (PreTrainedTokenizer): a pretrained HuggingFace tokenizer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t675" href="#t675">675</a></span><span class="t"><span class="str">            default_padding_side (str): "right" or "left", which side to pad on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t676" href="#t676">676</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t677" href="#t677">677</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t678" href="#t678">678</a></span><span class="t">        <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t679" href="#t679">679</a></span><span class="t">            <span class="nam">tokenizer</span><span class="op">,</span> <span class="nam">PreTrainedTokenizerBase</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t680" href="#t680">680</a></span><span class="t">        <span class="op">)</span><span class="op">,</span> <span class="str">f"{type(tokenizer)} is not a supported tokenizer, please use PreTrainedTokenizer or PreTrainedTokenizerFast"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t681" href="#t681">681</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t682" href="#t682">682</a></span><span class="t">        <span class="key">assert</span> <span class="nam">default_padding_side</span> <span class="key">in</span> <span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t683" href="#t683">683</a></span><span class="t">            <span class="str">"right"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t684" href="#t684">684</a></span><span class="t">            <span class="str">"left"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t685" href="#t685">685</a></span><span class="t">        <span class="op">]</span><span class="op">,</span> <span class="str">f"padding_side must be 'right' or 'left', got {default_padding_side}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t686" href="#t686">686</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t687" href="#t687">687</a></span><span class="t">        <span class="com"># Use a tokenizer that is initialized with add_bos_token=True as the default tokenizer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t688" href="#t688">688</a></span><span class="t">        <span class="com"># Such a tokenizer should be set as the default tokenizer because the tokenization of some</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t689" href="#t689">689</a></span><span class="t">        <span class="com"># tokenizers like LlamaTokenizer are different when bos token is automatically/manually</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t690" href="#t690">690</a></span><span class="t">        <span class="com"># prepended, and add_bos_token cannot be dynamically controlled after initialization</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t691" href="#t691">691</a></span><span class="t">        <span class="com"># (https://github.com/huggingface/transformers/issues/25886).</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t692" href="#t692">692</a></span><span class="t">        <span class="nam">tokenizer_with_bos</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">get_tokenizer_with_bos</span><span class="op">(</span><span class="nam">tokenizer</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t693" href="#t693">693</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="op">=</span> <span class="nam">tokenizer_with_bos</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t694" href="#t694">694</a></span><span class="t">        <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t695" href="#t695">695</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">padding_side</span> <span class="op">=</span> <span class="nam">default_padding_side</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t696" href="#t696">696</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t697" href="#t697">697</a></span><span class="t">        <span class="com"># Some tokenizers doesn't automatically prepend the BOS token even when they are initialized</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t698" href="#t698">698</a></span><span class="t">        <span class="com"># with add_bos_token=True. Therefore, we need this information to dynamically control prepend_bos.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t699" href="#t699">699</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_prepends_bos</span> <span class="op">=</span> <span class="nam">len</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">encode</span><span class="op">(</span><span class="str">""</span><span class="op">)</span><span class="op">)</span> <span class="op">></span> <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t700" href="#t700">700</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t701" href="#t701">701</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">701&#x202F;&#x219B;&#x202F;702</span><span class="annotate long">line 701 didn't jump to line 702, because the condition on line 701 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t702" href="#t702">702</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token</span> <span class="op">=</span> <span class="str">"&lt;|endoftext|>"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t703" href="#t703">703</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">pad_token</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t704" href="#t704">704</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">pad_token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t705" href="#t705">705</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">bos_token</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t706" href="#t706">706</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">bos_token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t707" href="#t707">707</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t708" href="#t708">708</a></span><span class="t">        <span class="com"># Infer vocab size from tokenizer</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t709" href="#t709">709</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t710" href="#t710">710</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab</span> <span class="op">=</span> <span class="nam">max</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">vocab</span><span class="op">.</span><span class="nam">values</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t711" href="#t711">711</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab_out</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t712" href="#t712">712</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab_out</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t713" href="#t713">713</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t714" href="#t714">714</a></span><span class="t">    <span class="key">def</span> <span class="nam">to_tokens</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t715" href="#t715">715</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t716" href="#t716">716</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t717" href="#t717">717</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t718" href="#t718">718</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t719" href="#t719">719</a></span><span class="t">        <span class="nam">move_to_device</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t720" href="#t720">720</a></span><span class="t">        <span class="nam">truncate</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t721" href="#t721">721</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t722" href="#t722">722</a></span><span class="t">        <span class="str">"""Converts a string to a tensor of tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t723" href="#t723">723</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t724" href="#t724">724</a></span><span class="t"><span class="str">        If prepend_bos is True, prepends the BOS token to the input - this is recommended when</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t725" href="#t725">725</a></span><span class="t"><span class="str">        creating a sequence of tokens to be input to a model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t726" href="#t726">726</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t727" href="#t727">727</a></span><span class="t"><span class="str">        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t728" href="#t728">728</a></span><span class="t"><span class="str">        inputting a prompt to the model as the first token is often treated weirdly, but should only</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t729" href="#t729">729</a></span><span class="t"><span class="str">        be done at the START of the prompt. Make sure to turn it off if you're looking at the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t730" href="#t730">730</a></span><span class="t"><span class="str">        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t731" href="#t731">731</a></span><span class="t"><span class="str">        token, others (OPT and my models) were)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t732" href="#t732">732</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t733" href="#t733">733</a></span><span class="t"><span class="str">        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t734" href="#t734">734</a></span><span class="t"><span class="str">        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t735" href="#t735">735</a></span><span class="t"><span class="str">        careful!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t736" href="#t736">736</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t737" href="#t737">737</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t738" href="#t738">738</a></span><span class="t"><span class="str">            input (Union[str, List[str]]): The input to tokenize.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t739" href="#t739">739</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t740" href="#t740">740</a></span><span class="t"><span class="str">                the BOS token to the input (only applies when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t741" href="#t741">741</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos which is set to True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t742" href="#t742">742</a></span><span class="t"><span class="str">                otherwise. Pass True or False to locally override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t743" href="#t743">743</a></span><span class="t"><span class="str">            padding_side (Union[Literal["left", "right"], None], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t744" href="#t744">744</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t745" href="#t745">745</a></span><span class="t"><span class="str">                multiple strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t746" href="#t746">746</a></span><span class="t"><span class="str">            move_to_device (bool): Whether to move the output tensor of tokens to the device the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t747" href="#t747">747</a></span><span class="t"><span class="str">                model lives on. Defaults to True truncate (bool): If the output tokens are too long,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t748" href="#t748">748</a></span><span class="t"><span class="str">                whether to truncate the output tokens to the model's max context window. Does nothing</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t749" href="#t749">749</a></span><span class="t"><span class="str">                for shorter inputs. Defaults to True.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t750" href="#t750">750</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t751" href="#t751">751</a></span><span class="t">        <span class="key">with</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">LocallyOverridenDefaults</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t752" href="#t752">752</a></span><span class="t">            <span class="nam">self</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t753" href="#t753">753</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t754" href="#t754">754</a></span><span class="t">            <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">,</span> <span class="str">"Cannot use to_tokens without a tokenizer"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t755" href="#t755">755</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t756" href="#t756">756</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_prepends_bos</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t757" href="#t757">757</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">"Set the tokenizer for the model by calling set_tokenizer"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t758" href="#t758">758</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t759" href="#t759">759</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">default_prepend_bos</span> <span class="key">and</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_prepends_bos</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t760" href="#t760">760</a></span><span class="t">                <span class="com"># We want to prepend bos but the tokenizer doesn't automatically do it, so we add it manually</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t761" href="#t761">761</a></span><span class="t">                <span class="nam">input</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">get_input_with_manually_prepended_bos</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">,</span> <span class="nam">input</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t762" href="#t762">762</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t763" href="#t763">763</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t764" href="#t764">764</a></span><span class="t">                <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t765" href="#t765">765</a></span><span class="t">                <span class="nam">return_tensors</span><span class="op">=</span><span class="str">"pt"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t766" href="#t766">766</a></span><span class="t">                <span class="nam">padding</span><span class="op">=</span><span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t767" href="#t767">767</a></span><span class="t">                <span class="nam">truncation</span><span class="op">=</span><span class="nam">truncate</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t768" href="#t768">768</a></span><span class="t">                <span class="nam">max_length</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_ctx</span> <span class="key">if</span> <span class="nam">truncate</span> <span class="key">else</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t769" href="#t769">769</a></span><span class="t">            <span class="op">)</span><span class="op">[</span><span class="str">"input_ids"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t770" href="#t770">770</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t771" href="#t771">771</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">default_prepend_bos</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">tokenizer_prepends_bos</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t772" href="#t772">772</a></span><span class="t">                <span class="com"># We don't want to prepend bos but the tokenizer does it automatically, so we remove it manually</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t773" href="#t773">773</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">get_tokens_with_bos_removed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t774" href="#t774">774</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t775" href="#t775">775</a></span><span class="t">            <span class="key">if</span> <span class="nam">move_to_device</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t776" href="#t776">776</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t777" href="#t777">777</a></span><span class="t">            <span class="key">return</span> <span class="nam">tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t778" href="#t778">778</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t779" href="#t779">779</a></span><span class="t">    <span class="key">def</span> <span class="nam">to_string</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t780" href="#t780">780</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t781" href="#t781">781</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t782" href="#t782">782</a></span><span class="t">            <span class="nam">List</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t783" href="#t783">783</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">""</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t784" href="#t784">784</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t785" href="#t785">785</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t786" href="#t786">786</a></span><span class="t">            <span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t787" href="#t787">787</a></span><span class="t">            <span class="nam">List</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t788" href="#t788">788</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t789" href="#t789">789</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t790" href="#t790">790</a></span><span class="t">        <span class="str">"""Tokens to String(s).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t791" href="#t791">791</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t792" href="#t792">792</a></span><span class="t"><span class="str">        Converts a tensor of tokens to a string (if rank 1) or a list of strings (if rank 2).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t793" href="#t793">793</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t794" href="#t794">794</a></span><span class="t"><span class="str">        Accepts lists of tokens and numpy arrays as inputs too (and converts to tensors internally)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t795" href="#t795">795</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t796" href="#t796">796</a></span><span class="t">        <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">,</span> <span class="str">"Cannot use to_string without a tokenizer"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t797" href="#t797">797</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t798" href="#t798">798</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t799" href="#t799">799</a></span><span class="t">            <span class="com"># We allow lists to be input</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t800" href="#t800">800</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t801" href="#t801">801</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t802" href="#t802">802</a></span><span class="t">        <span class="com"># I'm not sure what exactly clean_up_tokenization_spaces does, but if</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t803" href="#t803">803</a></span><span class="t">        <span class="com"># it's set, then tokenization is no longer invertible, and some tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t804" href="#t804">804</a></span><span class="t">        <span class="com"># with a bunch of whitespace get collapsed together</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t805" href="#t805">805</a></span><span class="t">        <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">==</span> <span class="num">2</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t806" href="#t806">806</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">batch_decode</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">clean_up_tokenization_spaces</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t807" href="#t807">807</a></span><span class="t">        <span class="key">elif</span> <span class="nam">len</span><span class="op">(</span><span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">807&#x202F;&#x219B;&#x202F;810</span><span class="annotate long">line 807 didn't jump to line 810, because the condition on line 807 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t808" href="#t808">808</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">decode</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">clean_up_tokenization_spaces</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t809" href="#t809">809</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t810" href="#t810">810</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"Invalid shape passed in: {tokens.shape}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t811" href="#t811">811</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t812" href="#t812">812</a></span><span class="t">    <span class="key">def</span> <span class="nam">to_str_tokens</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t813" href="#t813">813</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t814" href="#t814">814</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t815" href="#t815">815</a></span><span class="t">            <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t816" href="#t816">816</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t817" href="#t817">817</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"1 pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t818" href="#t818">818</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t819" href="#t819">819</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span><span class="op">,</span> <span class="str">"1 pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t820" href="#t820">820</a></span><span class="t">            <span class="nam">list</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t821" href="#t821">821</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t822" href="#t822">822</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t823" href="#t823">823</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t824" href="#t824">824</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span><span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t825" href="#t825">825</a></span><span class="t">        <span class="str">"""Map text, a list of text or tokens to a list of tokens as strings.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t826" href="#t826">826</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t827" href="#t827">827</a></span><span class="t"><span class="str">        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t828" href="#t828">828</a></span><span class="t"><span class="str">        inputting a prompt to the model as the first token is often treated weirdly, but should only</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t829" href="#t829">829</a></span><span class="t"><span class="str">        be done at the START of the prompt. If prepend_bos=None is passed, it implies the usage of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t830" href="#t830">830</a></span><span class="t"><span class="str">        self.cfg.default_prepend_bos which is set to True unless specified otherwise. Therefore,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t831" href="#t831">831</a></span><span class="t"><span class="str">        make sure to locally turn it off by passing prepend_bos=False if you're looking at the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t832" href="#t832">832</a></span><span class="t"><span class="str">        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t833" href="#t833">833</a></span><span class="t"><span class="str">        token, others (OPT and my models) were)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t834" href="#t834">834</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t835" href="#t835">835</a></span><span class="t"><span class="str">        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t836" href="#t836">836</a></span><span class="t"><span class="str">        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t837" href="#t837">837</a></span><span class="t"><span class="str">        careful!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t838" href="#t838">838</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t839" href="#t839">839</a></span><span class="t"><span class="str">        Gotcha3: If passing a string that exceeds the model's context length (model.cfg.n_ctx), it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t840" href="#t840">840</a></span><span class="t"><span class="str">        will be truncated.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t841" href="#t841">841</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t842" href="#t842">842</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t843" href="#t843">843</a></span><span class="t"><span class="str">            input (Union[str, list, torch.Tensor]): The input - either a string or a tensor of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t844" href="#t844">844</a></span><span class="t"><span class="str">                tokens. If tokens, should be a tensor of shape [pos] or [1, pos].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t845" href="#t845">845</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t846" href="#t846">846</a></span><span class="t"><span class="str">                the BOS token to the input (only applies when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t847" href="#t847">847</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos which is set to True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t848" href="#t848">848</a></span><span class="t"><span class="str">                otherwise. Pass True or False to locally override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t849" href="#t849">849</a></span><span class="t"><span class="str">            padding_side (Union[Literal["left", "right"], None], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t850" href="#t850">850</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t851" href="#t851">851</a></span><span class="t"><span class="str">                strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t852" href="#t852">852</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t853" href="#t853">853</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t854" href="#t854">854</a></span><span class="t"><span class="str">            str_tokens: List of individual tokens as strings</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t855" href="#t855">855</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t856" href="#t856">856</a></span><span class="t">        <span class="key">with</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">LocallyOverridenDefaults</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t857" href="#t857">857</a></span><span class="t">            <span class="nam">self</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t858" href="#t858">858</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t859" href="#t859">859</a></span><span class="t">            <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t860" href="#t860">860</a></span><span class="t">            <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t861" href="#t861">861</a></span><span class="t">            <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">list</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t862" href="#t862">862</a></span><span class="t">                <span class="key">return</span> <span class="nam">list</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t863" href="#t863">863</a></span><span class="t">                    <span class="nam">map</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t864" href="#t864">864</a></span><span class="t">                        <span class="key">lambda</span> <span class="nam">tokens</span><span class="op">:</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_str_tokens</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t865" href="#t865">865</a></span><span class="t">                        <span class="nam">input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t866" href="#t866">866</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t867" href="#t867">867</a></span><span class="t">                <span class="op">)</span>  <span class="com"># type: ignore</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t868" href="#t868">868</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t869" href="#t869">869</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">)</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t870" href="#t870">870</a></span><span class="t">                    <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t871" href="#t871">871</a></span><span class="t">                <span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t872" href="#t872">872</a></span><span class="t">                <span class="com"># Gemma tokenizer expects a batch dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t873" href="#t873">873</a></span><span class="t">                <span class="key">if</span> <span class="str">"gemma"</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">name_or_path</span> <span class="key">and</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">ndim</span> <span class="op">==</span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">873&#x202F;&#x219B;&#x202F;874</span><span class="annotate long">line 873 didn't jump to line 874, because the condition on line 873 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t874" href="#t874">874</a></span><span class="t">                    <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">(</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t875" href="#t875">875</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t876" href="#t876">876</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t877" href="#t877">877</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">squeeze</span><span class="op">(</span><span class="op">)</span>  <span class="com"># Get rid of a trivial batch dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t878" href="#t878">878</a></span><span class="t">                <span class="key">if</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">dim</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t879" href="#t879">879</a></span><span class="t">                    <span class="com"># Don't pass dimensionless tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t880" href="#t880">880</a></span><span class="t">                    <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">(</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t881" href="#t881">881</a></span><span class="t">                <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t882" href="#t882">882</a></span><span class="t">                    <span class="nam">tokens</span><span class="op">.</span><span class="nam">dim</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t883" href="#t883">883</a></span><span class="t">                <span class="op">)</span><span class="op">,</span> <span class="str">f"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t884" href="#t884">884</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">884&#x202F;&#x219B;&#x202F;894</span><span class="annotate long">line 884 didn't jump to line 894, because the condition on line 884 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t885" href="#t885">885</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t886" href="#t886">886</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">squeeze</span><span class="op">(</span><span class="op">)</span>  <span class="com"># Get rid of a trivial batch dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t887" href="#t887">887</a></span><span class="t">                <span class="key">if</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">ndim</span> <span class="op">==</span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t888" href="#t888">888</a></span><span class="t">                    <span class="com"># Don't pass dimensionless tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t889" href="#t889">889</a></span><span class="t">                    <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">np</span><span class="op">.</span><span class="nam">expand_dims</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">axis</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t890" href="#t890">890</a></span><span class="t">                <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t891" href="#t891">891</a></span><span class="t">                    <span class="nam">tokens</span><span class="op">.</span><span class="nam">ndim</span> <span class="op">==</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t892" href="#t892">892</a></span><span class="t">                <span class="op">)</span><span class="op">,</span> <span class="str">f"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t893" href="#t893">893</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t894" href="#t894">894</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"Invalid input type to to_str_tokens: {type(input)}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t895" href="#t895">895</a></span><span class="t">            <span class="nam">str_tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">batch_decode</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">clean_up_tokenization_spaces</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t896" href="#t896">896</a></span><span class="t">            <span class="key">return</span> <span class="nam">str_tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t897" href="#t897">897</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t898" href="#t898">898</a></span><span class="t">    <span class="key">def</span> <span class="nam">to_single_token</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">string</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t899" href="#t899">899</a></span><span class="t">        <span class="str">"""Map a string that makes up a single token to the id for that token.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t900" href="#t900">900</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t901" href="#t901">901</a></span><span class="t"><span class="str">        Raises an error for strings that are not a single token! If uncertain use to_tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t902" href="#t902">902</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t903" href="#t903">903</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t904" href="#t904">904</a></span><span class="t">        <span class="com"># We use the to_tokens method, do not append a BOS token</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t905" href="#t905">905</a></span><span class="t">        <span class="nam">token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span><span class="nam">string</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="key">False</span><span class="op">)</span><span class="op">.</span><span class="nam">squeeze</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t906" href="#t906">906</a></span><span class="t">        <span class="com"># If token shape is non-empty, raise error</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t907" href="#t907">907</a></span><span class="t">        <span class="key">assert</span> <span class="key">not</span> <span class="nam">token</span><span class="op">.</span><span class="nam">shape</span><span class="op">,</span> <span class="str">f"Input string: {string} is not a single token!"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t908" href="#t908">908</a></span><span class="t">        <span class="key">return</span> <span class="nam">token</span><span class="op">.</span><span class="nam">item</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t909" href="#t909">909</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t910" href="#t910">910</a></span><span class="t">    <span class="key">def</span> <span class="nam">to_single_str_token</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">int_token</span><span class="op">:</span> <span class="nam">int</span><span class="op">)</span> <span class="op">-></span> <span class="nam">str</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t911" href="#t911">911</a></span><span class="t">        <span class="com"># Gives the single token corresponding to an int in string form</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t912" href="#t912">912</a></span><span class="t">        <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">int_token</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t913" href="#t913">913</a></span><span class="t">        <span class="nam">token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_str_tokens</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="op">[</span><span class="nam">int_token</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t914" href="#t914">914</a></span><span class="t">        <span class="key">assert</span> <span class="nam">len</span><span class="op">(</span><span class="nam">token</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t915" href="#t915">915</a></span><span class="t">        <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">str</span><span class="op">,</span> <span class="nam">token</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t916" href="#t916">916</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t917" href="#t917">917</a></span><span class="t">    <span class="key">def</span> <span class="nam">get_token_position</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t918" href="#t918">918</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t919" href="#t919">919</a></span><span class="t">        <span class="nam">single_token</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">int</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t920" href="#t920">920</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">,</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"1 pos"</span><span class="op">]</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t921" href="#t921">921</a></span><span class="t">        <span class="nam">mode</span><span class="op">=</span><span class="str">"first"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t922" href="#t922">922</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t923" href="#t923">923</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t924" href="#t924">924</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t925" href="#t925">925</a></span><span class="t">        <span class="str">"""Get the position of a single_token in a string or sequence of tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t926" href="#t926">926</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t927" href="#t927">927</a></span><span class="t"><span class="str">        Raises an error if the token is not present.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t928" href="#t928">928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t929" href="#t929">929</a></span><span class="t"><span class="str">        Gotcha: If you're inputting a string, it'll automatically be tokenized. Be careful about the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t930" href="#t930">930</a></span><span class="t"><span class="str">        setting for prepend_bos! When a string is input to the model, a BOS (beginning of sequence)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t931" href="#t931">931</a></span><span class="t"><span class="str">        token is prepended by default when the string is tokenized because</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t932" href="#t932">932</a></span><span class="t"><span class="str">        self.cfg.default_prepend_bos is set to True unless specified otherwise. But this should only</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t933" href="#t933">933</a></span><span class="t"><span class="str">        be done at the START of the input, not when inputting part of the prompt. If you're getting</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t934" href="#t934">934</a></span><span class="t"><span class="str">        weird off-by-one errors, check carefully for what the setting should be!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t935" href="#t935">935</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t936" href="#t936">936</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t937" href="#t937">937</a></span><span class="t"><span class="str">            single_token (Union[str, int]): The token to search for. Can</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t938" href="#t938">938</a></span><span class="t"><span class="str">                be a token index, or a string (but the string must correspond to a single token).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t939" href="#t939">939</a></span><span class="t"><span class="str">            input (Union[str, torch.Tensor]): The sequence to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t940" href="#t940">940</a></span><span class="t"><span class="str">                search in. Can be a string or a rank 1 tensor of tokens or a rank 2 tensor of tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t941" href="#t941">941</a></span><span class="t"><span class="str">                with a dummy batch dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t942" href="#t942">942</a></span><span class="t"><span class="str">            mode (str, optional): If there are multiple matches, which match to return. Supports</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t943" href="#t943">943</a></span><span class="t"><span class="str">                "first" or "last". Defaults to "first".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t944" href="#t944">944</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t945" href="#t945">945</a></span><span class="t"><span class="str">                the BOS token to the input (only applies when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t946" href="#t946">946</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos which is set to True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t947" href="#t947">947</a></span><span class="t"><span class="str">                otherwise. Pass True or False to locally override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t948" href="#t948">948</a></span><span class="t"><span class="str">            padding_side (Union[Literal["left", "right"], None], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t949" href="#t949">949</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t950" href="#t950">950</a></span><span class="t"><span class="str">                strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t951" href="#t951">951</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t952" href="#t952">952</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t953" href="#t953">953</a></span><span class="t">            <span class="com"># If the input is a string, convert to tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t954" href="#t954">954</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t955" href="#t955">955</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t956" href="#t956">956</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t957" href="#t957">957</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t958" href="#t958">958</a></span><span class="t">        <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">==</span> <span class="num">2</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t959" href="#t959">959</a></span><span class="t">            <span class="com"># If the tokens have shape [1, seq_len], flatten to [seq_len]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t960" href="#t960">960</a></span><span class="t">            <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t961" href="#t961">961</a></span><span class="t">                <span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span> <span class="op">==</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t962" href="#t962">962</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">f"If tokens are rank two, they must have shape [1, seq_len], not {tokens.shape}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t963" href="#t963">963</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">[</span><span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t964" href="#t964">964</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t965" href="#t965">965</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">single_token</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t966" href="#t966">966</a></span><span class="t">            <span class="com"># If the single token is a string, convert to an integer</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t967" href="#t967">967</a></span><span class="t">            <span class="nam">single_token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_single_token</span><span class="op">(</span><span class="nam">single_token</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t968" href="#t968">968</a></span><span class="t">        <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">single_token</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">968&#x202F;&#x219B;&#x202F;969</span><span class="annotate long">line 968 didn't jump to line 969, because the condition on line 968 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t969" href="#t969">969</a></span><span class="t">            <span class="nam">single_token</span> <span class="op">=</span> <span class="nam">single_token</span><span class="op">.</span><span class="nam">item</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t970" href="#t970">970</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t971" href="#t971">971</a></span><span class="t">        <span class="nam">indices</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">len</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">tokens</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">[</span><span class="nam">tokens</span> <span class="op">==</span> <span class="nam">single_token</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t972" href="#t972">972</a></span><span class="t">        <span class="key">assert</span> <span class="nam">len</span><span class="op">(</span><span class="nam">indices</span><span class="op">)</span> <span class="op">></span> <span class="num">0</span><span class="op">,</span> <span class="str">"The token does not occur in the prompt"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t973" href="#t973">973</a></span><span class="t">        <span class="key">if</span> <span class="nam">mode</span> <span class="op">==</span> <span class="str">"first"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t974" href="#t974">974</a></span><span class="t">            <span class="key">return</span> <span class="nam">indices</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">item</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t975" href="#t975">975</a></span><span class="t">        <span class="key">elif</span> <span class="nam">mode</span> <span class="op">==</span> <span class="str">"last"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">975&#x202F;&#x219B;&#x202F;978</span><span class="annotate long">line 975 didn't jump to line 978, because the condition on line 975 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t976" href="#t976">976</a></span><span class="t">            <span class="key">return</span> <span class="nam">indices</span><span class="op">[</span><span class="op">-</span><span class="num">1</span><span class="op">]</span><span class="op">.</span><span class="nam">item</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t977" href="#t977">977</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t978" href="#t978">978</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"mode must be 'first' or 'last', not {mode}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t979" href="#t979">979</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t980" href="#t980">980</a></span><span class="t">    <span class="key">def</span> <span class="nam">tokens_to_residual_directions</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t981" href="#t981">981</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t982" href="#t982">982</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t983" href="#t983">983</a></span><span class="t">            <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t984" href="#t984">984</a></span><span class="t">            <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t985" href="#t985">985</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">""</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t986" href="#t986">986</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t987" href="#t987">987</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t988" href="#t988">988</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t989" href="#t989">989</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t990" href="#t990">990</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t991" href="#t991">991</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t992" href="#t992">992</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t993" href="#t993">993</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t994" href="#t994">994</a></span><span class="t">        <span class="str">"""Map tokens to a tensor with the unembedding vector for those tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t995" href="#t995">995</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t996" href="#t996">996</a></span><span class="t"><span class="str">        I.e. the vector in the residual stream that we dot with to the get the logit for that token.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t997" href="#t997">997</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t998" href="#t998">998</a></span><span class="t"><span class="str">        WARNING: If you use this without folding in LayerNorm, the results will be misleading and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t999" href="#t999">999</a></span><span class="t"><span class="str">        may be incorrect, as the LN weights change the unembed map. This is done automatically with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1000" href="#t1000">1000</a></span><span class="t"><span class="str">        the fold_ln flag on from_pretrained</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1001" href="#t1001">1001</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1002" href="#t1002">1002</a></span><span class="t"><span class="str">        WARNING 2: LayerNorm scaling will scale up or down the effective direction in the residual</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1003" href="#t1003">1003</a></span><span class="t"><span class="str">        stream for each output token on any given input token position.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1004" href="#t1004">1004</a></span><span class="t"><span class="str">        ActivationCache.apply_ln_to_stack will apply the appropriate scaling to these directions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1005" href="#t1005">1005</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1006" href="#t1006">1006</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1007" href="#t1007">1007</a></span><span class="t"><span class="str">            tokens (Union[str, int, torch.Tensor]): The token(s). If a single token, can be a single</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1008" href="#t1008">1008</a></span><span class="t"><span class="str">                element tensor, an integer, or string. If string, will be mapped to a single token</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1009" href="#t1009">1009</a></span><span class="t"><span class="str">                using to_single_token, and an error raised if it's multiple tokens. The method also</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1010" href="#t1010">1010</a></span><span class="t"><span class="str">                works for a batch of input tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1011" href="#t1011">1011</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1012" href="#t1012">1012</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1013" href="#t1013">1013</a></span><span class="t"><span class="str">            residual_direction torch.Tensor: The unembedding vector for the token(s), a stack of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1014" href="#t1014">1014</a></span><span class="t"><span class="str">                [d_model] tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1015" href="#t1015">1015</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1016" href="#t1016">1016</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">and</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">numel</span><span class="op">(</span><span class="op">)</span> <span class="op">></span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1017" href="#t1017">1017</a></span><span class="t">            <span class="com"># If the tokens are a tensor, and have more than one element, assume they are a batch of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1018" href="#t1018">1018</a></span><span class="t">            <span class="com"># tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1019" href="#t1019">1019</a></span><span class="t">            <span class="nam">residual_directions</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_U</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="nam">tokens</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1020" href="#t1020">1020</a></span><span class="t">            <span class="nam">residual_directions</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1021" href="#t1021">1021</a></span><span class="t">                <span class="nam">residual_directions</span><span class="op">,</span> <span class="str">"d_model ... -> ... d_model"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1022" href="#t1022">1022</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1023" href="#t1023">1023</a></span><span class="t">            <span class="key">return</span> <span class="nam">residual_directions</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1024" href="#t1024">1024</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1025" href="#t1025">1025</a></span><span class="t">            <span class="com"># Otherwise there is a single token</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1026" href="#t1026">1026</a></span><span class="t">            <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1026&#x202F;&#x219B;&#x202F;1027</span><span class="annotate long">line 1026 didn't jump to line 1027, because the condition on line 1026 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1027" href="#t1027">1027</a></span><span class="t">                <span class="nam">token</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_single_token</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1028" href="#t1028">1028</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1028&#x202F;&#x219B;&#x202F;1029</span><span class="annotate long">line 1028 didn't jump to line 1029, because the condition on line 1028 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1029" href="#t1029">1029</a></span><span class="t">                <span class="nam">token</span> <span class="op">=</span> <span class="nam">tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1030" href="#t1030">1030</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">and</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">numel</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1030&#x202F;&#x219B;&#x202F;1033</span><span class="annotate long">line 1030 didn't jump to line 1033, because the condition on line 1030 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1031" href="#t1031">1031</a></span><span class="t">                <span class="nam">token</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">item</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1032" href="#t1032">1032</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1033" href="#t1033">1033</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"Invalid token type: {type(tokens)}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1034" href="#t1034">1034</a></span><span class="t">            <span class="nam">residual_direction</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_U</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="nam">token</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1035" href="#t1035">1035</a></span><span class="t">            <span class="key">return</span> <span class="nam">residual_direction</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1036" href="#t1036">1036</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1037" href="#t1037">1037</a></span><span class="t">    <span class="key">def</span> <span class="nam">to</span><span class="op">(</span>  <span class="com"># type: ignore</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1038" href="#t1038">1038</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1039" href="#t1039">1039</a></span><span class="t">        <span class="nam">device_or_dtype</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span> <span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">dtype</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1040" href="#t1040">1040</a></span><span class="t">        <span class="nam">print_details</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1041" href="#t1041">1041</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1042" href="#t1042">1042</a></span><span class="t">        <span class="key">return</span> <span class="nam">devices</span><span class="op">.</span><span class="nam">move_to_and_update_config</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">device_or_dtype</span><span class="op">,</span> <span class="nam">print_details</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1043" href="#t1043">1043</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1044" href="#t1044">1044</a></span><span class="t">    <span class="key">def</span> <span class="nam">cuda</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1045" href="#t1045">1045</a></span><span class="t">        <span class="str">"""Wrapper around cuda that also changes `self.cfg.device`."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1046" href="#t1046">1046</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="str">"cuda"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1047" href="#t1047">1047</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1048" href="#t1048">1048</a></span><span class="t">    <span class="key">def</span> <span class="nam">cpu</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1049" href="#t1049">1049</a></span><span class="t">        <span class="str">"""Wrapper around cuda that also changes `self.cfg.device`."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1050" href="#t1050">1050</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="str">"cpu"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1051" href="#t1051">1051</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1052" href="#t1052">1052</a></span><span class="t">    <span class="key">def</span> <span class="nam">mps</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1053" href="#t1053">1053</a></span><span class="t">        <span class="str">"""Wrapper around mps that also changes `self.cfg.device`."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1054" href="#t1054">1054</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="str">"mps"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1055" href="#t1055">1055</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1056" href="#t1056">1056</a></span><span class="t">    <span class="key">def</span> <span class="nam">move_model_modules_to_device</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1057" href="#t1057">1057</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">embed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1058" href="#t1058">1058</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">hook_embed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1059" href="#t1059">1059</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">!=</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1060" href="#t1060">1060</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">pos_embed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1061" href="#t1061">1061</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">hook_pos_embed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1062" href="#t1062">1062</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1063" href="#t1063">1063</a></span><span class="t">        <span class="key">if</span> <span class="nam">hasattr</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="str">"ln_final"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1064" href="#t1064">1064</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span> <span class="op">-</span> <span class="num">1</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1065" href="#t1065">1065</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">unembed</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span> <span class="op">-</span> <span class="num">1</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1066" href="#t1066">1066</a></span><span class="t">        <span class="key">for</span> <span class="nam">i</span><span class="op">,</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">enumerate</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1067" href="#t1067">1067</a></span><span class="t">            <span class="nam">block</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="nam">i</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1068" href="#t1068">1068</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1069" href="#t1069">1069</a></span><span class="t">    <span class="op">@</span><span class="nam">classmethod</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1070" href="#t1070">1070</a></span><span class="t">    <span class="key">def</span> <span class="nam">from_pretrained</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1071" href="#t1071">1071</a></span><span class="t">        <span class="nam">cls</span><span class="op">:</span> <span class="nam">Type</span><span class="op">[</span><span class="nam">T</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1072" href="#t1072">1072</a></span><span class="t">        <span class="nam">model_name</span><span class="op">:</span> <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1073" href="#t1073">1073</a></span><span class="t">        <span class="nam">fold_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1074" href="#t1074">1074</a></span><span class="t">        <span class="nam">center_writing_weights</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1075" href="#t1075">1075</a></span><span class="t">        <span class="nam">center_unembed</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1076" href="#t1076">1076</a></span><span class="t">        <span class="nam">refactor_factored_attn_matrices</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1077" href="#t1077">1077</a></span><span class="t">        <span class="nam">checkpoint_index</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1078" href="#t1078">1078</a></span><span class="t">        <span class="nam">checkpoint_value</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1079" href="#t1079">1079</a></span><span class="t">        <span class="nam">hf_model</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">AutoModelForCausalLM</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1080" href="#t1080">1080</a></span><span class="t">        <span class="nam">device</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1081" href="#t1081">1081</a></span><span class="t">        <span class="nam">n_devices</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1082" href="#t1082">1082</a></span><span class="t">        <span class="nam">tokenizer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">PreTrainedTokenizerBase</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1083" href="#t1083">1083</a></span><span class="t">        <span class="nam">move_to_device</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1084" href="#t1084">1084</a></span><span class="t">        <span class="nam">fold_value_biases</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1085" href="#t1085">1085</a></span><span class="t">        <span class="nam">default_prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">bool</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1086" href="#t1086">1086</a></span><span class="t">        <span class="nam">default_padding_side</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span> <span class="op">=</span> <span class="str">"right"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1087" href="#t1087">1087</a></span><span class="t">        <span class="nam">dtype</span><span class="op">=</span><span class="str">"float32"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1088" href="#t1088">1088</a></span><span class="t">        <span class="nam">first_n_layers</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1089" href="#t1089">1089</a></span><span class="t">        <span class="op">**</span><span class="nam">from_pretrained_kwargs</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1090" href="#t1090">1090</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">T</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1091" href="#t1091">1091</a></span><span class="t">        <span class="str">"""Load in a Pretrained Model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1092" href="#t1092">1092</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1093" href="#t1093">1093</a></span><span class="t"><span class="str">        Load in pretrained model weights to the HookedTransformer format and optionally to do some</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1094" href="#t1094">1094</a></span><span class="t"><span class="str">        processing to make the model easier to interpret. Currently supports loading from most</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1095" href="#t1095">1095</a></span><span class="t"><span class="str">        autoregressive HuggingFace models (``gpt2``, ``neo``, ``gptj``, ``opt``...) and from a range</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1096" href="#t1096">1096</a></span><span class="t"><span class="str">        of toy models and SoLU models trained by Neel Nanda. The full list is available in the docs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1097" href="#t1097">1097</a></span><span class="t"><span class="str">        under :doc:`model properties&lt;/generated/model_properties_table>`. Also supports loading from</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1098" href="#t1098">1098</a></span><span class="t"><span class="str">        a checkpoint for checkpointed models (currently, models trained by NeelNanda and the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1099" href="#t1099">1099</a></span><span class="t"><span class="str">        stanford-crfm models (using parameters ``checkpoint_index`` and ``checkpoint_value``).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1100" href="#t1100">1100</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1101" href="#t1101">1101</a></span><span class="t"><span class="str">        See :meth:`load_and_process_state_dict` for details on the processing (folding layer norm,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1102" href="#t1102">1102</a></span><span class="t"><span class="str">        centering the unembedding and centering the writing weights).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1103" href="#t1103">1103</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1104" href="#t1104">1104</a></span><span class="t"><span class="str">        Example:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1105" href="#t1105">1105</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1106" href="#t1106">1106</a></span><span class="t"><span class="str">        >>> from transformer_lens import HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1107" href="#t1107">1107</a></span><span class="t"><span class="str">        >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1108" href="#t1108">1108</a></span><span class="t"><span class="str">        Loaded pretrained model tiny-stories-1M into HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1109" href="#t1109">1109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1110" href="#t1110">1110</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1111" href="#t1111">1111</a></span><span class="t"><span class="str">            model_name: The model name - must be an element of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1112" href="#t1112">1112</a></span><span class="t"><span class="str">                :const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES` or an alias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1113" href="#t1113">1113</a></span><span class="t"><span class="str">                of one. The full list of available models can be found in the docs under :doc:`model</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1114" href="#t1114">1114</a></span><span class="t"><span class="str">                properties&lt;/generated/model_properties_table>`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1115" href="#t1115">1115</a></span><span class="t"><span class="str">            fold_ln: Whether to fold in the LayerNorm weights to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1116" href="#t1116">1116</a></span><span class="t"><span class="str">                subsequent linear layer. This does not change the computation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1117" href="#t1117">1117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1118" href="#t1118">1118</a></span><span class="t"><span class="str">                `LayerNorm</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1119" href="#t1119">1119</a></span><span class="t"><span class="str">                &lt;https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1>`_</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1120" href="#t1120">1120</a></span><span class="t"><span class="str">                is a common regularization technique used in transformers. Unlike BatchNorm, it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1121" href="#t1121">1121</a></span><span class="t"><span class="str">                cannot be turned off at inference time, as it significantly alters the mathematical</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1122" href="#t1122">1122</a></span><span class="t"><span class="str">                function implemented by the transformer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1123" href="#t1123">1123</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1124" href="#t1124">1124</a></span><span class="t"><span class="str">                When `fold_ln` is set to True, LayerNorm (with weights :math:`w_{ln}` and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1125" href="#t1125">1125</a></span><span class="t"><span class="str">                :math:`b_{ln}`) followed by a linear layer (:math:`W + b`) is optimized to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1126" href="#t1126">1126</a></span><span class="t"><span class="str">                LayerNormPre (just centering &amp; normalizing) followed by a new linear layer with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1127" href="#t1127">1127</a></span><span class="t"><span class="str">                :math:`W_{eff} = w[:, \text{None}] * W` (element-wise multiplication) and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1128" href="#t1128">1128</a></span><span class="t"><span class="str">                :math:`b_{eff} = b + b_{ln} @ W`. This transformation is computationally equivalent</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1129" href="#t1129">1129</a></span><span class="t"><span class="str">                and simplifies the model's interpretability. It essentially merges LayerNorm weights</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1130" href="#t1130">1130</a></span><span class="t"><span class="str">                into the subsequent linear layer's weights, which is handled by HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1131" href="#t1131">1131</a></span><span class="t"><span class="str">                when loading pre-trained weights. Set `fold_ln` to False when loading a state dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1132" href="#t1132">1132</a></span><span class="t"><span class="str">                if you wish to turn this off.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1133" href="#t1133">1133</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1134" href="#t1134">1134</a></span><span class="t"><span class="str">                Mathematically, LayerNorm is defined as follows:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1135" href="#t1135">1135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1136" href="#t1136">1136</a></span><span class="t"><span class="str">                .. math::</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1137" href="#t1137">1137</a></span><span class="t"><span class="str">                    x_1 &amp;= x_0 - \\text{mean}(x_0)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1138" href="#t1138">1138</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1139" href="#t1139">1139</a></span><span class="t"><span class="str">                    x_2 &amp;= \\frac{x_1}{\\sqrt{\\text{mean}(x_1^2)}}</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1140" href="#t1140">1140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1141" href="#t1141">1141</a></span><span class="t"><span class="str">                    x_3 &amp;= x_2 \\cdot w</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1142" href="#t1142">1142</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1143" href="#t1143">1143</a></span><span class="t"><span class="str">                    x_4 &amp;= x_3 + b</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1144" href="#t1144">1144</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1145" href="#t1145">1145</a></span><span class="t"><span class="str">                For further details, refer to `this document</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1146" href="#t1146">1146</a></span><span class="t"><span class="str">                &lt;https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization>`_.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1147" href="#t1147">1147</a></span><span class="t"><span class="str">            center_writing_weights: Whether to center weights</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1148" href="#t1148">1148</a></span><span class="t"><span class="str">                writing to the residual stream (ie set mean to be zero). Due to LayerNorm this</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1149" href="#t1149">1149</a></span><span class="t"><span class="str">                doesn't change the computation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1150" href="#t1150">1150</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1151" href="#t1151">1151</a></span><span class="t"><span class="str">                A related idea to folding layernorm (``fold_ln``) - *every* component reading an</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1152" href="#t1152">1152</a></span><span class="t"><span class="str">                input from the residual stream is preceded by a LayerNorm, which means that the mean</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1153" href="#t1153">1153</a></span><span class="t"><span class="str">                of a residual stream vector (ie the component in the direction of all ones) never</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1154" href="#t1154">1154</a></span><span class="t"><span class="str">                matters. This means we can remove the all ones component of weights and biases whose</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1155" href="#t1155">1155</a></span><span class="t"><span class="str">                output *writes* to the residual stream. Mathematically, ``W_writing -=</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1156" href="#t1156">1156</a></span><span class="t"><span class="str">                W_writing.mean(dim=1, keepdim=True)``.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1157" href="#t1157">1157</a></span><span class="t"><span class="str">            center_unembed: Whether to center W_U (ie set mean</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1158" href="#t1158">1158</a></span><span class="t"><span class="str">                to be zero). Softmax is translation invariant so this doesn't affect log probs or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1159" href="#t1159">1159</a></span><span class="t"><span class="str">                loss, but does change logits.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1160" href="#t1160">1160</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1161" href="#t1161">1161</a></span><span class="t"><span class="str">                The logits are fed into a softmax. Softmax is translation invariant (eg, adding 1 to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1162" href="#t1162">1162</a></span><span class="t"><span class="str">                every logit doesn't change the output), so we can simplify things by setting the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1163" href="#t1163">1163</a></span><span class="t"><span class="str">                mean of the logits to be zero. This is equivalent to setting the mean of every</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1164" href="#t1164">1164</a></span><span class="t"><span class="str">                output vector of ``W_U`` to zero. In code, ``W_U -= W_U.mean(dim=-1,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1165" href="#t1165">1165</a></span><span class="t"><span class="str">                keepdim=True)``.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1166" href="#t1166">1166</a></span><span class="t"><span class="str">            refactor_factored_attn_matrices: Whether to convert the factored</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1167" href="#t1167">1167</a></span><span class="t"><span class="str">                matrices (W_Q &amp; W_K, and W_O &amp; W_V) to be "even". Defaults to False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1168" href="#t1168">1168</a></span><span class="t"><span class="str">            checkpoint_index: If loading from a checkpoint, the index of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1169" href="#t1169">1169</a></span><span class="t"><span class="str">                the checkpoint to load.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1170" href="#t1170">1170</a></span><span class="t"><span class="str">            checkpoint_value: If loading from a checkpoint, the value of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1171" href="#t1171">1171</a></span><span class="t"><span class="str">                the checkpoint to load, ie the step or token number (each model has checkpoints</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1172" href="#t1172">1172</a></span><span class="t"><span class="str">                labelled with exactly one of these). E.g. ``1000`` for a checkpoint taken at step</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1173" href="#t1173">1173</a></span><span class="t"><span class="str">                1000 or after 1000 tokens. If `checkpoint_index` is also specified, this will be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1174" href="#t1174">1174</a></span><span class="t"><span class="str">                ignored.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1175" href="#t1175">1175</a></span><span class="t"><span class="str">            hf_model: If you have already loaded in the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1176" href="#t1176">1176</a></span><span class="t"><span class="str">                HuggingFace model, you can pass it in here rather than needing to recreate the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1177" href="#t1177">1177</a></span><span class="t"><span class="str">                object. Defaults to None.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1178" href="#t1178">1178</a></span><span class="t"><span class="str">            device: The device to load the model onto. By</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1179" href="#t1179">1179</a></span><span class="t"><span class="str">                default will load to CUDA if available, else CPU.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1180" href="#t1180">1180</a></span><span class="t"><span class="str">            n_devices: The number of devices to split the model</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1181" href="#t1181">1181</a></span><span class="t"><span class="str">                across. Defaults to 1. If greater than 1, `device` must be cuda.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1182" href="#t1182">1182</a></span><span class="t"><span class="str">            tokenizer: The tokenizer to use for the model. If not</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1183" href="#t1183">1183</a></span><span class="t"><span class="str">                provided, it is inferred from cfg.tokenizer_name or initialized to None. If None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1184" href="#t1184">1184</a></span><span class="t"><span class="str">                then the model cannot be passed strings, and d_vocab must be explicitly set.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1185" href="#t1185">1185</a></span><span class="t"><span class="str">            move_to_device: Whether to move the model to the device specified in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1186" href="#t1186">1186</a></span><span class="t"><span class="str">                cfg. device. Must be true if `n_devices` in the config is greater than 1, since the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1187" href="#t1187">1187</a></span><span class="t"><span class="str">                model's layers will be split across multiple devices.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1188" href="#t1188">1188</a></span><span class="t"><span class="str">            fold_value_biases: Each attention head has a value bias. Values are averaged to create</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1189" href="#t1189">1189</a></span><span class="t"><span class="str">                mixed values (``z``), weighted by the attention pattern, but as the bias is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1190" href="#t1190">1190</a></span><span class="t"><span class="str">                constant, its contribution to ``z`` is exactly the same. The output of a head is ``z</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1191" href="#t1191">1191</a></span><span class="t"><span class="str">                @ W_O``, and so the value bias just linearly adds to the output of the head. This</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1192" href="#t1192">1192</a></span><span class="t"><span class="str">                means that the value bias of a head has nothing to do with the head, and is just a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1193" href="#t1193">1193</a></span><span class="t"><span class="str">                constant added to the attention layer outputs. We can take the sum across these and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1194" href="#t1194">1194</a></span><span class="t"><span class="str">                b_O to get an "effective bias" for the layer. In code, we set ``b_V=0``. and ``b_O =</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1195" href="#t1195">1195</a></span><span class="t"><span class="str">                (b_V @ W_O).sum(dim=0) + b_O``.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1196" href="#t1196">1196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1197" href="#t1197">1197</a></span><span class="t"><span class="str">                The technical derivation of this is as follows. ``v = residual @ W_V[h] +</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1198" href="#t1198">1198</a></span><span class="t"><span class="str">                broadcast_b_V[h]`` for each head ``h`` (where ``b_V`` is broadcast up from shape</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1199" href="#t1199">1199</a></span><span class="t"><span class="str">                ``d_head`` to shape ``[position, d_head]``). And ``z = pattern[h] @ v = pattern[h] @</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1200" href="#t1200">1200</a></span><span class="t"><span class="str">                residual @ W_V[h] + pattern[h] @ broadcast_b_V[h]``. Because ``pattern[h]`` is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1201" href="#t1201">1201</a></span><span class="t"><span class="str">                ``[destination_position, source_position]`` and ``broadcast_b_V`` is constant along</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1202" href="#t1202">1202</a></span><span class="t"><span class="str">                the ``(source_)position`` dimension, we're basically just multiplying it by the sum</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1203" href="#t1203">1203</a></span><span class="t"><span class="str">                of the pattern across the ``source_position`` dimension, which is just ``1``. So it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1204" href="#t1204">1204</a></span><span class="t"><span class="str">                remains exactly the same, and so is just broadcast across the destination positions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1205" href="#t1205">1205</a></span><span class="t"><span class="str">            default_prepend_bos: Default behavior of whether to prepend the BOS</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1206" href="#t1206">1206</a></span><span class="t"><span class="str">                token when the methods of HookedTransformer process input text to tokenize (only</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1207" href="#t1207">1207</a></span><span class="t"><span class="str">                when input is a string).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1208" href="#t1208">1208</a></span><span class="t"><span class="str">                Resolution order for default_prepend_bos:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1209" href="#t1209">1209</a></span><span class="t"><span class="str">                1. If user passes value explicitly, use that value</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1210" href="#t1210">1210</a></span><span class="t"><span class="str">                2. Model-specific default from cfg_dict if it exists (e.g. for bloom models it's False)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1211" href="#t1211">1211</a></span><span class="t"><span class="str">                3. Global default (True)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1212" href="#t1212">1212</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1213" href="#t1213">1213</a></span><span class="t"><span class="str">                Even for models not explicitly trained with the BOS token, heads often use the first position as a resting position</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1214" href="#t1214">1214</a></span><span class="t"><span class="str">                and accordingly lose information from the first token, so this empirically seems to give better</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1215" href="#t1215">1215</a></span><span class="t"><span class="str">                results. Note that you can also locally override the default behavior by passing in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1216" href="#t1216">1216</a></span><span class="t"><span class="str">                prepend_bos=True/False when you call a method that processes the input string.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1217" href="#t1217">1217</a></span><span class="t"><span class="str">            from_pretrained_kwargs: Any other optional argument passed to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1218" href="#t1218">1218</a></span><span class="t"><span class="str">                HuggingFace's from_pretrained (e.g. "cache_dir" or "torch_dtype"). Also passed to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1219" href="#t1219">1219</a></span><span class="t"><span class="str">                other HuggingFace functions when compatible. For some models or arguments it doesn't</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1220" href="#t1220">1220</a></span><span class="t"><span class="str">                work, especially for models that are not internally loaded with HuggingFace's</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1221" href="#t1221">1221</a></span><span class="t"><span class="str">                from_pretrained (e.g. SoLU models).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1222" href="#t1222">1222</a></span><span class="t"><span class="str">            dtype: What data type to load the model in (also sets the dtype of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1223" href="#t1223">1223</a></span><span class="t"><span class="str">                the HuggingFace model). Set to bfloat16 or float16 if you get out of memory errors when loading</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1224" href="#t1224">1224</a></span><span class="t"><span class="str">                the model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1225" href="#t1225">1225</a></span><span class="t"><span class="str">            default_padding_side: Which side to pad on when tokenizing. Defaults to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1226" href="#t1226">1226</a></span><span class="t"><span class="str">                "right".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1227" href="#t1227">1227</a></span><span class="t"><span class="str">            first_n_layers: If specified, only load the first n layers of the model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1228" href="#t1228">1228</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1229" href="#t1229">1229</a></span><span class="t">        <span class="key">if</span> <span class="nam">model_name</span><span class="op">.</span><span class="nam">lower</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">startswith</span><span class="op">(</span><span class="str">"t5"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1229&#x202F;&#x219B;&#x202F;1230</span><span class="annotate long">line 1229 didn't jump to line 1230, because the condition on line 1229 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1230" href="#t1230">1230</a></span><span class="t">            <span class="key">raise</span> <span class="nam">RuntimeError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1231" href="#t1231">1231</a></span><span class="t">                <span class="str">"Execution stopped: Please use HookedEncoderDecoder to load T5 models instead of HookedTransformer."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1232" href="#t1232">1232</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1233" href="#t1233">1233</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1234" href="#t1234">1234</a></span><span class="t">        <span class="key">assert</span> <span class="key">not</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1235" href="#t1235">1235</a></span><span class="t">            <span class="nam">from_pretrained_kwargs</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"load_in_8bit"</span><span class="op">,</span> <span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1236" href="#t1236">1236</a></span><span class="t">            <span class="key">or</span> <span class="nam">from_pretrained_kwargs</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"load_in_4bit"</span><span class="op">,</span> <span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1237" href="#t1237">1237</a></span><span class="t">        <span class="op">)</span><span class="op">,</span> <span class="str">"Quantization not supported"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1238" href="#t1238">1238</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1239" href="#t1239">1239</a></span><span class="t">        <span class="key">if</span> <span class="nam">hf_model</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1239&#x202F;&#x219B;&#x202F;1240</span><span class="annotate long">line 1239 didn't jump to line 1240, because the condition on line 1239 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1240" href="#t1240">1240</a></span><span class="t">            <span class="nam">hf_cfg</span> <span class="op">=</span> <span class="nam">hf_model</span><span class="op">.</span><span class="nam">config</span><span class="op">.</span><span class="nam">to_dict</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1241" href="#t1241">1241</a></span><span class="t">            <span class="nam">qc</span> <span class="op">=</span> <span class="nam">hf_cfg</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"quantization_config"</span><span class="op">,</span> <span class="op">{</span><span class="op">}</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1242" href="#t1242">1242</a></span><span class="t">            <span class="nam">load_in_4bit</span> <span class="op">=</span> <span class="nam">qc</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"load_in_4bit"</span><span class="op">,</span> <span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1243" href="#t1243">1243</a></span><span class="t">            <span class="nam">load_in_8bit</span> <span class="op">=</span> <span class="nam">qc</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"load_in_8bit"</span><span class="op">,</span> <span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1244" href="#t1244">1244</a></span><span class="t">            <span class="nam">quant_method</span> <span class="op">=</span> <span class="nam">qc</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"quant_method"</span><span class="op">,</span> <span class="str">""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1245" href="#t1245">1245</a></span><span class="t">            <span class="key">assert</span> <span class="key">not</span> <span class="nam">load_in_8bit</span><span class="op">,</span> <span class="str">"8-bit quantization is not supported"</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1246" href="#t1246">1246</a></span><span class="t">            <span class="key">assert</span> <span class="key">not</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1247" href="#t1247">1247</a></span><span class="t">                <span class="nam">load_in_4bit</span> <span class="key">and</span> <span class="op">(</span><span class="nam">version</span><span class="op">.</span><span class="nam">parse</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">__version__</span><span class="op">)</span> <span class="op">&lt;</span> <span class="nam">version</span><span class="op">.</span><span class="nam">parse</span><span class="op">(</span><span class="str">"2.1.1"</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1248" href="#t1248">1248</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">"Quantization is only supported for torch versions >= 2.1.1"</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1249" href="#t1249">1249</a></span><span class="t">            <span class="key">assert</span> <span class="key">not</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1250" href="#t1250">1250</a></span><span class="t">                <span class="nam">load_in_4bit</span> <span class="key">and</span> <span class="op">(</span><span class="str">"llama"</span> <span class="key">not</span> <span class="key">in</span> <span class="nam">model_name</span><span class="op">.</span><span class="nam">lower</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1251" href="#t1251">1251</a></span><span class="t">            <span class="op">)</span><span class="op">,</span> <span class="str">"Quantization is only supported for Llama models"</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1252" href="#t1252">1252</a></span><span class="t">            <span class="key">if</span> <span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1253" href="#t1253">1253</a></span><span class="t">                <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1254" href="#t1254">1254</a></span><span class="t">                    <span class="nam">qc</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"quant_method"</span><span class="op">,</span> <span class="str">""</span><span class="op">)</span> <span class="op">==</span> <span class="str">"bitsandbytes"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1255" href="#t1255">1255</a></span><span class="t">                <span class="op">)</span><span class="op">,</span> <span class="str">"Only bitsandbytes quantization is supported"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1256" href="#t1256">1256</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1257" href="#t1257">1257</a></span><span class="t">            <span class="nam">hf_cfg</span> <span class="op">=</span> <span class="op">{</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1258" href="#t1258">1258</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1259" href="#t1259">1259</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1260" href="#t1260">1260</a></span><span class="t">            <span class="com"># Convert from string to a torch dtype</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1261" href="#t1261">1261</a></span><span class="t">            <span class="nam">dtype</span> <span class="op">=</span> <span class="nam">DTYPE_FROM_STRING</span><span class="op">[</span><span class="nam">dtype</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1262" href="#t1262">1262</a></span><span class="t">        <span class="key">if</span> <span class="str">"torch_dtype"</span> <span class="key">in</span> <span class="nam">from_pretrained_kwargs</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1262&#x202F;&#x219B;&#x202F;1265</span><span class="annotate long">line 1262 didn't jump to line 1265, because the condition on line 1262 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t1263" href="#t1263">1263</a></span><span class="t">            <span class="com"># For backwards compatibility with the previous way to do low precision loading</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1264" href="#t1264">1264</a></span><span class="t">            <span class="com"># This should maybe check the user did not explicitly set dtype *and* torch_dtype</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1265" href="#t1265">1265</a></span><span class="t">            <span class="nam">dtype</span> <span class="op">=</span> <span class="nam">from_pretrained_kwargs</span><span class="op">[</span><span class="str">"torch_dtype"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1266" href="#t1266">1266</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1267" href="#t1267">1267</a></span><span class="t">        <span class="key">if</span> <span class="op">(</span>&nbsp;</span><span class="r"><span class="annotate short">1267&#x202F;&#x219B;&#x202F;1271</span><span class="annotate long">line 1267 didn't jump to line 1271, because the condition on line 1267 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t1268" href="#t1268">1268</a></span><span class="t">            <span class="op">(</span><span class="nam">from_pretrained_kwargs</span><span class="op">.</span><span class="nam">get</span><span class="op">(</span><span class="str">"torch_dtype"</span><span class="op">,</span> <span class="key">None</span><span class="op">)</span> <span class="op">==</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float16</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1269" href="#t1269">1269</a></span><span class="t">            <span class="key">or</span> <span class="nam">dtype</span> <span class="op">==</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float16</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1270" href="#t1270">1270</a></span><span class="t">        <span class="op">)</span> <span class="key">and</span> <span class="nam">device</span> <span class="key">in</span> <span class="op">[</span><span class="str">"cpu"</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1271" href="#t1271">1271</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">"float16 models may not work on CPU. Consider using a GPU or bfloat16."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1272" href="#t1272">1272</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1273" href="#t1273">1273</a></span><span class="t">        <span class="com"># Get the model name used in HuggingFace, rather than the alias.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1274" href="#t1274">1274</a></span><span class="t">        <span class="nam">official_model_name</span> <span class="op">=</span> <span class="nam">loading</span><span class="op">.</span><span class="nam">get_official_model_name</span><span class="op">(</span><span class="nam">model_name</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1275" href="#t1275">1275</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1276" href="#t1276">1276</a></span><span class="t">        <span class="com"># Load the config into an HookedTransformerConfig object. If loading from a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1277" href="#t1277">1277</a></span><span class="t">        <span class="com"># checkpoint, the config object will contain the information about the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1278" href="#t1278">1278</a></span><span class="t">        <span class="com"># checkpoint</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1279" href="#t1279">1279</a></span><span class="t">        <span class="nam">cfg</span> <span class="op">=</span> <span class="nam">loading</span><span class="op">.</span><span class="nam">get_pretrained_model_config</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1280" href="#t1280">1280</a></span><span class="t">            <span class="nam">official_model_name</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1281" href="#t1281">1281</a></span><span class="t">            <span class="nam">hf_cfg</span><span class="op">=</span><span class="nam">hf_cfg</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1282" href="#t1282">1282</a></span><span class="t">            <span class="nam">checkpoint_index</span><span class="op">=</span><span class="nam">checkpoint_index</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1283" href="#t1283">1283</a></span><span class="t">            <span class="nam">checkpoint_value</span><span class="op">=</span><span class="nam">checkpoint_value</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1284" href="#t1284">1284</a></span><span class="t">            <span class="nam">fold_ln</span><span class="op">=</span><span class="nam">fold_ln</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1285" href="#t1285">1285</a></span><span class="t">            <span class="nam">device</span><span class="op">=</span><span class="nam">device</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1286" href="#t1286">1286</a></span><span class="t">            <span class="nam">n_devices</span><span class="op">=</span><span class="nam">n_devices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1287" href="#t1287">1287</a></span><span class="t">            <span class="nam">default_prepend_bos</span><span class="op">=</span><span class="nam">default_prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1288" href="#t1288">1288</a></span><span class="t">            <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1289" href="#t1289">1289</a></span><span class="t">            <span class="nam">first_n_layers</span><span class="op">=</span><span class="nam">first_n_layers</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1290" href="#t1290">1290</a></span><span class="t">            <span class="op">**</span><span class="nam">from_pretrained_kwargs</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1291" href="#t1291">1291</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1292" href="#t1292">1292</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1293" href="#t1293">1293</a></span><span class="t">        <span class="key">if</span> <span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">==</span> <span class="str">"shortformer"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1294" href="#t1294">1294</a></span><span class="t">            <span class="key">if</span> <span class="nam">fold_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1295" href="#t1295">1295</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1296" href="#t1296">1296</a></span><span class="t">                    <span class="str">"You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1297" href="#t1297">1297</a></span><span class="t">                    <span class="str">"ln=False instead."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1298" href="#t1298">1298</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1299" href="#t1299">1299</a></span><span class="t">                <span class="nam">fold_ln</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1300" href="#t1300">1300</a></span><span class="t">            <span class="key">if</span> <span class="nam">center_unembed</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1301" href="#t1301">1301</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1302" href="#t1302">1302</a></span><span class="t">                    <span class="str">"You tried to specify center_unembed=True for a shortformer model, but this can't be done! "</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1303" href="#t1303">1303</a></span><span class="t">                    <span class="str">"Setting center_unembed=False instead."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1304" href="#t1304">1304</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1305" href="#t1305">1305</a></span><span class="t">                <span class="nam">center_unembed</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1306" href="#t1306">1306</a></span><span class="t">            <span class="key">if</span> <span class="nam">center_writing_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1307" href="#t1307">1307</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1308" href="#t1308">1308</a></span><span class="t">                    <span class="str">"You tried to specify center_writing_weights=True for a shortformer model, but this can't be done! "</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1309" href="#t1309">1309</a></span><span class="t">                    <span class="str">"Setting center_writing_weights=False instead."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1310" href="#t1310">1310</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1311" href="#t1311">1311</a></span><span class="t">                <span class="nam">center_writing_weights</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1312" href="#t1312">1312</a></span><span class="t">        <span class="key">if</span> <span class="nam">center_unembed</span> <span class="key">and</span> <span class="nam">cfg</span><span class="op">.</span><span class="nam">output_logits_soft_cap</span> <span class="op">></span> <span class="num">0.0</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1312&#x202F;&#x219B;&#x202F;1313</span><span class="annotate long">line 1312 didn't jump to line 1313, because the condition on line 1312 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1313" href="#t1313">1313</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1314" href="#t1314">1314</a></span><span class="t">                <span class="str">"You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant "</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1315" href="#t1315">1315</a></span><span class="t">                <span class="str">"Setting center_unembed=False instead."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1316" href="#t1316">1316</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1317" href="#t1317">1317</a></span><span class="t">            <span class="nam">center_unembed</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1318" href="#t1318">1318</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1319" href="#t1319">1319</a></span><span class="t">        <span class="com"># Get the state dict of the model (ie a mapping of parameter names to tensors), processed to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1320" href="#t1320">1320</a></span><span class="t">        <span class="com"># match the HookedTransformer parameter names.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1321" href="#t1321">1321</a></span><span class="t">        <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">loading</span><span class="op">.</span><span class="nam">get_pretrained_state_dict</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1322" href="#t1322">1322</a></span><span class="t">            <span class="nam">official_model_name</span><span class="op">,</span> <span class="nam">cfg</span><span class="op">,</span> <span class="nam">hf_model</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span> <span class="op">**</span><span class="nam">from_pretrained_kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1323" href="#t1323">1323</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1324" href="#t1324">1324</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1325" href="#t1325">1325</a></span><span class="t">        <span class="com"># Create the HookedTransformer object</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1326" href="#t1326">1326</a></span><span class="t">        <span class="nam">model</span> <span class="op">=</span> <span class="nam">cls</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1327" href="#t1327">1327</a></span><span class="t">            <span class="nam">cfg</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1328" href="#t1328">1328</a></span><span class="t">            <span class="nam">tokenizer</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1329" href="#t1329">1329</a></span><span class="t">            <span class="nam">move_to_device</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1330" href="#t1330">1330</a></span><span class="t">            <span class="nam">default_padding_side</span><span class="op">=</span><span class="nam">default_padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1331" href="#t1331">1331</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1332" href="#t1332">1332</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1333" href="#t1333">1333</a></span><span class="t">        <span class="nam">model</span><span class="op">.</span><span class="nam">load_and_process_state_dict</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1334" href="#t1334">1334</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1335" href="#t1335">1335</a></span><span class="t">            <span class="nam">fold_ln</span><span class="op">=</span><span class="nam">fold_ln</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1336" href="#t1336">1336</a></span><span class="t">            <span class="nam">center_writing_weights</span><span class="op">=</span><span class="nam">center_writing_weights</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1337" href="#t1337">1337</a></span><span class="t">            <span class="nam">center_unembed</span><span class="op">=</span><span class="nam">center_unembed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1338" href="#t1338">1338</a></span><span class="t">            <span class="nam">fold_value_biases</span><span class="op">=</span><span class="nam">fold_value_biases</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1339" href="#t1339">1339</a></span><span class="t">            <span class="nam">refactor_factored_attn_matrices</span><span class="op">=</span><span class="nam">refactor_factored_attn_matrices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1340" href="#t1340">1340</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1341" href="#t1341">1341</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1342" href="#t1342">1342</a></span><span class="t">        <span class="key">if</span> <span class="nam">move_to_device</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1342&#x202F;&#x219B;&#x202F;1345</span><span class="annotate long">line 1342 didn't jump to line 1345, because the condition on line 1342 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1343" href="#t1343">1343</a></span><span class="t">            <span class="nam">model</span><span class="op">.</span><span class="nam">move_model_modules_to_device</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1344" href="#t1344">1344</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1345" href="#t1345">1345</a></span><span class="t">        <span class="nam">print</span><span class="op">(</span><span class="str">f"Loaded pretrained model {model_name} into HookedTransformer"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1346" href="#t1346">1346</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1347" href="#t1347">1347</a></span><span class="t">        <span class="key">return</span> <span class="nam">model</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1348" href="#t1348">1348</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1349" href="#t1349">1349</a></span><span class="t">    <span class="op">@</span><span class="nam">classmethod</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1350" href="#t1350">1350</a></span><span class="t">    <span class="key">def</span> <span class="nam">from_pretrained_no_processing</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1351" href="#t1351">1351</a></span><span class="t">        <span class="nam">cls</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1352" href="#t1352">1352</a></span><span class="t">        <span class="nam">model_name</span><span class="op">:</span> <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1353" href="#t1353">1353</a></span><span class="t">        <span class="nam">fold_ln</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1354" href="#t1354">1354</a></span><span class="t">        <span class="nam">center_writing_weights</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1355" href="#t1355">1355</a></span><span class="t">        <span class="nam">center_unembed</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1356" href="#t1356">1356</a></span><span class="t">        <span class="nam">refactor_factored_attn_matrices</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1357" href="#t1357">1357</a></span><span class="t">        <span class="nam">fold_value_biases</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1358" href="#t1358">1358</a></span><span class="t">        <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1359" href="#t1359">1359</a></span><span class="t">        <span class="nam">default_prepend_bos</span><span class="op">=</span><span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1360" href="#t1360">1360</a></span><span class="t">        <span class="nam">default_padding_side</span><span class="op">=</span><span class="str">"right"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1361" href="#t1361">1361</a></span><span class="t">        <span class="op">**</span><span class="nam">from_pretrained_kwargs</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1362" href="#t1362">1362</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1363" href="#t1363">1363</a></span><span class="t">        <span class="str">"""Wrapper for from_pretrained.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1364" href="#t1364">1364</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1365" href="#t1365">1365</a></span><span class="t"><span class="str">        Wrapper for from_pretrained with all boolean flags related to simplifying the model set to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1366" href="#t1366">1366</a></span><span class="t"><span class="str">        False. Refer to from_pretrained for details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1367" href="#t1367">1367</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1368" href="#t1368">1368</a></span><span class="t">        <span class="key">return</span> <span class="nam">cls</span><span class="op">.</span><span class="nam">from_pretrained</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1369" href="#t1369">1369</a></span><span class="t">            <span class="nam">model_name</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1370" href="#t1370">1370</a></span><span class="t">            <span class="nam">fold_ln</span><span class="op">=</span><span class="nam">fold_ln</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1371" href="#t1371">1371</a></span><span class="t">            <span class="nam">center_writing_weights</span><span class="op">=</span><span class="nam">center_writing_weights</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1372" href="#t1372">1372</a></span><span class="t">            <span class="nam">center_unembed</span><span class="op">=</span><span class="nam">center_unembed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1373" href="#t1373">1373</a></span><span class="t">            <span class="nam">fold_value_biases</span><span class="op">=</span><span class="nam">fold_value_biases</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1374" href="#t1374">1374</a></span><span class="t">            <span class="nam">refactor_factored_attn_matrices</span><span class="op">=</span><span class="nam">refactor_factored_attn_matrices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1375" href="#t1375">1375</a></span><span class="t">            <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1376" href="#t1376">1376</a></span><span class="t">            <span class="nam">default_prepend_bos</span><span class="op">=</span><span class="nam">default_prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1377" href="#t1377">1377</a></span><span class="t">            <span class="nam">default_padding_side</span><span class="op">=</span><span class="nam">default_padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1378" href="#t1378">1378</a></span><span class="t">            <span class="op">**</span><span class="nam">from_pretrained_kwargs</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1379" href="#t1379">1379</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1380" href="#t1380">1380</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1381" href="#t1381">1381</a></span><span class="t">    <span class="key">def</span> <span class="nam">init_weights</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1382" href="#t1382">1382</a></span><span class="t">        <span class="str">"""Initialize weights.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1383" href="#t1383">1383</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1384" href="#t1384">1384</a></span><span class="t"><span class="str">        LayerNorm weights are already initialized to 1.0, and all biases are initialized to 0.0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1385" href="#t1385">1385</a></span><span class="t"><span class="str">        (including LayerNorm), so this just initializes weight matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1386" href="#t1386">1386</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1387" href="#t1387">1387</a></span><span class="t"><span class="str">        Weight matrices are set to empty by default (to save space + compute, since they're the bulk</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1388" href="#t1388">1388</a></span><span class="t"><span class="str">        of the parameters), so it is important to call this if you are not loading in pretrained</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1389" href="#t1389">1389</a></span><span class="t"><span class="str">        weights! Note that this function assumes that weight names being with `W_`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1390" href="#t1390">1390</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1391" href="#t1391">1391</a></span><span class="t"><span class="str">        Set seed here to ensure determinism.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1392" href="#t1392">1392</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1393" href="#t1393">1393</a></span><span class="t"><span class="str">        This does NOT follow the PyTorch scheme, which as far as I can tell is super out of date but</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1394" href="#t1394">1394</a></span><span class="t"><span class="str">        no one has gotten round to updating it? https://github.com/pytorch/pytorch/issues/18182</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1395" href="#t1395">1395</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1396" href="#t1396">1396</a></span><span class="t"><span class="str">        The default PyTorch scheme is the following: all linear layers use uniform(-1/sqrt(fan_in),</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1397" href="#t1397">1397</a></span><span class="t"><span class="str">        1/sqrt(fan_in)) for weights, and uniform(-1/sqrt(fan_in), 1/sqrt(fan_in)) for biases. For</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1398" href="#t1398">1398</a></span><span class="t"><span class="str">        biases, fan_in is computed using the fan_in for the weight matrix of the linear layer. Note</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1399" href="#t1399">1399</a></span><span class="t"><span class="str">        tha it *does not actually* use Kaiming initialization, despite the fact that it calls the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1400" href="#t1400">1400</a></span><span class="t"><span class="str">        function.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1401" href="#t1401">1401</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1402" href="#t1402">1402</a></span><span class="t"><span class="str">        However, for Transformer blocks, it instead initializes biases to zero and weights using Xavier uniform, that</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1403" href="#t1403">1403</a></span><span class="t"><span class="str">        is: uniform(-sqrt(6 / (fan_in + fan_out)), sqrt(6 / (fan_in + fan_out))) for weights.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1404" href="#t1404">1404</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1405" href="#t1405">1405</a></span><span class="t"><span class="str">        PyTorch Transformers are especially bad - TransformerEncoder initializes all layers to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1406" href="#t1406">1406</a></span><span class="t"><span class="str">        exact same weights?! https://github.com/pytorch/pytorch/issues/72253.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1407" href="#t1407">1407</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1408" href="#t1408">1408</a></span><span class="t"><span class="str">        The best paper I've found on transformer initialization is the muP paper, but haven't</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1409" href="#t1409">1409</a></span><span class="t"><span class="str">        integrated those ideas yet: https://arxiv.org/abs/2203.03466</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1410" href="#t1410">1410</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1411" href="#t1411">1411</a></span><span class="t"><span class="str">        We split off the initialization into separate functions because muP initialization handles</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1412" href="#t1412">1412</a></span><span class="t"><span class="str">        different parts of the model differently.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1413" href="#t1413">1413</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1414" href="#t1414">1414</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1415" href="#t1415">1415</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">seed</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1415&#x202F;&#x219B;&#x202F;1416</span><span class="annotate long">line 1415 didn't jump to line 1416, because the condition on line 1415 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1416" href="#t1416">1416</a></span><span class="t">            <span class="nam">torch</span><span class="op">.</span><span class="nam">manual_seed</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">seed</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1417" href="#t1417">1417</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1418" href="#t1418">1418</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"gpt2"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1418&#x202F;&#x219B;&#x202F;1420</span><span class="annotate long">line 1418 didn't jump to line 1420, because the condition on line 1418 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1419" href="#t1419">1419</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_gpt2</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1420" href="#t1420">1420</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"xavier_uniform"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1421" href="#t1421">1421</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_xavier</span><span class="op">(</span><span class="nam">dist_type</span><span class="op">=</span><span class="str">"uniform"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1422" href="#t1422">1422</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"xavier_normal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1423" href="#t1423">1423</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_xavier</span><span class="op">(</span><span class="nam">dist_type</span><span class="op">=</span><span class="str">"normal"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1424" href="#t1424">1424</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"kaiming_uniform"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1425" href="#t1425">1425</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_kaiming</span><span class="op">(</span><span class="nam">dist_type</span><span class="op">=</span><span class="str">"uniform"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1426" href="#t1426">1426</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"kaiming_normal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1427" href="#t1427">1427</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_kaiming</span><span class="op">(</span><span class="nam">dist_type</span><span class="op">=</span><span class="str">"normal"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1428" href="#t1428">1428</a></span><span class="t">        <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">init_mode</span> <span class="op">==</span> <span class="str">"muP"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1429" href="#t1429">1429</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">_init_weights_muP</span><span class="op">(</span><span class="nam">dist_type</span><span class="op">=</span><span class="str">"normal"</span><span class="op">)</span>  <span class="com"># muP uses normal initialization</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1430" href="#t1430">1430</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1431" href="#t1431">1431</a></span><span class="t">    <span class="key">def</span> <span class="nam">_init_weights_gpt2</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1432" href="#t1432">1432</a></span><span class="t">        <span class="str">"""Initialize weights with GPT-2 initialization. Biases are initialized to 0.0 and weights</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1433" href="#t1433">1433</a></span><span class="t"><span class="str">        are initialized to N(0, 0.64/d_model) if initializer_range is not set, otherwise std is initializer_range.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1434" href="#t1434">1434</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1435" href="#t1435">1435</a></span><span class="t">        <span class="key">for</span> <span class="nam">name</span><span class="op">,</span> <span class="nam">param</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">named_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1436" href="#t1436">1436</a></span><span class="t">            <span class="key">if</span> <span class="str">"W_"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1437" href="#t1437">1437</a></span><span class="t">                <span class="nam">nn</span><span class="op">.</span><span class="nam">init</span><span class="op">.</span><span class="nam">normal_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">std</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">initializer_range</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1438" href="#t1438">1438</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1439" href="#t1439">1439</a></span><span class="t">    <span class="key">def</span> <span class="nam">_init_weights_xavier</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">dist_type</span><span class="op">=</span><span class="str">"normal"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1440" href="#t1440">1440</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1441" href="#t1441">1441</a></span><span class="t"><span class="str">        Initialize weights with Xavier initialization -- that is, scale the weights by sqrt(6 /</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1442" href="#t1442">1442</a></span><span class="t"><span class="str">        (fan_in + fan_out)) for a [-1, 1] uniform distribution, or sqrt(2 / (fan_in + fan_out)) for a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1443" href="#t1443">1443</a></span><span class="t"><span class="str">        standard normal.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1444" href="#t1444">1444</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1445" href="#t1445">1445</a></span><span class="t"><span class="str">        Note that since TransformerLens implements the matrices in the opposite orientation to what</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1446" href="#t1446">1446</a></span><span class="t"><span class="str">        torch does (e.g. it's d_in x d_out, not d_out x d_in as in torch), we need to calculate it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1447" href="#t1447">1447</a></span><span class="t"><span class="str">        ourselves.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1448" href="#t1448">1448</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1449" href="#t1449">1449</a></span><span class="t">        <span class="nam">gain</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">initializer_range</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1450" href="#t1450">1450</a></span><span class="t">        <span class="key">for</span> <span class="nam">name</span><span class="op">,</span> <span class="nam">param</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">named_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1451" href="#t1451">1451</a></span><span class="t">            <span class="key">if</span> <span class="str">"W_"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1452" href="#t1452">1452</a></span><span class="t">                <span class="key">if</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"uniform"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1453" href="#t1453">1453</a></span><span class="t">                    <span class="nam">init_xavier_uniform_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">gain</span><span class="op">=</span><span class="nam">gain</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1454" href="#t1454">1454</a></span><span class="t">                <span class="key">elif</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"normal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1455" href="#t1455">1455</a></span><span class="t">                    <span class="nam">init_xavier_normal_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">gain</span><span class="op">=</span><span class="nam">gain</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1456" href="#t1456">1456</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1457" href="#t1457">1457</a></span><span class="t">    <span class="key">def</span> <span class="nam">_init_weights_kaiming</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">dist_type</span><span class="op">=</span><span class="str">"uniform"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1458" href="#t1458">1458</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1459" href="#t1459">1459</a></span><span class="t"><span class="str">        Initialize weights with Kaiming initialization -- that is, scale the weights by</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1460" href="#t1460">1460</a></span><span class="t"><span class="str">        c / sqrt(fan_in), where c = sqrt(2) if the params were immediately preceded by a relu and 1 for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1461" href="#t1461">1461</a></span><span class="t"><span class="str">        everything else.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1462" href="#t1462">1462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1463" href="#t1463">1463</a></span><span class="t"><span class="str">        Note that the numbers are actually incorrect here when you're using a nonlinearity other</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1464" href="#t1464">1464</a></span><span class="t"><span class="str">        than relu, e.g. the correct c for SiLu is ~1.74, for tanh it's 5/3 ~= 1.67, and for GeLU it's ~1.57.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1465" href="#t1465">1465</a></span><span class="t"><span class="str">        But this is unlikely to matter in practice.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1466" href="#t1466">1466</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1467" href="#t1467">1467</a></span><span class="t"><span class="str">        I'm just using fan_mode = "fan_in" for now, but it should be trivial to add fan_out.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1468" href="#t1468">1468</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1469" href="#t1469">1469</a></span><span class="t"><span class="str">        Again, we have to implement it ourselves because of the orientation of the matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1470" href="#t1470">1470</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1471" href="#t1471">1471</a></span><span class="t">        <span class="nam">gain</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">initializer_range</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1472" href="#t1472">1472</a></span><span class="t">        <span class="key">for</span> <span class="nam">name</span><span class="op">,</span> <span class="nam">param</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">named_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1473" href="#t1473">1473</a></span><span class="t">            <span class="key">if</span> <span class="str">"W_"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1474" href="#t1474">1474</a></span><span class="t">                <span class="key">if</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"uniform"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1475" href="#t1475">1475</a></span><span class="t">                    <span class="nam">init_kaiming_uniform_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">gain</span><span class="op">=</span><span class="nam">gain</span><span class="op">,</span> <span class="nam">nonlinearity</span><span class="op">=</span><span class="str">"relu"</span><span class="op">,</span> <span class="nam">mode</span><span class="op">=</span><span class="str">"fan_in"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1476" href="#t1476">1476</a></span><span class="t">                <span class="key">elif</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"normal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1477" href="#t1477">1477</a></span><span class="t">                    <span class="nam">init_kaiming_normal_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">gain</span><span class="op">=</span><span class="nam">gain</span><span class="op">,</span> <span class="nam">nonlinearity</span><span class="op">=</span><span class="str">"relu"</span><span class="op">,</span> <span class="nam">mode</span><span class="op">=</span><span class="str">"fan_in"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1478" href="#t1478">1478</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1479" href="#t1479">1479</a></span><span class="t">    <span class="key">def</span> <span class="nam">_init_weights_muP</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">dist_type</span><span class="op">=</span><span class="str">"uniform"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1480" href="#t1480">1480</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1481" href="#t1481">1481</a></span><span class="t"><span class="str">        Initialize weights with muParameterization. This involves scaling output weights by a factor</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1482" href="#t1482">1482</a></span><span class="t"><span class="str">        of 1/fan_in, input weights and biases by 1, everything else by a factor of 1/sqrt(fan_in).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1483" href="#t1483">1483</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1484" href="#t1484">1484</a></span><span class="t"><span class="str">        Also, you need to use muAdamW, which rescales the learning rate for output weights and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1485" href="#t1485">1485</a></span><span class="t"><span class="str">        hidden weights by a factor of 1/fan_in.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1486" href="#t1486">1486</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1487" href="#t1487">1487</a></span><span class="t"><span class="str">        All biases are still assumed to be initialized to 0.0, so we only need to change the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1488" href="#t1488">1488</a></span><span class="t"><span class="str">        weights.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1489" href="#t1489">1489</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1490" href="#t1490">1490</a></span><span class="t">        <span class="key">for</span> <span class="nam">name</span><span class="op">,</span> <span class="nam">param</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">named_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1491" href="#t1491">1491</a></span><span class="t">            <span class="key">if</span> <span class="str">"W_"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1492" href="#t1492">1492</a></span><span class="t">                <span class="nam">fan_in</span><span class="op">,</span> <span class="nam">_</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">calc_fan_in_and_fan_out</span><span class="op">(</span><span class="nam">param</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1493" href="#t1493">1493</a></span><span class="t">                <span class="key">if</span> <span class="str">"embed"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1494" href="#t1494">1494</a></span><span class="t">                    <span class="nam">scale</span> <span class="op">=</span> <span class="nam">float</span><span class="op">(</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1495" href="#t1495">1495</a></span><span class="t">                <span class="key">elif</span> <span class="str">"unembed"</span> <span class="key">in</span> <span class="nam">name</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1496" href="#t1496">1496</a></span><span class="t">                    <span class="nam">scale</span> <span class="op">=</span> <span class="num">1</span> <span class="op">/</span> <span class="nam">fan_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1497" href="#t1497">1497</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1498" href="#t1498">1498</a></span><span class="t">                    <span class="nam">scale</span> <span class="op">=</span> <span class="num">1</span> <span class="op">/</span> <span class="nam">fan_in</span><span class="op">**</span><span class="num">0.5</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1499" href="#t1499">1499</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1500" href="#t1500">1500</a></span><span class="t">                <span class="key">if</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"uniform"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1501" href="#t1501">1501</a></span><span class="t">                    <span class="nam">scale</span> <span class="op">*=</span> <span class="num">3</span><span class="op">**</span><span class="num">0.5</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1502" href="#t1502">1502</a></span><span class="t">                    <span class="nam">nn</span><span class="op">.</span><span class="nam">init</span><span class="op">.</span><span class="nam">uniform_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="op">-</span><span class="nam">scale</span><span class="op">,</span> <span class="nam">scale</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1503" href="#t1503">1503</a></span><span class="t">                <span class="key">elif</span> <span class="nam">dist_type</span> <span class="op">==</span> <span class="str">"normal"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1504" href="#t1504">1504</a></span><span class="t">                    <span class="nam">nn</span><span class="op">.</span><span class="nam">init</span><span class="op">.</span><span class="nam">normal_</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">std</span><span class="op">=</span><span class="nam">scale</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1505" href="#t1505">1505</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1506" href="#t1506">1506</a></span><span class="t">    <span class="key">def</span> <span class="nam">load_and_process_state_dict</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1507" href="#t1507">1507</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1508" href="#t1508">1508</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1509" href="#t1509">1509</a></span><span class="t">        <span class="nam">fold_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1510" href="#t1510">1510</a></span><span class="t">        <span class="nam">center_writing_weights</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1511" href="#t1511">1511</a></span><span class="t">        <span class="nam">center_unembed</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1512" href="#t1512">1512</a></span><span class="t">        <span class="nam">fold_value_biases</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1513" href="#t1513">1513</a></span><span class="t">        <span class="nam">refactor_factored_attn_matrices</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1514" href="#t1514">1514</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1515" href="#t1515">1515</a></span><span class="t">        <span class="str">"""Load &amp; Process State Dict.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1516" href="#t1516">1516</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1517" href="#t1517">1517</a></span><span class="t"><span class="str">        Load a state dict into the model, and to apply processing to simplify it. The state dict is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1518" href="#t1518">1518</a></span><span class="t"><span class="str">        assumed to be in the HookedTransformer format.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1519" href="#t1519">1519</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1520" href="#t1520">1520</a></span><span class="t"><span class="str">        See the relevant method (same name as the flag) for more details on the folding, centering</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1521" href="#t1521">1521</a></span><span class="t"><span class="str">        and processing flags.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1522" href="#t1522">1522</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1523" href="#t1523">1523</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1524" href="#t1524">1524</a></span><span class="t"><span class="str">            state_dict (dict): The state dict of the model, in HookedTransformer format. fold_ln</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1525" href="#t1525">1525</a></span><span class="t"><span class="str">            fold_ln (bool, optional): Whether to fold in the LayerNorm weights to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1526" href="#t1526">1526</a></span><span class="t"><span class="str">                subsequent linear layer. This does not change the computation. Defaults to True.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1527" href="#t1527">1527</a></span><span class="t"><span class="str">            center_writing_weights (bool, optional): Whether to center weights writing to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1528" href="#t1528">1528</a></span><span class="t"><span class="str">                residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1529" href="#t1529">1529</a></span><span class="t"><span class="str">                computation. Defaults to True.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1530" href="#t1530">1530</a></span><span class="t"><span class="str">            center_unembed (bool, optional): Whether to center W_U (ie set mean to be zero).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1531" href="#t1531">1531</a></span><span class="t"><span class="str">                Softmax is translation invariant so this doesn't affect log probs or loss, but does</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1532" href="#t1532">1532</a></span><span class="t"><span class="str">                change logits. Defaults to True.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1533" href="#t1533">1533</a></span><span class="t"><span class="str">            fold_value_biases (bool, optional): Whether to fold the value biases into the output</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1534" href="#t1534">1534</a></span><span class="t"><span class="str">                bias. Because attention patterns add up to 1, the value biases always have a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1535" href="#t1535">1535</a></span><span class="t"><span class="str">                constant effect on a layer's output, and it doesn't matter which head a bias is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1536" href="#t1536">1536</a></span><span class="t"><span class="str">                associated with. We can factor this all into a single output bias to the layer, and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1537" href="#t1537">1537</a></span><span class="t"><span class="str">                make it easier to interpret the head's output.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1538" href="#t1538">1538</a></span><span class="t"><span class="str">            refactor_factored_attn_matrices (bool, optional): Whether to convert the factored</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1539" href="#t1539">1539</a></span><span class="t"><span class="str">                matrices (W_Q &amp; W_K, and W_O &amp; W_V) to be "even". Defaults to False.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1540" href="#t1540">1540</a></span><span class="t"><span class="str">            model_name (str, optional): checks the model name for special cases of state dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1541" href="#t1541">1541</a></span><span class="t"><span class="str">                loading. Only used for Redwood 2L model currently.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1542" href="#t1542">1542</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1543" href="#t1543">1543</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">not</span> <span class="key">in</span> <span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float64</span><span class="op">]</span> <span class="key">and</span> <span class="nam">fold_ln</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1543&#x202F;&#x219B;&#x202F;1544</span><span class="annotate long">line 1543 didn't jump to line 1544, because the condition on line 1543 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1544" href="#t1544">1544</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1545" href="#t1545">1545</a></span><span class="t">                <span class="str">"With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1546" href="#t1546">1546</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1547" href="#t1547">1547</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1548" href="#t1548">1548</a></span><span class="t">        <span class="key">if</span> <span class="op">(</span>&nbsp;</span><span class="r"><span class="annotate short">1548&#x202F;&#x219B;&#x202F;1553</span><span class="annotate long">line 1548 didn't jump to line 1553</span></span></p>
    <p class="pln"><span class="n"><a id="t1549" href="#t1549">1549</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">not</span> <span class="key">in</span> <span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">float64</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1550" href="#t1550">1550</a></span><span class="t">            <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1551" href="#t1551">1551</a></span><span class="t">            <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span> <span class="op">></span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1552" href="#t1552">1552</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1553" href="#t1553">1553</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1554" href="#t1554">1554</a></span><span class="t">                <span class="str">"When running MoE models, it is advised to use a higher precision data type. See docs for more info."</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1555" href="#t1555">1555</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1556" href="#t1556">1556</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1557" href="#t1557">1557</a></span><span class="t">        <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">fill_missing_keys</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1558" href="#t1558">1558</a></span><span class="t">        <span class="key">if</span> <span class="nam">fold_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1559" href="#t1559">1559</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span> <span class="op">></span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1559&#x202F;&#x219B;&#x202F;1560</span><span class="annotate long">line 1559 didn't jump to line 1560, because the condition on line 1559 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1560" href="#t1560">1560</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1561" href="#t1561">1561</a></span><span class="t">                    <span class="str">"You are using MoE, so the layer norm weights can't be folded! Skipping"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1562" href="#t1562">1562</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1563" href="#t1563">1563</a></span><span class="t">            <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">in</span> <span class="op">[</span><span class="str">"LN"</span><span class="op">,</span> <span class="str">"LNPre"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1564" href="#t1564">1564</a></span><span class="t">                <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">fold_layer_norm</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1565" href="#t1565">1565</a></span><span class="t">            <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">in</span> <span class="op">[</span><span class="str">"RMS"</span><span class="op">,</span> <span class="str">"RMSPre"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1565&#x202F;&#x219B;&#x202F;1570</span><span class="annotate long">line 1565 didn't jump to line 1570, because the condition on line 1565 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1566" href="#t1566">1566</a></span><span class="t">                <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">fold_layer_norm</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1567" href="#t1567">1567</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">,</span> <span class="nam">fold_biases</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">center_weights</span><span class="op">=</span><span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1568" href="#t1568">1568</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1569" href="#t1569">1569</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1570" href="#t1570">1570</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1571" href="#t1571">1571</a></span><span class="t">                    <span class="str">"You are not using LayerNorm or RMSNorm, so the layer norm weights can't be folded! Skipping"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1572" href="#t1572">1572</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1573" href="#t1573">1573</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1574" href="#t1574">1574</a></span><span class="t">        <span class="key">if</span> <span class="nam">center_writing_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1575" href="#t1575">1575</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">not</span> <span class="key">in</span> <span class="op">[</span><span class="str">"LN"</span><span class="op">,</span> <span class="str">"LNPre"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1576" href="#t1576">1576</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1577" href="#t1577">1577</a></span><span class="t">                    <span class="str">"You are not using LayerNorm, so the writing weights can't be centered! Skipping"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1578" href="#t1578">1578</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1579" href="#t1579">1579</a></span><span class="t">            <span class="key">elif</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">final_rms</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1580" href="#t1580">1580</a></span><span class="t">                <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1581" href="#t1581">1581</a></span><span class="t">                    <span class="str">"This model is using final RMS normalization, so the writing weights can't be centered! Skipping"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1582" href="#t1582">1582</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1583" href="#t1583">1583</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1584" href="#t1584">1584</a></span><span class="t">                <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">center_writing_weights</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1585" href="#t1585">1585</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1586" href="#t1586">1586</a></span><span class="t">        <span class="key">if</span> <span class="nam">center_unembed</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1587" href="#t1587">1587</a></span><span class="t">            <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">center_unembed</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1588" href="#t1588">1588</a></span><span class="t">        <span class="key">if</span> <span class="nam">fold_value_biases</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1589" href="#t1589">1589</a></span><span class="t">            <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">fold_value_biases</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1590" href="#t1590">1590</a></span><span class="t">        <span class="key">if</span> <span class="nam">refactor_factored_attn_matrices</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1591" href="#t1591">1591</a></span><span class="t">            <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">refactor_factored_attn_matrices</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1592" href="#t1592">1592</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1593" href="#t1593">1593</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">load_in_4bit</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1593&#x202F;&#x219B;&#x202F;1596</span><span class="annotate long">line 1593 didn't jump to line 1596, because the condition on line 1593 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t1594" href="#t1594">1594</a></span><span class="t">            <span class="com"># with quantization, parameters should be assigned</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1595" href="#t1595">1595</a></span><span class="t">            <span class="com"># so that quantization settings are not lost</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t1596" href="#t1596">1596</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">load_state_dict</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">,</span> <span class="nam">assign</span><span class="op">=</span><span class="key">True</span><span class="op">,</span> <span class="nam">strict</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1597" href="#t1597">1597</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1598" href="#t1598">1598</a></span><span class="t">            <span class="nam">state_dict_keys</span> <span class="op">=</span> <span class="nam">list</span><span class="op">(</span><span class="nam">state_dict</span><span class="op">.</span><span class="nam">keys</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1599" href="#t1599">1599</a></span><span class="t">            <span class="key">for</span> <span class="nam">key</span> <span class="key">in</span> <span class="nam">state_dict_keys</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1600" href="#t1600">1600</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">load_state_dict</span><span class="op">(</span><span class="op">{</span><span class="nam">key</span><span class="op">:</span> <span class="nam">state_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span><span class="op">}</span><span class="op">,</span> <span class="nam">strict</span><span class="op">=</span><span class="key">False</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1601" href="#t1601">1601</a></span><span class="t">                <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1602" href="#t1602">1602</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1603" href="#t1603">1603</a></span><span class="t">    <span class="key">def</span> <span class="nam">fill_missing_keys</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1604" href="#t1604">1604</a></span><span class="t">        <span class="key">return</span> <span class="nam">loading</span><span class="op">.</span><span class="nam">fill_missing_keys</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1605" href="#t1605">1605</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1606" href="#t1606">1606</a></span><span class="t">    <span class="key">def</span> <span class="nam">fold_layer_norm</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1607" href="#t1607">1607</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">,</span> <span class="nam">fold_biases</span><span class="op">=</span><span class="key">True</span><span class="op">,</span> <span class="nam">center_weights</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1608" href="#t1608">1608</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1609" href="#t1609">1609</a></span><span class="t">        <span class="str">"""Fold Layer Norm. Can also be used to fold RMS Norm, when fold_biases and center_weights are set to False.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1610" href="#t1610">1610</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1611" href="#t1611">1611</a></span><span class="t"><span class="str">        Takes in a state dict from a pretrained model, formatted to be consistent with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1612" href="#t1612">1612</a></span><span class="t"><span class="str">        HookedTransformer but with LayerNorm weights and biases. Folds these into the neighbouring</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1613" href="#t1613">1613</a></span><span class="t"><span class="str">        weights. See further_comments.md for more details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1614" href="#t1614">1614</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1615" href="#t1615">1615</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1616" href="#t1616">1616</a></span><span class="t"><span class="str">            state_dict (Dict[str, torch.Tensor]): State dict of pretrained model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1617" href="#t1617">1617</a></span><span class="t"><span class="str">            fold_biases (bool): Enables folding of LN biases. Should be disabled when RMS Norm is used.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1618" href="#t1618">1618</a></span><span class="t"><span class="str">            center_weights (bool): Enables the centering of weights after folding in LN. Should be disabled when RMS Norm is used.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1619" href="#t1619">1619</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1620" href="#t1620">1620</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1621" href="#t1621">1621</a></span><span class="t">        <span class="com"># Models that use Grouped Query Attention (Only Mistral at the time of writing) prefix their K/V weights and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1622" href="#t1622">1622</a></span><span class="t">        <span class="com"># biases with an underscore in order to distinguish them, but folding the LN into them still works the same,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1623" href="#t1623">1623</a></span><span class="t">        <span class="com"># so we just add the underscore if GQA is used (i.e. if `cfg.n_key_value_heads is specified`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1624" href="#t1624">1624</a></span><span class="t">        <span class="nam">gqa</span> <span class="op">=</span> <span class="str">""</span> <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span> <span class="key">is</span> <span class="key">None</span> <span class="key">else</span> <span class="str">"_"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1625" href="#t1625">1625</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1626" href="#t1626">1626</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1627" href="#t1627">1627</a></span><span class="t">            <span class="com"># Fold ln1 into attention - it's important to fold biases first, since biases depend on</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1628" href="#t1628">1628</a></span><span class="t">            <span class="com"># weights but not vice versa The various indexing is just to broadcast ln.b and ln.w</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1629" href="#t1629">1629</a></span><span class="t">            <span class="com"># along every axis other than d_model. Each weight matrix right multiplies. To fold in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1630" href="#t1630">1630</a></span><span class="t">            <span class="com"># the bias, we use the W_ matrix to map it to the hidden space of the layer, so we need</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1631" href="#t1631">1631</a></span><span class="t">            <span class="com"># to sum along axis -2, which is the residual stream space axis.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1632" href="#t1632">1632</a></span><span class="t">            <span class="key">if</span> <span class="nam">fold_biases</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1633" href="#t1633">1633</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_Q"</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1634" href="#t1634">1634</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1635" href="#t1635">1635</a></span><span class="t">                    <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.b"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1636" href="#t1636">1636</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1637" href="#t1637">1637</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}b_K"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1638" href="#t1638">1638</a></span><span class="t">                    <span class="str">f"blocks.{l}.attn.{gqa}b_K"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1639" href="#t1639">1639</a></span><span class="t">                <span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1640" href="#t1640">1640</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_K"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1641" href="#t1641">1641</a></span><span class="t">                    <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.b"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1642" href="#t1642">1642</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1643" href="#t1643">1643</a></span><span class="t">                    <span class="op">-</span><span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1644" href="#t1644">1644</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1645" href="#t1645">1645</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}b_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1646" href="#t1646">1646</a></span><span class="t">                    <span class="str">f"blocks.{l}.attn.{gqa}b_V"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1647" href="#t1647">1647</a></span><span class="t">                <span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1648" href="#t1648">1648</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1649" href="#t1649">1649</a></span><span class="t">                    <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.b"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1650" href="#t1650">1650</a></span><span class="t">                <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1651" href="#t1651">1651</a></span><span class="t">                    <span class="op">-</span><span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1652" href="#t1652">1652</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1653" href="#t1653">1653</a></span><span class="t">                <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.b"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1654" href="#t1654">1654</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1655" href="#t1655">1655</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1656" href="#t1656">1656</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span> <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.w"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1657" href="#t1657">1657</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1658" href="#t1658">1658</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_K"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1659" href="#t1659">1659</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_K"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1660" href="#t1660">1660</a></span><span class="t">                <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.w"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1661" href="#t1661">1661</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1662" href="#t1662">1662</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_V"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1663" href="#t1663">1663</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1664" href="#t1664">1664</a></span><span class="t">                <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.w"</span><span class="op">]</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1665" href="#t1665">1665</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1666" href="#t1666">1666</a></span><span class="t">            <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.w"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1667" href="#t1667">1667</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1668" href="#t1668">1668</a></span><span class="t">            <span class="com"># Finally, we center the weights reading from the residual stream. The output of the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1669" href="#t1669">1669</a></span><span class="t">            <span class="com"># first part of the LayerNorm is mean 0 and standard deviation 1, so the mean of any</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1670" href="#t1670">1670</a></span><span class="t">            <span class="com"># input vector of the matrix doesn't matter and can be set to zero. Equivalently, the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1671" href="#t1671">1671</a></span><span class="t">            <span class="com"># output of LayerNormPre is orthogonal to the vector of all 1s (because dotting with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1672" href="#t1672">1672</a></span><span class="t">            <span class="com"># that gets the sum), so we can remove the component of the matrix parallel to this.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1673" href="#t1673">1673</a></span><span class="t">            <span class="key">if</span> <span class="nam">center_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1674" href="#t1674">1674</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1675" href="#t1675">1675</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1676" href="#t1676">1676</a></span><span class="t">                    <span class="str">"head_index d_model d_head -> head_index 1 d_head"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1677" href="#t1677">1677</a></span><span class="t">                    <span class="str">"mean"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1678" href="#t1678">1678</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1679" href="#t1679">1679</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_K"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1680" href="#t1680">1680</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_K"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1681" href="#t1681">1681</a></span><span class="t">                    <span class="str">"head_index d_model d_head -> head_index 1 d_head"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1682" href="#t1682">1682</a></span><span class="t">                    <span class="str">"mean"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1683" href="#t1683">1683</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1684" href="#t1684">1684</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_V"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1685" href="#t1685">1685</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.{gqa}W_V"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1686" href="#t1686">1686</a></span><span class="t">                    <span class="str">"head_index d_model d_head -> head_index 1 d_head"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1687" href="#t1687">1687</a></span><span class="t">                    <span class="str">"mean"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1688" href="#t1688">1688</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1689" href="#t1689">1689</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1690" href="#t1690">1690</a></span><span class="t">            <span class="com"># Fold ln2 into MLP</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1691" href="#t1691">1691</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_only</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1692" href="#t1692">1692</a></span><span class="t">                <span class="key">if</span> <span class="nam">fold_biases</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1693" href="#t1693">1693</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_in"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_in"</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1694" href="#t1694">1694</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1695" href="#t1695">1695</a></span><span class="t">                        <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.b"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1696" href="#t1696">1696</a></span><span class="t">                    <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1697" href="#t1697">1697</a></span><span class="t">                    <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.b"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1698" href="#t1698">1698</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1699" href="#t1699">1699</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1700" href="#t1700">1700</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span> <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.w"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1701" href="#t1701">1701</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1702" href="#t1702">1702</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1703" href="#t1703">1703</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">gated_mlp</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1704" href="#t1704">1704</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_gate"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1705" href="#t1705">1705</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_gate"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1706" href="#t1706">1706</a></span><span class="t">                        <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.w"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1707" href="#t1707">1707</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1708" href="#t1708">1708</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1709" href="#t1709">1709</a></span><span class="t">                <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.w"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1710" href="#t1710">1710</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1711" href="#t1711">1711</a></span><span class="t">                <span class="key">if</span> <span class="nam">center_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1712" href="#t1712">1712</a></span><span class="t">                    <span class="com"># Center the weights that read in from the LayerNormPre</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1713" href="#t1713">1713</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1714" href="#t1714">1714</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1715" href="#t1715">1715</a></span><span class="t">                        <span class="str">"d_model d_mlp -> 1 d_mlp"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1716" href="#t1716">1716</a></span><span class="t">                        <span class="str">"mean"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1717" href="#t1717">1717</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1718" href="#t1718">1718</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1719" href="#t1719">1719</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">act_fn</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">act_fn</span><span class="op">.</span><span class="nam">startswith</span><span class="op">(</span><span class="str">"solu"</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1720" href="#t1720">1720</a></span><span class="t">                    <span class="com"># Fold ln3 into activation</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1721" href="#t1721">1721</a></span><span class="t">                    <span class="key">if</span> <span class="nam">fold_biases</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1721&#x202F;&#x219B;&#x202F;1733</span><span class="annotate long">line 1721 didn't jump to line 1733</span></span></p>
    <p class="run"><span class="n"><a id="t1722" href="#t1722">1722</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_out"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1723" href="#t1723">1723</a></span><span class="t">                            <span class="str">f"blocks.{l}.mlp.b_out"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1724" href="#t1724">1724</a></span><span class="t">                        <span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1725" href="#t1725">1725</a></span><span class="t">                            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1726" href="#t1726">1726</a></span><span class="t">                            <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.ln.b"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1727" href="#t1727">1727</a></span><span class="t">                        <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1728" href="#t1728">1728</a></span><span class="t">                            <span class="op">-</span><span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1729" href="#t1729">1729</a></span><span class="t">                        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1730" href="#t1730">1730</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1731" href="#t1731">1731</a></span><span class="t">                        <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.ln.b"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1732" href="#t1732">1732</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1733" href="#t1733">1733</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1734" href="#t1734">1734</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1735" href="#t1735">1735</a></span><span class="t">                        <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.ln.w"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1736" href="#t1736">1736</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1737" href="#t1737">1737</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1738" href="#t1738">1738</a></span><span class="t">                    <span class="key">if</span> <span class="nam">center_weights</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1738&#x202F;&#x219B;&#x202F;1746</span><span class="annotate long">line 1738 didn't jump to line 1746, because the condition on line 1738 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t1739" href="#t1739">1739</a></span><span class="t">                        <span class="com"># Center the weights that read in from the LayerNormPre</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1740" href="#t1740">1740</a></span><span class="t">                        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1741" href="#t1741">1741</a></span><span class="t">                            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1742" href="#t1742">1742</a></span><span class="t">                            <span class="str">"d_mlp d_model -> 1 d_model"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1743" href="#t1743">1743</a></span><span class="t">                            <span class="str">"mean"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1744" href="#t1744">1744</a></span><span class="t">                        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1745" href="#t1745">1745</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1746" href="#t1746">1746</a></span><span class="t">                    <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.ln.w"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1747" href="#t1747">1747</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1748" href="#t1748">1748</a></span><span class="t">        <span class="com"># Fold ln_final into Unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1749" href="#t1749">1749</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">final_rms</span> <span class="key">and</span> <span class="nam">fold_biases</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1750" href="#t1750">1750</a></span><span class="t">            <span class="com"># Dumb bug from my old SoLU training code, some models have RMSNorm instead of LayerNorm</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1751" href="#t1751">1751</a></span><span class="t">            <span class="com"># pre unembed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1752" href="#t1752">1752</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.b_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.b_U"</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1753" href="#t1753">1753</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.W_U"</span><span class="op">]</span> <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"ln_final.b"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1754" href="#t1754">1754</a></span><span class="t">            <span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1755" href="#t1755">1755</a></span><span class="t">            <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"ln_final.b"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1756" href="#t1756">1756</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1757" href="#t1757">1757</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.W_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.W_U"</span><span class="op">]</span> <span class="op">*</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"ln_final.w"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1758" href="#t1758">1758</a></span><span class="t">        <span class="key">del</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"ln_final.w"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1759" href="#t1759">1759</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1760" href="#t1760">1760</a></span><span class="t">        <span class="key">if</span> <span class="nam">center_weights</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1761" href="#t1761">1761</a></span><span class="t">            <span class="com"># Center the weights that read in from the LayerNormPre</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1762" href="#t1762">1762</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.W_U"</span><span class="op">]</span> <span class="op">-=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">reduce</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1763" href="#t1763">1763</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"unembed.W_U"</span><span class="op">]</span><span class="op">,</span> <span class="str">"d_model d_vocab -> 1 d_vocab"</span><span class="op">,</span> <span class="str">"mean"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1764" href="#t1764">1764</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1765" href="#t1765">1765</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1766" href="#t1766">1766</a></span><span class="t">        <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1767" href="#t1767">1767</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1768" href="#t1768">1768</a></span><span class="t">    <span class="key">def</span> <span class="nam">center_writing_weights</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1769" href="#t1769">1769</a></span><span class="t">        <span class="str">"""Center Writing Weights.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1770" href="#t1770">1770</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1771" href="#t1771">1771</a></span><span class="t"><span class="str">        Centers the weights of the model that write to the residual stream - W_out, W_E, W_pos and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1772" href="#t1772">1772</a></span><span class="t"><span class="str">        W_out. This is done by subtracting the mean of the weights from the weights themselves. This</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1773" href="#t1773">1773</a></span><span class="t"><span class="str">        is done in-place. See fold_layer_norm for more details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1774" href="#t1774">1774</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1775" href="#t1775">1775</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">"embed.W_E"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"embed.W_E"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"embed.W_E"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1776" href="#t1776">1776</a></span><span class="t">            <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1777" href="#t1777">1777</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1778" href="#t1778">1778</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">!=</span> <span class="str">"rotary"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1779" href="#t1779">1779</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">"pos_embed.W_pos"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"pos_embed.W_pos"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1780" href="#t1780">1780</a></span><span class="t">                <span class="str">"pos_embed.W_pos"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1781" href="#t1781">1781</a></span><span class="t">            <span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1782" href="#t1782">1782</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1783" href="#t1783">1783</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_O"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1784" href="#t1784">1784</a></span><span class="t">                <span class="str">f"blocks.{l}.attn.W_O"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1785" href="#t1785">1785</a></span><span class="t">            <span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1786" href="#t1786">1786</a></span><span class="t">                <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1787" href="#t1787">1787</a></span><span class="t">            <span class="op">)</span>  <span class="com"># W_O is [head_index, d_model, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1788" href="#t1788">1788</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1789" href="#t1789">1789</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1790" href="#t1790">1790</a></span><span class="t">            <span class="op">)</span>  <span class="com"># b_O is [d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1791" href="#t1791">1791</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_only</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1792" href="#t1792">1792</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1793" href="#t1793">1793</a></span><span class="t">                    <span class="str">f"blocks.{l}.mlp.W_out"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1794" href="#t1794">1794</a></span><span class="t">                <span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1795" href="#t1795">1795</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_out"</span><span class="op">]</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1796" href="#t1796">1796</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_out"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_out"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1797" href="#t1797">1797</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1798" href="#t1798">1798</a></span><span class="t">        <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1799" href="#t1799">1799</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1800" href="#t1800">1800</a></span><span class="t">    <span class="key">def</span> <span class="nam">center_unembed</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1801" href="#t1801">1801</a></span><span class="t">        <span class="str">"""Center the unembedding weights W_U.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1802" href="#t1802">1802</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1803" href="#t1803">1803</a></span><span class="t"><span class="str">        This is done by subtracting the mean of the weights from the weights themselves. This is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1804" href="#t1804">1804</a></span><span class="t"><span class="str">        done in-place. As softmax is translation invariant, this changes the logits but not the log</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1805" href="#t1805">1805</a></span><span class="t"><span class="str">        probs, and makes the model logits (slightly) more interpretable - when trying to understand</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1806" href="#t1806">1806</a></span><span class="t"><span class="str">        how components contribute to the logits, we'll be less misled by components that just add</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1807" href="#t1807">1807</a></span><span class="t"><span class="str">        something to every logit.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1808" href="#t1808">1808</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1809" href="#t1809">1809</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.W_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.W_U"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.W_U"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1810" href="#t1810">1810</a></span><span class="t">            <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1811" href="#t1811">1811</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1812" href="#t1812">1812</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.b_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.b_U"</span><span class="op">]</span> <span class="op">-</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.b_U"</span><span class="op">]</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1813" href="#t1813">1813</a></span><span class="t">        <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1814" href="#t1814">1814</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1815" href="#t1815">1815</a></span><span class="t">    <span class="key">def</span> <span class="nam">fold_value_biases</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1816" href="#t1816">1816</a></span><span class="t">        <span class="str">"""Fold the value biases into the output bias.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1817" href="#t1817">1817</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1818" href="#t1818">1818</a></span><span class="t"><span class="str">        Because attention patterns add up to 1, the value biases always have a constant effect on a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1819" href="#t1819">1819</a></span><span class="t"><span class="str">        head's output. Further, as the outputs of each head in a layer add together, each head's</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1820" href="#t1820">1820</a></span><span class="t"><span class="str">        value bias has a constant effect on the *layer's* output, which can make it harder to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1821" href="#t1821">1821</a></span><span class="t"><span class="str">        interpret the effect of any given head, and it doesn't matter which head a bias is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1822" href="#t1822">1822</a></span><span class="t"><span class="str">        associated with. We can factor this all into a single output bias to the layer, and make it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1823" href="#t1823">1823</a></span><span class="t"><span class="str">        easier to interpret the head's output. Formally, we take b_O_new = b_O_original +</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1824" href="#t1824">1824</a></span><span class="t"><span class="str">        sum_head(b_V_head @ W_O_head).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1825" href="#t1825">1825</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1826" href="#t1826">1826</a></span><span class="t">        <span class="key">for</span> <span class="nam">layer</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1827" href="#t1827">1827</a></span><span class="t">            <span class="com"># shape [head_index, d_head]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1828" href="#t1828">1828</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1829" href="#t1829">1829</a></span><span class="t">                <span class="nam">b_V</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.b_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1830" href="#t1830">1830</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1831" href="#t1831">1831</a></span><span class="t">                <span class="nam">b_V</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn._b_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1832" href="#t1832">1832</a></span><span class="t">                <span class="nam">b_V</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">repeat_interleave</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1833" href="#t1833">1833</a></span><span class="t">                    <span class="nam">b_V</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">,</span> <span class="nam">repeats</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span> <span class="op">//</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1834" href="#t1834">1834</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1835" href="#t1835">1835</a></span><span class="t">            <span class="com"># [head_index, d_head, d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1836" href="#t1836">1836</a></span><span class="t">            <span class="nam">W_O</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.W_O"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1837" href="#t1837">1837</a></span><span class="t">            <span class="com"># [d_model]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1838" href="#t1838">1838</a></span><span class="t">            <span class="nam">b_O_original</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.b_O"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1839" href="#t1839">1839</a></span><span class="t">            <span class="nam">folded_b_O</span> <span class="op">=</span> <span class="nam">b_O_original</span> <span class="op">+</span> <span class="op">(</span><span class="nam">b_V</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span> <span class="op">*</span> <span class="nam">W_O</span><span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="op">[</span><span class="num">0</span><span class="op">,</span> <span class="num">1</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1840" href="#t1840">1840</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1841" href="#t1841">1841</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.b_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">folded_b_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1842" href="#t1842">1842</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1843" href="#t1843">1843</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.b_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">b_V</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1844" href="#t1844">1844</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1845" href="#t1845">1845</a></span><span class="t">                <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn._b_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1846" href="#t1846">1846</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn._b_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1847" href="#t1847">1847</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1848" href="#t1848">1848</a></span><span class="t">        <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1849" href="#t1849">1849</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1850" href="#t1850">1850</a></span><span class="t">    <span class="key">def</span> <span class="nam">refactor_factored_attn_matrices</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">state_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1851" href="#t1851">1851</a></span><span class="t">        <span class="str">"""Experimental method for managing queries, keys and values.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1852" href="#t1852">1852</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1853" href="#t1853">1853</a></span><span class="t"><span class="str">        As argued in [A Mathematical Framework for Transformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1854" href="#t1854">1854</a></span><span class="t"><span class="str">        Circuits](https://transformer-circuits.pub/2021/framework/index.html), queries, keys and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1855" href="#t1855">1855</a></span><span class="t"><span class="str">        values are somewhat arbitrary intermediate terms when computing with the low rank factored</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1856" href="#t1856">1856</a></span><span class="t"><span class="str">        matrices W_QK = W_Q @ W_K.T and W_OV = W_V @ W_O, and these matrices are the only thing</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1857" href="#t1857">1857</a></span><span class="t"><span class="str">        determining head behaviour. But there are many ways to find a low rank factorization to a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1858" href="#t1858">1858</a></span><span class="t"><span class="str">        given matrix, and hopefully some of these are more interpretable than others! This method is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1859" href="#t1859">1859</a></span><span class="t"><span class="str">        one attempt, which makes all of the matrices have orthogonal rows or columns, W_O into a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1860" href="#t1860">1860</a></span><span class="t"><span class="str">        rotation and W_Q and W_K having the nth column in each having the same norm. The formula is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1861" href="#t1861">1861</a></span><span class="t"><span class="str">        $W_V = U @ S,W_O=Vh.T,W_Q=U@S.sqrt(),W_K=Vh@S.sqrt()$.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1862" href="#t1862">1862</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1863" href="#t1863">1863</a></span><span class="t"><span class="str">        More details:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1864" href="#t1864">1864</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1865" href="#t1865">1865</a></span><span class="t"><span class="str">        If W_OV = U @ S @ Vh.T in its singular value decomposition, (where S is in R^d_head not</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1866" href="#t1866">1866</a></span><span class="t"><span class="str">        R^d_model, as W_OV is low rank), W_OV = (U @ S) @ (Vh.T) is an equivalent low rank</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1867" href="#t1867">1867</a></span><span class="t"><span class="str">        factorisation, where rows/columns of each matrix are orthogonal! So setting $W_V=US$ and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1868" href="#t1868">1868</a></span><span class="t"><span class="str">        $W_O=Vh.T$ works just as well. I *think* this is a more interpretable setup, because now</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1869" href="#t1869">1869</a></span><span class="t"><span class="str">        $W_O$ is just a rotation, and doesn't change the norm, so $z$ has the same norm as the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1870" href="#t1870">1870</a></span><span class="t"><span class="str">        result of the head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1871" href="#t1871">1871</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1872" href="#t1872">1872</a></span><span class="t"><span class="str">        For $W_QK = W_Q @ W_K.T$ we use the refactor $W_Q = U @ S.sqrt()$ and $W_K = Vh @ S.sqrt()$,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1873" href="#t1873">1873</a></span><span class="t"><span class="str">        which is also equivalent ($S==S.sqrt() @ S.sqrt()$ as $S$ is diagonal). Here we keep the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1874" href="#t1874">1874</a></span><span class="t"><span class="str">        matrices as having the same norm, since there's not an obvious asymmetry between the keys</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1875" href="#t1875">1875</a></span><span class="t"><span class="str">        and queries.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1876" href="#t1876">1876</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1877" href="#t1877">1877</a></span><span class="t"><span class="str">        Biases are more fiddly to deal with. For OV it's pretty easy - we just need (x @ W_V + b_V)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1878" href="#t1878">1878</a></span><span class="t"><span class="str">        @ W_O + b_O to be preserved, so we can set b_V' = 0. and b_O' = b_V @ W_O + b_O (note that</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1879" href="#t1879">1879</a></span><span class="t"><span class="str">        b_V in R^{head_index x d_head} while b_O in R^{d_model}, so we need to sum b_V @ W_O along</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1880" href="#t1880">1880</a></span><span class="t"><span class="str">        the head_index dimension too).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1881" href="#t1881">1881</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1882" href="#t1882">1882</a></span><span class="t"><span class="str">        For QK it's messy - we need to preserve the bilinear form of (x @ W_Q + b_Q) * (y @ W_K +</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1883" href="#t1883">1883</a></span><span class="t"><span class="str">        b_K), which is fairly messy. To deal with the biases, we concatenate them to W_Q and W_K to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1884" href="#t1884">1884</a></span><span class="t"><span class="str">        simulate a d_model+1 dimensional input (whose final coordinate is always 1), do the SVD</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1885" href="#t1885">1885</a></span><span class="t"><span class="str">        factorization on this effective matrix, then separate out into final weights and biases.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1886" href="#t1886">1886</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1887" href="#t1887">1887</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1888" href="#t1888">1888</a></span><span class="t">        <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1889" href="#t1889">1889</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">positional_embedding_type</span> <span class="op">!=</span> <span class="str">"rotary"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1890" href="#t1890">1890</a></span><span class="t">        <span class="op">)</span><span class="op">,</span> <span class="str">"You can't refactor the QK circuit when using rotary embeddings (as the QK matrix depends on the position of the query and key)"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1891" href="#t1891">1891</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1892" href="#t1892">1892</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1893" href="#t1893">1893</a></span><span class="t">            <span class="com"># W_QK = W_Q @ W_K.T</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1894" href="#t1894">1894</a></span><span class="t">            <span class="com"># Concatenate biases to make a d_model+1 input dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1895" href="#t1895">1895</a></span><span class="t">            <span class="nam">W_Q_eff</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1896" href="#t1896">1896</a></span><span class="t">                <span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1897" href="#t1897">1897</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1898" href="#t1898">1898</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_Q"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1899" href="#t1899">1899</a></span><span class="t">                <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1900" href="#t1900">1900</a></span><span class="t">                <span class="nam">dim</span><span class="op">=</span><span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1901" href="#t1901">1901</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1902" href="#t1902">1902</a></span><span class="t">            <span class="nam">W_K_eff</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1903" href="#t1903">1903</a></span><span class="t">                <span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1904" href="#t1904">1904</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_K"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1905" href="#t1905">1905</a></span><span class="t">                    <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_K"</span><span class="op">]</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1906" href="#t1906">1906</a></span><span class="t">                <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1907" href="#t1907">1907</a></span><span class="t">                <span class="nam">dim</span><span class="op">=</span><span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1908" href="#t1908">1908</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1909" href="#t1909">1909</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1910" href="#t1910">1910</a></span><span class="t">            <span class="nam">W_Q_eff_even</span><span class="op">,</span> <span class="nam">W_K_eff_even_T</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1911" href="#t1911">1911</a></span><span class="t">                <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">W_Q_eff</span><span class="op">,</span> <span class="nam">W_K_eff</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">)</span><span class="op">.</span><span class="nam">make_even</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">pair</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1912" href="#t1912">1912</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1913" href="#t1913">1913</a></span><span class="t">            <span class="nam">W_K_eff_even</span> <span class="op">=</span> <span class="nam">W_K_eff_even_T</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1914" href="#t1914">1914</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1915" href="#t1915">1915</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W_Q_eff_even</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">:</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1916" href="#t1916">1916</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W_Q_eff_even</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1917" href="#t1917">1917</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_K"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W_K_eff_even</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">:</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1918" href="#t1918">1918</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_K"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W_K_eff_even</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1919" href="#t1919">1919</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1920" href="#t1920">1920</a></span><span class="t">            <span class="com"># W_OV = W_V @ W_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1921" href="#t1921">1921</a></span><span class="t">            <span class="nam">W_V</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1922" href="#t1922">1922</a></span><span class="t">            <span class="nam">W_O</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_O"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1923" href="#t1923">1923</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1924" href="#t1924">1924</a></span><span class="t">            <span class="com"># Factors the bias to be consistent.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1925" href="#t1925">1925</a></span><span class="t">            <span class="nam">b_V</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_V"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1926" href="#t1926">1926</a></span><span class="t">            <span class="nam">b_O</span> <span class="op">=</span> <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1927" href="#t1927">1927</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1928" href="#t1928">1928</a></span><span class="t">            <span class="com"># Add singleton dimension for broadcasting</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1929" href="#t1929">1929</a></span><span class="t">            <span class="nam">b_V_expanded</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span><span class="nam">b_V</span><span class="op">,</span> <span class="str">"head_index d_head -> head_index d_head 1"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1930" href="#t1930">1930</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1931" href="#t1931">1931</a></span><span class="t">            <span class="com"># Element-wise multiplication of b_V and W_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1932" href="#t1932">1932</a></span><span class="t">            <span class="nam">b_V_times_W_O</span> <span class="op">=</span> <span class="nam">b_V_expanded</span> <span class="op">*</span> <span class="nam">W_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1933" href="#t1933">1933</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1934" href="#t1934">1934</a></span><span class="t">            <span class="com"># Sum over d_head and head_index dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1935" href="#t1935">1935</a></span><span class="t">            <span class="nam">b_V_contribution</span> <span class="op">=</span> <span class="nam">b_V_times_W_O</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="num">1</span><span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1936" href="#t1936">1936</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1937" href="#t1937">1937</a></span><span class="t">            <span class="nam">effective_bias</span> <span class="op">=</span> <span class="nam">b_O</span> <span class="op">+</span> <span class="nam">b_V_contribution</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1938" href="#t1938">1938</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">b_V</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1939" href="#t1939">1939</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">effective_bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1940" href="#t1940">1940</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1941" href="#t1941">1941</a></span><span class="t">            <span class="com"># Helper class to efficiently deal with low rank factored matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1942" href="#t1942">1942</a></span><span class="t">            <span class="nam">W_OV</span> <span class="op">=</span> <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">W_V</span><span class="op">,</span> <span class="nam">W_O</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1943" href="#t1943">1943</a></span><span class="t">            <span class="nam">U</span><span class="op">,</span> <span class="nam">S</span><span class="op">,</span> <span class="nam">Vh</span> <span class="op">=</span> <span class="nam">W_OV</span><span class="op">.</span><span class="nam">svd</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1944" href="#t1944">1944</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">U</span> <span class="op">@</span> <span class="nam">S</span><span class="op">.</span><span class="nam">diag_embed</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1945" href="#t1945">1945</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="nam">Vh</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1946" href="#t1946">1946</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1947" href="#t1947">1947</a></span><span class="t">        <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1948" href="#t1948">1948</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1949" href="#t1949">1949</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_use_attn_result</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">use_attn_result</span><span class="op">:</span> <span class="nam">bool</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1950" href="#t1950">1950</a></span><span class="t">        <span class="str">"""Toggle whether to explicitly calculate and expose the result for each attention head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1951" href="#t1951">1951</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1952" href="#t1952">1952</a></span><span class="t"><span class="str">        Useful for interpretability but can easily burn through GPU memory.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1953" href="#t1953">1953</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1954" href="#t1954">1954</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_result</span> <span class="op">=</span> <span class="nam">use_attn_result</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1955" href="#t1955">1955</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1956" href="#t1956">1956</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_use_split_qkv_input</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">use_split_qkv_input</span><span class="op">:</span> <span class="nam">bool</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1957" href="#t1957">1957</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1958" href="#t1958">1958</a></span><span class="t"><span class="str">        Toggles whether to allow editing of inputs to each attention head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1959" href="#t1959">1959</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1960" href="#t1960">1960</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_split_qkv_input</span> <span class="op">=</span> <span class="nam">use_split_qkv_input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1961" href="#t1961">1961</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1962" href="#t1962">1962</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_use_hook_mlp_in</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">use_hook_mlp_in</span><span class="op">:</span> <span class="nam">bool</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1963" href="#t1963">1963</a></span><span class="t">        <span class="str">"""Toggles whether to allow storing and editing inputs to each MLP layer."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1964" href="#t1964">1964</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1965" href="#t1965">1965</a></span><span class="t">        <span class="key">assert</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_only</span><span class="op">,</span> <span class="str">"Can't use hook_mlp_in with attn_only model"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1966" href="#t1966">1966</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_hook_mlp_in</span> <span class="op">=</span> <span class="nam">use_hook_mlp_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1967" href="#t1967">1967</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1968" href="#t1968">1968</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_use_attn_in</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">use_attn_in</span><span class="op">:</span> <span class="nam">bool</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1969" href="#t1969">1969</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1970" href="#t1970">1970</a></span><span class="t"><span class="str">        Toggles whether to allow editing of inputs to each attention head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1971" href="#t1971">1971</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1972" href="#t1972">1972</a></span><span class="t">        <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1973" href="#t1973">1973</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_key_value_heads</span> <span class="key">is</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1974" href="#t1974">1974</a></span><span class="t">        <span class="op">)</span><span class="op">,</span> <span class="str">"Can't use attn_in with GroupedQueryAttention, please use split_qkv_input instead"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1975" href="#t1975">1975</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">use_attn_in</span> <span class="op">=</span> <span class="nam">use_attn_in</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1976" href="#t1976">1976</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1977" href="#t1977">1977</a></span><span class="t">    <span class="key">def</span> <span class="nam">set_ungroup_grouped_query_attention</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">ungroup_grouped_query_attention</span><span class="op">:</span> <span class="nam">bool</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1978" href="#t1978">1978</a></span><span class="t">        <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1979" href="#t1979">1979</a></span><span class="t"><span class="str">        Toggles whether to ungroup the grouped key and value heads in models with grouped query attention (GQA).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1980" href="#t1980">1980</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1981" href="#t1981">1981</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">ungroup_grouped_query_attention</span> <span class="op">=</span> <span class="nam">ungroup_grouped_query_attention</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1982" href="#t1982">1982</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1983" href="#t1983">1983</a></span><span class="t">    <span class="key">def</span> <span class="nam">process_weights_</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1984" href="#t1984">1984</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1985" href="#t1985">1985</a></span><span class="t">        <span class="nam">fold_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1986" href="#t1986">1986</a></span><span class="t">        <span class="nam">center_writing_weights</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1987" href="#t1987">1987</a></span><span class="t">        <span class="nam">center_unembed</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1988" href="#t1988">1988</a></span><span class="t">        <span class="nam">refactor_factored_attn_matrices</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1989" href="#t1989">1989</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1990" href="#t1990">1990</a></span><span class="t">        <span class="str">"""Wrapper around `load_and_process_state_dict`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1991" href="#t1991">1991</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1992" href="#t1992">1992</a></span><span class="t"><span class="str">        Wrapper around load_and_process_state_dict to allow for in-place processing of the weights.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1993" href="#t1993">1993</a></span><span class="t"><span class="str">        This is useful if using HookedTransformer for training, if we then want to analyse a cleaner</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1994" href="#t1994">1994</a></span><span class="t"><span class="str">        version of the same model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1995" href="#t1995">1995</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1996" href="#t1996">1996</a></span><span class="t">        <span class="nam">state_dict</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">state_dict</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1997" href="#t1997">1997</a></span><span class="t">        <span class="key">if</span> <span class="nam">fold_ln</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">num_experts</span> <span class="op">></span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1997&#x202F;&#x219B;&#x202F;2000</span><span class="annotate long">line 1997 didn't jump to line 2000, because the condition on line 1997 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t1998" href="#t1998">1998</a></span><span class="t">            <span class="com"># If we're using MoE, we don't fold the layer norm weights, so we don't need to do any preprocessing</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1999" href="#t1999">1999</a></span><span class="t">            <span class="com"># A warning is already issued in `load_and_process_state_dict`</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2000" href="#t2000">2000</a></span><span class="t">            <span class="key">pass</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2001" href="#t2001">2001</a></span><span class="t">        <span class="key">elif</span> <span class="nam">fold_ln</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"LN"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2001&#x202F;&#x219B;&#x202F;2012</span><span class="annotate long">line 2001 didn't jump to line 2012, because the condition on line 2001 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t2002" href="#t2002">2002</a></span><span class="t">            <span class="com"># If we're folding the LN into the weights, we need to replace all the layernorm layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2003" href="#t2003">2003</a></span><span class="t">            <span class="com"># with LayerNormPres, which do not have learnable parameters. This is somewhat hacky,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2004" href="#t2004">2004</a></span><span class="t">            <span class="com"># but it's the easiest way to do it.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2005" href="#t2005">2005</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">=</span> <span class="str">"LNPre"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2006" href="#t2006">2006</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">LayerNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2007" href="#t2007">2007</a></span><span class="t">            <span class="key">for</span> <span class="nam">layer</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2008" href="#t2008">2008</a></span><span class="t">                <span class="nam">layer</span><span class="op">.</span><span class="nam">ln1</span> <span class="op">=</span> <span class="nam">LayerNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2009" href="#t2009">2009</a></span><span class="t">                <span class="nam">layer</span><span class="op">.</span><span class="nam">ln2</span> <span class="op">=</span> <span class="nam">LayerNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2010" href="#t2010">2010</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">is_layer_norm_activation</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2010&#x202F;&#x219B;&#x202F;2011</span><span class="annotate long">line 2010 didn't jump to line 2011, because the condition on line 2010 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2011" href="#t2011">2011</a></span><span class="t">                    <span class="nam">layer</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">ln</span> <span class="op">=</span> <span class="nam">LayerNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2012" href="#t2012">2012</a></span><span class="t">        <span class="key">elif</span> <span class="nam">fold_ln</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">==</span> <span class="str">"RMS"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2013" href="#t2013">2013</a></span><span class="t">            <span class="com"># We do the same for RMSNorm if used</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2014" href="#t2014">2014</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="op">=</span> <span class="str">"RMSPre"</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2015" href="#t2015">2015</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">ln_final</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2016" href="#t2016">2016</a></span><span class="t">            <span class="key">for</span> <span class="nam">layer</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2017" href="#t2017">2017</a></span><span class="t">                <span class="nam">layer</span><span class="op">.</span><span class="nam">ln1</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2018" href="#t2018">2018</a></span><span class="t">                <span class="nam">layer</span><span class="op">.</span><span class="nam">ln2</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2019" href="#t2019">2019</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">is_layer_norm_activation</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2020" href="#t2020">2020</a></span><span class="t">                    <span class="nam">layer</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">ln</span> <span class="op">=</span> <span class="nam">RMSNormPre</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2021" href="#t2021">2021</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2022" href="#t2022">2022</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">load_and_process_state_dict</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2023" href="#t2023">2023</a></span><span class="t">            <span class="nam">state_dict</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2024" href="#t2024">2024</a></span><span class="t">            <span class="nam">fold_ln</span><span class="op">=</span><span class="nam">fold_ln</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2025" href="#t2025">2025</a></span><span class="t">            <span class="nam">center_writing_weights</span><span class="op">=</span><span class="nam">center_writing_weights</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2026" href="#t2026">2026</a></span><span class="t">            <span class="nam">center_unembed</span><span class="op">=</span><span class="nam">center_unembed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2027" href="#t2027">2027</a></span><span class="t">            <span class="nam">refactor_factored_attn_matrices</span><span class="op">=</span><span class="nam">refactor_factored_attn_matrices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2028" href="#t2028">2028</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2029" href="#t2029">2029</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2030" href="#t2030">2030</a></span><span class="t">    <span class="op">@</span><span class="nam">torch</span><span class="op">.</span><span class="nam">inference_mode</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2031" href="#t2031">2031</a></span><span class="t">    <span class="key">def</span> <span class="nam">generate</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2032" href="#t2032">2032</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2033" href="#t2033">2033</a></span><span class="t">        <span class="nam">input</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="str">""</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2034" href="#t2034">2034</a></span><span class="t">        <span class="nam">max_new_tokens</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="num">10</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2035" href="#t2035">2035</a></span><span class="t">        <span class="nam">stop_at_eos</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2036" href="#t2036">2036</a></span><span class="t">        <span class="nam">eos_token_id</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2037" href="#t2037">2037</a></span><span class="t">        <span class="nam">do_sample</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2038" href="#t2038">2038</a></span><span class="t">        <span class="nam">top_k</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2039" href="#t2039">2039</a></span><span class="t">        <span class="nam">top_p</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">float</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2040" href="#t2040">2040</a></span><span class="t">        <span class="nam">temperature</span><span class="op">:</span> <span class="nam">float</span> <span class="op">=</span> <span class="num">1.0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2041" href="#t2041">2041</a></span><span class="t">        <span class="nam">freq_penalty</span><span class="op">:</span> <span class="nam">float</span> <span class="op">=</span> <span class="num">0.0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2042" href="#t2042">2042</a></span><span class="t">        <span class="nam">use_past_kv_cache</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2043" href="#t2043">2043</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">bool</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2044" href="#t2044">2044</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2045" href="#t2045">2045</a></span><span class="t">        <span class="nam">return_type</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span> <span class="op">=</span> <span class="str">"input"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2046" href="#t2046">2046</a></span><span class="t">        <span class="nam">verbose</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2047" href="#t2047">2047</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch pos_plus_new_tokens"</span><span class="op">]</span><span class="op">,</span> <span class="nam">str</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2048" href="#t2048">2048</a></span><span class="t">        <span class="str">"""Sample Tokens from the Model.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2049" href="#t2049">2049</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2050" href="#t2050">2050</a></span><span class="t"><span class="str">        Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2051" href="#t2051">2051</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2052" href="#t2052">2052</a></span><span class="t"><span class="str">        To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2053" href="#t2053">2053</a></span><span class="t"><span class="str">        (by producing an EOT token), we keep running the model on the entire batch, but throw away</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2054" href="#t2054">2054</a></span><span class="t"><span class="str">        the output for a finished sequence and just keep adding EOTs to pad.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2055" href="#t2055">2055</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2056" href="#t2056">2056</a></span><span class="t"><span class="str">        This supports entering a single string, but not a list of strings - if the strings don't</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2057" href="#t2057">2057</a></span><span class="t"><span class="str">        tokenize to exactly the same length, this gets messy. If that functionality is needed,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2058" href="#t2058">2058</a></span><span class="t"><span class="str">        convert them to a batch of tokens and input that instead.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2059" href="#t2059">2059</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2060" href="#t2060">2060</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2061" href="#t2061">2061</a></span><span class="t"><span class="str">            input (Union[str, Int[torch.Tensor, "batch pos"])]): Either a batch of tokens ([batch,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2062" href="#t2062">2062</a></span><span class="t"><span class="str">                pos]) or a text string (this will be converted to a batch of tokens with batch size</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2063" href="#t2063">2063</a></span><span class="t"><span class="str">                1).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2064" href="#t2064">2064</a></span><span class="t"><span class="str">            max_new_tokens (int): Maximum number of tokens to generate.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2065" href="#t2065">2065</a></span><span class="t"><span class="str">            stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2066" href="#t2066">2066</a></span><span class="t"><span class="str">            eos_token_id (Optional[Union[int, Sequence]]): The token ID to use for end</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2067" href="#t2067">2067</a></span><span class="t"><span class="str">                of sentence. If None, use the tokenizer's eos_token_id - required if using</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2068" href="#t2068">2068</a></span><span class="t"><span class="str">                stop_at_eos. It's also possible to provide a list of token IDs (not just the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2069" href="#t2069">2069</a></span><span class="t"><span class="str">                eos_token_id), in which case the generation will stop when any of them are output</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2070" href="#t2070">2070</a></span><span class="t"><span class="str">                (useful e.g. for stable_lm).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2071" href="#t2071">2071</a></span><span class="t"><span class="str">            do_sample (bool): If True, sample from the model's output distribution. Otherwise, use</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2072" href="#t2072">2072</a></span><span class="t"><span class="str">                greedy search (take the max logit each time).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2073" href="#t2073">2073</a></span><span class="t"><span class="str">            top_k (int): Number of tokens to sample from. If None, sample from all tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2074" href="#t2074">2074</a></span><span class="t"><span class="str">            top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If &lt;1.0,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2075" href="#t2075">2075</a></span><span class="t"><span class="str">                we take the top tokens with cumulative probability >= top_p.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2076" href="#t2076">2076</a></span><span class="t"><span class="str">            temperature (float): Temperature for sampling. Higher values will make the model more</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2077" href="#t2077">2077</a></span><span class="t"><span class="str">                random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2078" href="#t2078">2078</a></span><span class="t"><span class="str">                sampling from a uniform distribution).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2079" href="#t2079">2079</a></span><span class="t"><span class="str">            freq_penalty (float): Frequency penalty for sampling - how much to penalise previous</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2080" href="#t2080">2080</a></span><span class="t"><span class="str">                tokens. Higher values will make the model more random.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2081" href="#t2081">2081</a></span><span class="t"><span class="str">            use_past_kv_cache (bool): If True, create and use cache to speed up generation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2082" href="#t2082">2082</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2083" href="#t2083">2083</a></span><span class="t"><span class="str">                the BOS token to the input (applicable when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2084" href="#t2084">2084</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos (default is True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2085" href="#t2085">2085</a></span><span class="t"><span class="str">                otherwise). Pass True or False to override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2086" href="#t2086">2086</a></span><span class="t"><span class="str">            padding_side (Union[Literal["left", "right"], None], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2087" href="#t2087">2087</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2088" href="#t2088">2088</a></span><span class="t"><span class="str">                strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2089" href="#t2089">2089</a></span><span class="t"><span class="str">            return_type (Optional[str]): The type of the output to return - either a string (str),</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2090" href="#t2090">2090</a></span><span class="t"><span class="str">                a tensor of tokens (tensor) or whatever the format of the input was (input).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2091" href="#t2091">2091</a></span><span class="t"><span class="str">            verbose (bool): If True, show tqdm progress bars for generation.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2092" href="#t2092">2092</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2093" href="#t2093">2093</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2094" href="#t2094">2094</a></span><span class="t"><span class="str">            outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2095" href="#t2095">2095</a></span><span class="t"><span class="str">                (by default returns same type as input).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2096" href="#t2096">2096</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2097" href="#t2097">2097</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2098" href="#t2098">2098</a></span><span class="t">        <span class="key">with</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">LocallyOverridenDefaults</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2099" href="#t2099">2099</a></span><span class="t">            <span class="nam">self</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2100" href="#t2100">2100</a></span><span class="t">        <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2101" href="#t2101">2101</a></span><span class="t">            <span class="key">if</span> <span class="nam">type</span><span class="op">(</span><span class="nam">input</span><span class="op">)</span> <span class="op">==</span> <span class="nam">str</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2101&#x202F;&#x219B;&#x202F;2108</span><span class="annotate long">line 2101 didn't jump to line 2108, because the condition on line 2101 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t2102" href="#t2102">2102</a></span><span class="t">                <span class="com"># If text, convert to tokens (batch_size=1)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2103" href="#t2103">2103</a></span><span class="t">                <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2104" href="#t2104">2104</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2105" href="#t2105">2105</a></span><span class="t">                <span class="op">)</span><span class="op">,</span> <span class="str">"Must provide a tokenizer if passing a string to the model"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2106" href="#t2106">2106</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span> <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2107" href="#t2107">2107</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2108" href="#t2108">2108</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2109" href="#t2109">2109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2110" href="#t2110">2110</a></span><span class="t">            <span class="key">if</span> <span class="nam">return_type</span> <span class="op">==</span> <span class="str">"input"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2110&#x202F;&#x219B;&#x202F;2116</span><span class="annotate long">line 2110 didn't jump to line 2116, because the condition on line 2110 was never false</span></span></p>
    <p class="par run show_par"><span class="n"><a id="t2111" href="#t2111">2111</a></span><span class="t">                <span class="key">if</span> <span class="nam">type</span><span class="op">(</span><span class="nam">input</span><span class="op">)</span> <span class="op">==</span> <span class="nam">str</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2111&#x202F;&#x219B;&#x202F;2114</span><span class="annotate long">line 2111 didn't jump to line 2114, because the condition on line 2111 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t2112" href="#t2112">2112</a></span><span class="t">                    <span class="nam">return_type</span> <span class="op">=</span> <span class="str">"str"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2113" href="#t2113">2113</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2114" href="#t2114">2114</a></span><span class="t">                    <span class="nam">return_type</span> <span class="op">=</span> <span class="str">"tensor"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2115" href="#t2115">2115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2116" href="#t2116">2116</a></span><span class="t">            <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2117" href="#t2117">2117</a></span><span class="t">            <span class="nam">batch_size</span><span class="op">,</span> <span class="nam">ctx_length</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2118" href="#t2118">2118</a></span><span class="t">            <span class="nam">device</span> <span class="op">=</span> <span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2119" href="#t2119">2119</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2120" href="#t2120">2120</a></span><span class="t">            <span class="key">if</span> <span class="nam">use_past_kv_cache</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2120&#x202F;&#x219B;&#x202F;2125</span><span class="annotate long">line 2120 didn't jump to line 2125, because the condition on line 2120 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t2121" href="#t2121">2121</a></span><span class="t">                <span class="nam">past_kv_cache</span> <span class="op">=</span> <span class="nam">HookedTransformerKeyValueCache</span><span class="op">.</span><span class="nam">init_cache</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2122" href="#t2122">2122</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span> <span class="nam">batch_size</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2123" href="#t2123">2123</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2124" href="#t2124">2124</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2125" href="#t2125">2125</a></span><span class="t">                <span class="nam">past_kv_cache</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2126" href="#t2126">2126</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2127" href="#t2127">2127</a></span><span class="t">            <span class="nam">stop_tokens</span><span class="op">:</span> <span class="nam">List</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2128" href="#t2128">2128</a></span><span class="t">            <span class="nam">eos_token_for_padding</span> <span class="op">=</span> <span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2129" href="#t2129">2129</a></span><span class="t">            <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2130" href="#t2130">2130</a></span><span class="t">            <span class="key">if</span> <span class="nam">stop_at_eos</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2130&#x202F;&#x219B;&#x202F;2152</span><span class="annotate long">line 2130 didn't jump to line 2152, because the condition on line 2130 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t2131" href="#t2131">2131</a></span><span class="t">                <span class="nam">tokenizer_has_eos_token</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2132" href="#t2132">2132</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token_id</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2133" href="#t2133">2133</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2134" href="#t2134">2134</a></span><span class="t">                <span class="key">if</span> <span class="nam">eos_token_id</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2134&#x202F;&#x219B;&#x202F;2141</span><span class="annotate long">line 2134 didn't jump to line 2141, because the condition on line 2134 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t2135" href="#t2135">2135</a></span><span class="t">                    <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2136" href="#t2136">2136</a></span><span class="t">                        <span class="nam">tokenizer_has_eos_token</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2137" href="#t2137">2137</a></span><span class="t">                    <span class="op">)</span><span class="op">,</span> <span class="str">"Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2138" href="#t2138">2138</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2139" href="#t2139">2139</a></span><span class="t">                    <span class="nam">eos_token_id</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token_id</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2140" href="#t2140">2140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2141" href="#t2141">2141</a></span><span class="t">                <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">eos_token_id</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2141&#x202F;&#x219B;&#x202F;2146</span><span class="annotate long">line 2141 didn't jump to line 2146, because the condition on line 2141 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t2142" href="#t2142">2142</a></span><span class="t">                    <span class="nam">stop_tokens</span> <span class="op">=</span> <span class="op">[</span><span class="nam">eos_token_id</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2143" href="#t2143">2143</a></span><span class="t">                    <span class="nam">eos_token_for_padding</span> <span class="op">=</span> <span class="nam">eos_token_id</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2144" href="#t2144">2144</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2145" href="#t2145">2145</a></span><span class="t">                    <span class="com"># eos_token_id is a Sequence (e.g. list or tuple)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2146" href="#t2146">2146</a></span><span class="t">                    <span class="nam">stop_tokens</span> <span class="op">=</span> <span class="nam">eos_token_id</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2147" href="#t2147">2147</a></span><span class="t">                    <span class="nam">eos_token_for_padding</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2148" href="#t2148">2148</a></span><span class="t">                        <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">eos_token_id</span> <span class="key">if</span> <span class="nam">tokenizer_has_eos_token</span> <span class="key">else</span> <span class="nam">eos_token_id</span><span class="op">[</span><span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2149" href="#t2149">2149</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2150" href="#t2150">2150</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2151" href="#t2151">2151</a></span><span class="t">            <span class="com"># An array to track which sequences in the batch have finished.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2152" href="#t2152">2152</a></span><span class="t">            <span class="nam">finished_sequences</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="nam">batch_size</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bool</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2153" href="#t2153">2153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2154" href="#t2154">2154</a></span><span class="t">            <span class="com"># Currently nothing in HookedTransformer changes with eval, but this is here in case</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2155" href="#t2155">2155</a></span><span class="t">            <span class="com"># that changes in the future.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2156" href="#t2156">2156</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">eval</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2157" href="#t2157">2157</a></span><span class="t">            <span class="key">for</span> <span class="nam">index</span> <span class="key">in</span> <span class="nam">tqdm</span><span class="op">.</span><span class="nam">tqdm</span><span class="op">(</span><span class="nam">range</span><span class="op">(</span><span class="nam">max_new_tokens</span><span class="op">)</span><span class="op">,</span> <span class="nam">disable</span><span class="op">=</span><span class="key">not</span> <span class="nam">verbose</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2158" href="#t2158">2158</a></span><span class="t">                <span class="com"># While generating, we keep generating logits, throw away all but the final logits,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2159" href="#t2159">2159</a></span><span class="t">                <span class="com"># and then use those logits to sample from the distribution We keep adding the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2160" href="#t2160">2160</a></span><span class="t">                <span class="com"># sampled tokens to the end of tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2161" href="#t2161">2161</a></span><span class="t">                <span class="key">if</span> <span class="nam">use_past_kv_cache</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2161&#x202F;&#x219B;&#x202F;2182</span><span class="annotate long">line 2161 didn't jump to line 2182, because the condition on line 2161 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t2162" href="#t2162">2162</a></span><span class="t">                    <span class="com"># We just take the final tokens, as a [batch, 1] tensor</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2163" href="#t2163">2163</a></span><span class="t">                    <span class="key">if</span> <span class="nam">index</span> <span class="op">></span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2164" href="#t2164">2164</a></span><span class="t">                        <span class="nam">logits</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2165" href="#t2165">2165</a></span><span class="t">                            <span class="nam">tokens</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2166" href="#t2166">2166</a></span><span class="t">                            <span class="nam">return_type</span><span class="op">=</span><span class="str">"logits"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2167" href="#t2167">2167</a></span><span class="t">                            <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2168" href="#t2168">2168</a></span><span class="t">                            <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2169" href="#t2169">2169</a></span><span class="t">                            <span class="nam">past_kv_cache</span><span class="op">=</span><span class="nam">past_kv_cache</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2170" href="#t2170">2170</a></span><span class="t">                        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2171" href="#t2171">2171</a></span><span class="t">                    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2172" href="#t2172">2172</a></span><span class="t">                        <span class="nam">logits</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2173" href="#t2173">2173</a></span><span class="t">                            <span class="nam">tokens</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2174" href="#t2174">2174</a></span><span class="t">                            <span class="nam">return_type</span><span class="op">=</span><span class="str">"logits"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2175" href="#t2175">2175</a></span><span class="t">                            <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2176" href="#t2176">2176</a></span><span class="t">                            <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2177" href="#t2177">2177</a></span><span class="t">                            <span class="nam">past_kv_cache</span><span class="op">=</span><span class="nam">past_kv_cache</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2178" href="#t2178">2178</a></span><span class="t">                        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2179" href="#t2179">2179</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2180" href="#t2180">2180</a></span><span class="t">                    <span class="com"># We input the entire sequence, as a [batch, pos] tensor, since we aren't using</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2181" href="#t2181">2181</a></span><span class="t">                    <span class="com"># the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2182" href="#t2182">2182</a></span><span class="t">                    <span class="nam">logits</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">forward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2183" href="#t2183">2183</a></span><span class="t">                        <span class="nam">tokens</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2184" href="#t2184">2184</a></span><span class="t">                        <span class="nam">return_type</span><span class="op">=</span><span class="str">"logits"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2185" href="#t2185">2185</a></span><span class="t">                        <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2186" href="#t2186">2186</a></span><span class="t">                        <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2187" href="#t2187">2187</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2188" href="#t2188">2188</a></span><span class="t">                <span class="nam">final_logits</span> <span class="op">=</span> <span class="nam">logits</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2189" href="#t2189">2189</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2190" href="#t2190">2190</a></span><span class="t">                <span class="key">if</span> <span class="nam">do_sample</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2190&#x202F;&#x219B;&#x202F;2191</span><span class="annotate long">line 2190 didn't jump to line 2191, because the condition on line 2190 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2191" href="#t2191">2191</a></span><span class="t">                    <span class="nam">sampled_tokens</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">sample_logits</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2192" href="#t2192">2192</a></span><span class="t">                        <span class="nam">final_logits</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2193" href="#t2193">2193</a></span><span class="t">                        <span class="nam">top_k</span><span class="op">=</span><span class="nam">top_k</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2194" href="#t2194">2194</a></span><span class="t">                        <span class="nam">top_p</span><span class="op">=</span><span class="nam">top_p</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2195" href="#t2195">2195</a></span><span class="t">                        <span class="nam">temperature</span><span class="op">=</span><span class="nam">temperature</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2196" href="#t2196">2196</a></span><span class="t">                        <span class="nam">freq_penalty</span><span class="op">=</span><span class="nam">freq_penalty</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2197" href="#t2197">2197</a></span><span class="t">                        <span class="nam">tokens</span><span class="op">=</span><span class="nam">tokens</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2198" href="#t2198">2198</a></span><span class="t">                    <span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2199" href="#t2199">2199</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2200" href="#t2200">2200</a></span><span class="t">                    <span class="nam">sampled_tokens</span> <span class="op">=</span> <span class="nam">final_logits</span><span class="op">.</span><span class="nam">argmax</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2201" href="#t2201">2201</a></span><span class="t">                        <span class="nam">devices</span><span class="op">.</span><span class="nam">get_device_for_block_index</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2202" href="#t2202">2202</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2203" href="#t2203">2203</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2204" href="#t2204">2204</a></span><span class="t">                <span class="key">if</span> <span class="nam">stop_at_eos</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2204&#x202F;&#x219B;&#x202F;2216</span><span class="annotate long">line 2204 didn't jump to line 2216, because the condition on line 2204 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t2205" href="#t2205">2205</a></span><span class="t">                    <span class="com"># For all unfinished sequences, add on the next token. If a sequence was</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2206" href="#t2206">2206</a></span><span class="t">                    <span class="com"># finished, throw away the generated token and add eos_token_for_padding</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2207" href="#t2207">2207</a></span><span class="t">                    <span class="com"># instead.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2208" href="#t2208">2208</a></span><span class="t">                    <span class="nam">sampled_tokens</span><span class="op">[</span><span class="nam">finished_sequences</span><span class="op">]</span> <span class="op">=</span> <span class="nam">eos_token_for_padding</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2209" href="#t2209">2209</a></span><span class="t">                    <span class="nam">finished_sequences</span><span class="op">.</span><span class="nam">logical_or_</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2210" href="#t2210">2210</a></span><span class="t">                        <span class="nam">torch</span><span class="op">.</span><span class="nam">isin</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2211" href="#t2211">2211</a></span><span class="t">                            <span class="nam">sampled_tokens</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2212" href="#t2212">2212</a></span><span class="t">                            <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="nam">stop_tokens</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2213" href="#t2213">2213</a></span><span class="t">                        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2214" href="#t2214">2214</a></span><span class="t">                    <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2215" href="#t2215">2215</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2216" href="#t2216">2216</a></span><span class="t">                <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="op">[</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">sampled_tokens</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">(</span><span class="op">-</span><span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2217" href="#t2217">2217</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2218" href="#t2218">2218</a></span><span class="t">                <span class="key">if</span> <span class="nam">stop_at_eos</span> <span class="key">and</span> <span class="nam">finished_sequences</span><span class="op">.</span><span class="nam">all</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2218&#x202F;&#x219B;&#x202F;2219</span><span class="annotate long">line 2218 didn't jump to line 2219, because the condition on line 2218 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2219" href="#t2219">2219</a></span><span class="t">                    <span class="key">break</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2220" href="#t2220">2220</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2221" href="#t2221">2221</a></span><span class="t">            <span class="key">if</span> <span class="nam">return_type</span> <span class="op">==</span> <span class="str">"str"</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2221&#x202F;&#x219B;&#x202F;2229</span><span class="annotate long">line 2221 didn't jump to line 2229, because the condition on line 2221 was never false</span></span></p>
    <p class="par run show_par"><span class="n"><a id="t2222" href="#t2222">2222</a></span><span class="t">                <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">default_prepend_bos</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2222&#x202F;&#x219B;&#x202F;2224</span><span class="annotate long">line 2222 didn't jump to line 2224, because the condition on line 2222 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t2223" href="#t2223">2223</a></span><span class="t">                    <span class="com"># If we prepended a BOS token, remove it when returning output.</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2224" href="#t2224">2224</a></span><span class="t">                    <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">decode</span><span class="op">(</span><span class="nam">tokens</span><span class="op">[</span><span class="num">0</span><span class="op">,</span> <span class="num">1</span><span class="op">:</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2225" href="#t2225">2225</a></span><span class="t">                <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2226" href="#t2226">2226</a></span><span class="t">                    <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">tokenizer</span><span class="op">.</span><span class="nam">decode</span><span class="op">(</span><span class="nam">tokens</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2227" href="#t2227">2227</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2228" href="#t2228">2228</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2229" href="#t2229">2229</a></span><span class="t">                <span class="key">return</span> <span class="nam">tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2230" href="#t2230">2230</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2231" href="#t2231">2231</a></span><span class="t">    <span class="com"># Give access to all weights as properties.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2232" href="#t2232">2232</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2233" href="#t2233">2233</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_U</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_model d_vocab"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2234" href="#t2234">2234</a></span><span class="t">        <span class="str">"""Convenience to get the unembedding matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2235" href="#t2235">2235</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2236" href="#t2236">2236</a></span><span class="t"><span class="str">        I.e. the linear map from the final residual stream to the output logits).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2237" href="#t2237">2237</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2238" href="#t2238">2238</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">unembed</span><span class="op">.</span><span class="nam">W_U</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2239" href="#t2239">2239</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2240" href="#t2240">2240</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2241" href="#t2241">2241</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_U</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_vocab"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2242" href="#t2242">2242</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">unembed</span><span class="op">.</span><span class="nam">b_U</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2243" href="#t2243">2243</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2244" href="#t2244">2244</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2245" href="#t2245">2245</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_E</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_vocab d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2246" href="#t2246">2246</a></span><span class="t">        <span class="str">"""Convenience to get the embedding matrix."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2247" href="#t2247">2247</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">embed</span><span class="op">.</span><span class="nam">W_E</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2248" href="#t2248">2248</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2249" href="#t2249">2249</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2250" href="#t2250">2250</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_pos</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_ctx d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2251" href="#t2251">2251</a></span><span class="t">        <span class="str">"""Convenience function to get the positional embedding.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2252" href="#t2252">2252</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2253" href="#t2253">2253</a></span><span class="t"><span class="str">        Only works on models with absolute positional embeddings!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2254" href="#t2254">2254</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2255" href="#t2255">2255</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">pos_embed</span><span class="op">.</span><span class="nam">W_pos</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2256" href="#t2256">2256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2257" href="#t2257">2257</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2258" href="#t2258">2258</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_E_pos</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_vocab+n_ctx d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2259" href="#t2259">2259</a></span><span class="t">        <span class="str">"""Concatenated W_E and W_pos.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2260" href="#t2260">2260</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2261" href="#t2261">2261</a></span><span class="t"><span class="str">        Used as a full (overcomplete) basis of the input space, useful for full QK and full OV</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2262" href="#t2262">2262</a></span><span class="t"><span class="str">        circuits.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2263" href="#t2263">2263</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2264" href="#t2264">2264</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="op">[</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_E</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_pos</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2265" href="#t2265">2265</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2266" href="#t2266">2266</a></span><span class="t">    <span class="com"># Layer-specific weights are stacked into one massive tensor and given as properties for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2267" href="#t2267">2267</a></span><span class="t">    <span class="com"># convenience and a cache is used to avoid repeated computation. Often a useful convenience when</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2268" href="#t2268">2268</a></span><span class="t">    <span class="com"># we want to do analysis on weights across all layers. If GPU memory is a bottleneck, don't use</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2269" href="#t2269">2269</a></span><span class="t">    <span class="com"># these properties!</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2270" href="#t2270">2270</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2271" href="#t2271">2271</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2272" href="#t2272">2272</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_K</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_model d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2273" href="#t2273">2273</a></span><span class="t">        <span class="str">"""Stack the key weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2274" href="#t2274">2274</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">W_K</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2274&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2274&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2274 didn't run the list comprehension on line 2274, 2) line 2274 didn't return from function 'W_K', because the return on line 2274 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2275" href="#t2275">2275</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2276" href="#t2276">2276</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2277" href="#t2277">2277</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_Q</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_model d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2278" href="#t2278">2278</a></span><span class="t">        <span class="str">"""Stack the query weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2279" href="#t2279">2279</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">W_Q</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2279&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2279&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2279 didn't run the list comprehension on line 2279, 2) line 2279 didn't return from function 'W_Q', because the return on line 2279 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2280" href="#t2280">2280</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2281" href="#t2281">2281</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2282" href="#t2282">2282</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_V</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_model d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2283" href="#t2283">2283</a></span><span class="t">        <span class="str">"""Stack the value weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2284" href="#t2284">2284</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">W_V</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2284&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2284&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2284 didn't run the list comprehension on line 2284, 2) line 2284 didn't return from function 'W_V', because the return on line 2284 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2285" href="#t2285">2285</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2286" href="#t2286">2286</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2287" href="#t2287">2287</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_O</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_head d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2288" href="#t2288">2288</a></span><span class="t">        <span class="str">"""Stack the attn output weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2289" href="#t2289">2289</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">W_O</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2289&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2289&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2289 didn't run the list comprehension on line 2289, 2) line 2289 didn't return from function 'W_O', because the return on line 2289 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2290" href="#t2290">2290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2291" href="#t2291">2291</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2292" href="#t2292">2292</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_in</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_model d_mlp"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2293" href="#t2293">2293</a></span><span class="t">        <span class="str">"""Stack the MLP input weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2294" href="#t2294">2294</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">W_in</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2294&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2294&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2294 didn't run the list comprehension on line 2294, 2) line 2294 didn't return from function 'W_in', because the return on line 2294 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2295" href="#t2295">2295</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2296" href="#t2296">2296</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2297" href="#t2297">2297</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_gate</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_model d_mlp"</span><span class="op">]</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2298" href="#t2298">2298</a></span><span class="t">        <span class="str">"""Stack the MLP gate weights across all layers.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2299" href="#t2299">2299</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2300" href="#t2300">2300</a></span><span class="t"><span class="str">        Only works for models with gated MLPs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2301" href="#t2301">2301</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2302" href="#t2302">2302</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">gated_mlp</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2303" href="#t2303">2303</a></span><span class="t">            <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">W_gate</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2304" href="#t2304">2304</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2305" href="#t2305">2305</a></span><span class="t">            <span class="key">return</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2306" href="#t2306">2306</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2307" href="#t2307">2307</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2308" href="#t2308">2308</a></span><span class="t">    <span class="key">def</span> <span class="nam">W_out</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_mlp d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2309" href="#t2309">2309</a></span><span class="t">        <span class="str">"""Stack the MLP output weights across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2310" href="#t2310">2310</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">W_out</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2310&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2310&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2310 didn't run the list comprehension on line 2310, 2) line 2310 didn't return from function 'W_out', because the return on line 2310 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2311" href="#t2311">2311</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2312" href="#t2312">2312</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2313" href="#t2313">2313</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_K</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2314" href="#t2314">2314</a></span><span class="t">        <span class="str">"""Stack the key biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2315" href="#t2315">2315</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_K</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2315&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2315&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2315 didn't run the list comprehension on line 2315, 2) line 2315 didn't return from function 'b_K', because the return on line 2315 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2316" href="#t2316">2316</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2317" href="#t2317">2317</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2318" href="#t2318">2318</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_Q</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2319" href="#t2319">2319</a></span><span class="t">        <span class="str">"""Stack the query biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2320" href="#t2320">2320</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_Q</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2320&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2320&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2320 didn't run the list comprehension on line 2320, 2) line 2320 didn't return from function 'b_Q', because the return on line 2320 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2321" href="#t2321">2321</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2322" href="#t2322">2322</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2323" href="#t2323">2323</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_V</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads d_head"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2324" href="#t2324">2324</a></span><span class="t">        <span class="str">"""Stack the value biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2325" href="#t2325">2325</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_V</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2325&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2325&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2325 didn't run the list comprehension on line 2325, 2) line 2325 didn't return from function 'b_V', because the return on line 2325 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2326" href="#t2326">2326</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2327" href="#t2327">2327</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2328" href="#t2328">2328</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_O</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2329" href="#t2329">2329</a></span><span class="t">        <span class="str">"""Stack the attn output biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2330" href="#t2330">2330</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_O</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2330&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2330&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2330 didn't run the list comprehension on line 2330, 2) line 2330 didn't return from function 'b_O', because the return on line 2330 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2331" href="#t2331">2331</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2332" href="#t2332">2332</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2333" href="#t2333">2333</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_in</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_mlp"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2334" href="#t2334">2334</a></span><span class="t">        <span class="str">"""Stack the MLP input biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2335" href="#t2335">2335</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">b_in</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2335&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2335&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2335 didn't run the list comprehension on line 2335, 2) line 2335 didn't return from function 'b_in', because the return on line 2335 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2336" href="#t2336">2336</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2337" href="#t2337">2337</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2338" href="#t2338">2338</a></span><span class="t">    <span class="key">def</span> <span class="nam">b_out</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2339" href="#t2339">2339</a></span><span class="t">        <span class="str">"""Stack the MLP output biases across all layers."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2340" href="#t2340">2340</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">block</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">b_out</span> <span class="key">for</span> <span class="nam">block</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"><span class="annotate short">2340&#x202F;&#x219B;&#x202F;exit,&nbsp;&nbsp; 2340&#x202F;&#x219B;&#x202F;exit</span><span class="annotate long">2 missed branches: 1) line 2340 didn't run the list comprehension on line 2340, 2) line 2340 didn't return from function 'b_out', because the return on line 2340 wasn't executed</span></span></p>
    <p class="pln"><span class="n"><a id="t2341" href="#t2341">2341</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2342" href="#t2342">2342</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2343" href="#t2343">2343</a></span><span class="t">    <span class="key">def</span> <span class="nam">QK</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2344" href="#t2344">2344</a></span><span class="t">        <span class="key">return</span> <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_Q</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_K</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="op">-</span><span class="num">2</span><span class="op">,</span> <span class="op">-</span><span class="num">1</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2345" href="#t2345">2345</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2346" href="#t2346">2346</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2347" href="#t2347">2347</a></span><span class="t">    <span class="key">def</span> <span class="nam">OV</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2348" href="#t2348">2348</a></span><span class="t">        <span class="key">return</span> <span class="nam">FactoredMatrix</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">W_V</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">W_O</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2349" href="#t2349">2349</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2350" href="#t2350">2350</a></span><span class="t">    <span class="com"># Various utility functions</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2351" href="#t2351">2351</a></span><span class="t">    <span class="key">def</span> <span class="nam">accumulated_bias</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2352" href="#t2352">2352</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="nam">layer</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span> <span class="nam">mlp_input</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span> <span class="nam">include_mlp_biases</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2353" href="#t2353">2353</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2354" href="#t2354">2354</a></span><span class="t">        <span class="str">"""Accumulated Bias.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2355" href="#t2355">2355</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2356" href="#t2356">2356</a></span><span class="t"><span class="str">        Returns the accumulated bias from all layer outputs (ie the b_Os and b_outs), up to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2357" href="#t2357">2357</a></span><span class="t"><span class="str">        input of layer L.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2358" href="#t2358">2358</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2359" href="#t2359">2359</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2360" href="#t2360">2360</a></span><span class="t"><span class="str">            layer (int): Layer number, in [0, n_layers]. layer==0 means no layers, layer==n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2361" href="#t2361">2361</a></span><span class="t"><span class="str">                means all layers.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2362" href="#t2362">2362</a></span><span class="t"><span class="str">            mlp_input (bool): If True, we take the bias up to the input of the MLP</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2363" href="#t2363">2363</a></span><span class="t"><span class="str">                of layer L (ie we include the bias from the attention output of the current layer,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2364" href="#t2364">2364</a></span><span class="t"><span class="str">                otherwise just biases from previous layers)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2365" href="#t2365">2365</a></span><span class="t"><span class="str">            include_mlp_biases (bool): Whether to include the biases of MLP layers. Often useful to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2366" href="#t2366">2366</a></span><span class="t"><span class="str">                have as False if we're expanding attn_out into individual heads, but keeping mlp_out</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2367" href="#t2367">2367</a></span><span class="t"><span class="str">                as is.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2368" href="#t2368">2368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2369" href="#t2369">2369</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2370" href="#t2370">2370</a></span><span class="t"><span class="str">            bias (torch.Tensor): [d_model], accumulated bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2371" href="#t2371">2371</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2372" href="#t2372">2372</a></span><span class="t">        <span class="nam">accumulated_bias</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_model</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2373" href="#t2373">2373</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2374" href="#t2374">2374</a></span><span class="t">        <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2375" href="#t2375">2375</a></span><span class="t">            <span class="nam">accumulated_bias</span> <span class="op">+=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2376" href="#t2376">2376</a></span><span class="t">            <span class="key">if</span> <span class="nam">include_mlp_biases</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2377" href="#t2377">2377</a></span><span class="t">                <span class="nam">accumulated_bias</span> <span class="op">+=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">b_out</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t2378" href="#t2378">2378</a></span><span class="t">        <span class="key">if</span> <span class="nam">mlp_input</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">2378&#x202F;&#x219B;&#x202F;2379</span><span class="annotate long">line 2378 didn't jump to line 2379, because the condition on line 2378 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2379" href="#t2379">2379</a></span><span class="t">            <span class="key">assert</span> <span class="nam">layer</span> <span class="op">&lt;</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">,</span> <span class="str">"Cannot include attn_bias from beyond the final layer"</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2380" href="#t2380">2380</a></span><span class="t">            <span class="nam">accumulated_bias</span> <span class="op">+=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">blocks</span><span class="op">[</span><span class="nam">layer</span><span class="op">]</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">b_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2381" href="#t2381">2381</a></span><span class="t">        <span class="key">return</span> <span class="nam">accumulated_bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2382" href="#t2382">2382</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2383" href="#t2383">2383</a></span><span class="t">    <span class="key">def</span> <span class="nam">all_composition_scores</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2384" href="#t2384">2384</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span> <span class="nam">mode</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2385" href="#t2385">2385</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"n_layers n_heads n_layers n_heads"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2386" href="#t2386">2386</a></span><span class="t">        <span class="str">"""All Composition Scores.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2387" href="#t2387">2387</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2388" href="#t2388">2388</a></span><span class="t"><span class="str">        Returns the Composition scores for all pairs of heads, as a L1, H1, L2, H2 tensor (which is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2389" href="#t2389">2389</a></span><span class="t"><span class="str">        upper triangular on the first and third axes).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2390" href="#t2390">2390</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2391" href="#t2391">2391</a></span><span class="t"><span class="str">        See</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2392" href="#t2392">2392</a></span><span class="t"><span class="str">        https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2393" href="#t2393">2393</a></span><span class="t"><span class="str">        for three metrics used.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2394" href="#t2394">2394</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2395" href="#t2395">2395</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2396" href="#t2396">2396</a></span><span class="t"><span class="str">            mode (str): One of ["Q", "K", "V"], the mode to use for the composition score.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2397" href="#t2397">2397</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2398" href="#t2398">2398</a></span><span class="t">        <span class="nam">left</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">OV</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2399" href="#t2399">2399</a></span><span class="t">        <span class="key">if</span> <span class="nam">mode</span> <span class="op">==</span> <span class="str">"Q"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2400" href="#t2400">2400</a></span><span class="t">            <span class="nam">right</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">QK</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2401" href="#t2401">2401</a></span><span class="t">        <span class="key">elif</span> <span class="nam">mode</span> <span class="op">==</span> <span class="str">"K"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2402" href="#t2402">2402</a></span><span class="t">            <span class="nam">right</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">QK</span><span class="op">.</span><span class="nam">T</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2403" href="#t2403">2403</a></span><span class="t">        <span class="key">elif</span> <span class="nam">mode</span> <span class="op">==</span> <span class="str">"V"</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2404" href="#t2404">2404</a></span><span class="t">            <span class="nam">right</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">OV</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2405" href="#t2405">2405</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2406" href="#t2406">2406</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">f"mode must be one of ['Q', 'K', 'V'] not {mode}"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2407" href="#t2407">2407</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2408" href="#t2408">2408</a></span><span class="t">        <span class="nam">scores</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">composition_scores</span><span class="op">(</span><span class="nam">left</span><span class="op">,</span> <span class="nam">right</span><span class="op">,</span> <span class="nam">broadcast_dims</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2409" href="#t2409">2409</a></span><span class="t">        <span class="com"># Mask scores to be zero for all pairs with the right head in the same layer or earlier</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2410" href="#t2410">2410</a></span><span class="t">        <span class="com"># layer than the left head.</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2411" href="#t2411">2411</a></span><span class="t">        <span class="nam">mask</span> <span class="op">=</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2412" href="#t2412">2412</a></span><span class="t">            <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">[</span><span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2413" href="#t2413">2413</a></span><span class="t">            <span class="op">&lt;</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">[</span><span class="key">None</span><span class="op">,</span> <span class="key">None</span><span class="op">,</span> <span class="op">:</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2414" href="#t2414">2414</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2415" href="#t2415">2415</a></span><span class="t">        <span class="nam">scores</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">mask</span><span class="op">,</span> <span class="nam">scores</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">scores</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2416" href="#t2416">2416</a></span><span class="t">        <span class="key">return</span> <span class="nam">scores</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2417" href="#t2417">2417</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2418" href="#t2418">2418</a></span><span class="t">    <span class="key">def</span> <span class="nam">all_head_labels</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2419" href="#t2419">2419</a></span><span class="t">        <span class="str">"""Returns a list of all head names in the model."""</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2420" href="#t2420">2420</a></span><span class="t">        <span class="key">return</span> <span class="op">[</span><span class="str">f"L{l}H{h}"</span> <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span> <span class="key">for</span> <span class="nam">h</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2421" href="#t2421">2421</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2422" href="#t2422">2422</a></span><span class="t">    <span class="key">def</span> <span class="nam">load_sample_training_dataset</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2423" href="#t2423">2423</a></span><span class="t">        <span class="str">"""Load Sample Training Dataset.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2424" href="#t2424">2424</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2425" href="#t2425">2425</a></span><span class="t"><span class="str">        Helper function to load in a 10K-20K dataset of elements from the model's training data</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2426" href="#t2426">2426</a></span><span class="t"><span class="str">        distribution.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2427" href="#t2427">2427</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2428" href="#t2428">2428</a></span><span class="t"><span class="str">        Wrapper around utils.get_dataset, which identifies the appropriate dataset the pretrained</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2429" href="#t2429">2429</a></span><span class="t"><span class="str">        models. Each dataset has a 'text' field, which contains the relevant info, some have several</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2430" href="#t2430">2430</a></span><span class="t"><span class="str">        meta data fields.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2431" href="#t2431">2431</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2432" href="#t2432">2432</a></span><span class="t"><span class="str">        Kwargs will be passed to utils.get_dataset (e.g. cache_dir to set download location)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2433" href="#t2433">2433</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2434" href="#t2434">2434</a></span><span class="t"><span class="str">        Notes:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2435" href="#t2435">2435</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2436" href="#t2436">2436</a></span><span class="t"><span class="str">        - PT-2's training data is not open source. OpenWebText is a replication (links with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2437" href="#t2437">2437</a></span><span class="t"><span class="str">            >3 karma on Reddit)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2438" href="#t2438">2438</a></span><span class="t"><span class="str">        - OPT's training data is not open source, and is a mess of different things that is hard to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2439" href="#t2439">2439</a></span><span class="t"><span class="str">          replicate. I default to the Pile, which covers some of it, but imperfectly.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2440" href="#t2440">2440</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2441" href="#t2441">2441</a></span><span class="t"><span class="str">        (Some models will have actually been trained on the data supplied here, for some it's from</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2442" href="#t2442">2442</a></span><span class="t"><span class="str">        the validation set).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2443" href="#t2443">2443</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2444" href="#t2444">2444</a></span><span class="t">        <span class="nam">model_dataset_map</span> <span class="op">=</span> <span class="op">{</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2445" href="#t2445">2445</a></span><span class="t">            <span class="str">"neel"</span><span class="op">:</span> <span class="str">"c4_code"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2446" href="#t2446">2446</a></span><span class="t">            <span class="str">"neel-solu-old"</span><span class="op">:</span> <span class="str">"pile"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2447" href="#t2447">2447</a></span><span class="t">            <span class="str">"GPT2LMHeadModel"</span><span class="op">:</span> <span class="str">"openwebtext"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2448" href="#t2448">2448</a></span><span class="t">            <span class="str">"GPTNeoForCausalLM"</span><span class="op">:</span> <span class="str">"pile"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2449" href="#t2449">2449</a></span><span class="t">            <span class="str">"GPTNeoXForCausalLM"</span><span class="op">:</span> <span class="str">"pile"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2450" href="#t2450">2450</a></span><span class="t">            <span class="str">"GPTJForCausalLM"</span><span class="op">:</span> <span class="str">"pile"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2451" href="#t2451">2451</a></span><span class="t">            <span class="str">"OPTForCausalLM"</span><span class="op">:</span> <span class="str">"pile"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2452" href="#t2452">2452</a></span><span class="t">        <span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2453" href="#t2453">2453</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">original_architecture</span> <span class="key">in</span> <span class="nam">model_dataset_map</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2454" href="#t2454">2454</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span> <span class="op">=</span> <span class="nam">utils</span><span class="op">.</span><span class="nam">get_dataset</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2455" href="#t2455">2455</a></span><span class="t">                <span class="nam">model_dataset_map</span><span class="op">[</span><span class="nam">self</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">original_architecture</span><span class="op">]</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2456" href="#t2456">2456</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2457" href="#t2457">2457</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2458" href="#t2458">2458</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2459" href="#t2459">2459</a></span><span class="t">                <span class="str">f"We do not have an available dataset for the relevant model: {self.cfg.original_architecture}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2460" href="#t2460">2460</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2461" href="#t2461">2461</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2462" href="#t2462">2462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2463" href="#t2463">2463</a></span><span class="t">    <span class="key">def</span> <span class="nam">sample_datapoint</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2464" href="#t2464">2464</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2465" href="#t2465">2465</a></span><span class="t">        <span class="nam">tokenize</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2466" href="#t2466">2466</a></span><span class="t">        <span class="nam">prepend_bos</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">bool</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2467" href="#t2467">2467</a></span><span class="t">        <span class="nam">padding_side</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Literal</span><span class="op">[</span><span class="str">"left"</span><span class="op">,</span> <span class="str">"right"</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="nam">USE_DEFAULT_VALUE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2468" href="#t2468">2468</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"1 pos"</span><span class="op">]</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2469" href="#t2469">2469</a></span><span class="t">        <span class="str">"""Sample Data Point from Dataset.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2470" href="#t2470">2470</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2471" href="#t2471">2471</a></span><span class="t"><span class="str">        Helper function to randomly sample a data point from self.dataset, a small dataset from the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2472" href="#t2472">2472</a></span><span class="t"><span class="str">        data distribution the model was trained on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2473" href="#t2473">2473</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2474" href="#t2474">2474</a></span><span class="t"><span class="str">        Implicitly calls self.load_sample_training_dataset if it hasn't already been called. Only</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2475" href="#t2475">2475</a></span><span class="t"><span class="str">        works for pretrained models with an associated dataset. But you can manually replace</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2476" href="#t2476">2476</a></span><span class="t"><span class="str">        self.dataset with a dataset of your choice if you want.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2477" href="#t2477">2477</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2478" href="#t2478">2478</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2479" href="#t2479">2479</a></span><span class="t"><span class="str">            tokenize (bool): Whether to return tokens (instead of text). Defaults to False. Note</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2480" href="#t2480">2480</a></span><span class="t"><span class="str">                that the returned tokens will be automatically truncated to the model's max context</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2481" href="#t2481">2481</a></span><span class="t"><span class="str">                size.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2482" href="#t2482">2482</a></span><span class="t"><span class="str">            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2483" href="#t2483">2483</a></span><span class="t"><span class="str">                the BOS token to the input (applicable when input is a string). Defaults to None,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2484" href="#t2484">2484</a></span><span class="t"><span class="str">                implying usage of self.cfg.default_prepend_bos (default is True unless specified</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2485" href="#t2485">2485</a></span><span class="t"><span class="str">                otherwise). Pass True or False to override the default.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2486" href="#t2486">2486</a></span><span class="t"><span class="str">            padding_side (Union[Literal["left", "right"], None], optional): Overrides</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2487" href="#t2487">2487</a></span><span class="t"><span class="str">                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2488" href="#t2488">2488</a></span><span class="t"><span class="str">                strings of different lengths.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2489" href="#t2489">2489</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2490" href="#t2490">2490</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2491" href="#t2491">2491</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">load_sample_training_dataset</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2492" href="#t2492">2492</a></span><span class="t">        <span class="key">assert</span> <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2493" href="#t2493">2493</a></span><span class="t">        <span class="nam">sample_dataset_size</span> <span class="op">=</span> <span class="nam">len</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2494" href="#t2494">2494</a></span><span class="t">        <span class="nam">index</span> <span class="op">=</span> <span class="nam">np</span><span class="op">.</span><span class="nam">random</span><span class="op">.</span><span class="nam">randint</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">sample_dataset_size</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2495" href="#t2495">2495</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">tokenize</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t2496" href="#t2496">2496</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span><span class="op">[</span><span class="nam">index</span><span class="op">]</span><span class="op">[</span><span class="str">"text"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2497" href="#t2497">2497</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2498" href="#t2498">2498</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to_tokens</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2499" href="#t2499">2499</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">dataset</span><span class="op">[</span><span class="nam">index</span><span class="op">]</span><span class="op">[</span><span class="str">"text"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2500" href="#t2500">2500</a></span><span class="t">                <span class="nam">prepend_bos</span><span class="op">=</span><span class="nam">prepend_bos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2501" href="#t2501">2501</a></span><span class="t">                <span class="nam">padding_side</span><span class="op">=</span><span class="nam">padding_side</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2502" href="#t2502">2502</a></span><span class="t">                <span class="nam">truncate</span><span class="op">=</span><span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2503" href="#t2503">2503</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
</main>
<footer>
    <div class="content">
        <p>
            <a id="prevFileLink" class="nav" href="d_af97b5493da09a14_HookedEncoderDecoder_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_af97b5493da09a14_HookedTransformerConfig_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2024-12-31 02:13 +0000
        </p>
    </div>
</footer>
</body>
</html>
