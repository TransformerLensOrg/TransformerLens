<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Coverage for transformer_lens/pretrained/weight_conversions/neox.py: 100%</title>
    <link rel="icon" sizes="32x32" href="favicon_32.png">
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="coverage_html.js" defer></script>
</head>
<body class="pyfile">
<header>
    <div class="content">
        <h1>
            <span class="text">Coverage for </span><b>transformer_lens/pretrained/weight_conversions/neox.py</b>:
            <span class="pc_cov">100%</span>
        </h1>
        <aside id="help_panel_wrapper">
            <input id="help_panel_state" type="checkbox">
            <label for="help_panel_state">
                <img id="keyboard_icon" src="keybd_closed.png" alt="Show/hide keyboard shortcuts" />
            </label>
            <div id="help_panel">
                <p class="legend">Shortcuts on this page</p>
                <div class="keyhelp">
                    <p>
                        <kbd>r</kbd>
                        <kbd>m</kbd>
                        <kbd>x</kbd>
                        <kbd>p</kbd>
                        &nbsp; toggle line displays
                    </p>
                    <p>
                        <kbd>j</kbd>
                        <kbd>k</kbd>
                        &nbsp; next/prev highlighted chunk
                    </p>
                    <p>
                        <kbd>0</kbd> &nbsp; (zero) top of page
                    </p>
                    <p>
                        <kbd>1</kbd> &nbsp; (one) first highlighted chunk
                    </p>
                    <p>
                        <kbd>[</kbd>
                        <kbd>]</kbd>
                        &nbsp; prev/next file
                    </p>
                    <p>
                        <kbd>u</kbd> &nbsp; up to the index
                    </p>
                    <p>
                        <kbd>?</kbd> &nbsp; show/hide this help
                    </p>
                </div>
            </div>
        </aside>
        <h2>
            <span class="text">34 statements &nbsp;</span>
            <button type="button" class="run button_toggle_run" value="run" data-shortcut="r" title="Toggle lines run">34<span class="text"> run</span></button>
            <button type="button" class="mis show_mis button_toggle_mis" value="mis" data-shortcut="m" title="Toggle lines missing">0<span class="text"> missing</span></button>
            <button type="button" class="exc show_exc button_toggle_exc" value="exc" data-shortcut="x" title="Toggle lines excluded">0<span class="text"> excluded</span></button>
            <button type="button" class="par run show_par button_toggle_par" value="par" data-shortcut="p" title="Toggle lines partially run">0<span class="text"> partial</span></button>
        </h2>
        <p class="text">
            <a id="prevFileLink" class="nav" href="d_c1ea89878f9b2ac7_neo_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_c1ea89878f9b2ac7_opt_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2024-10-16 00:05 +0000
        </p>
        <aside class="hidden">
            <button type="button" class="button_next_chunk" data-shortcut="j"/>
            <button type="button" class="button_prev_chunk" data-shortcut="k"/>
            <button type="button" class="button_top_of_page" data-shortcut="0"/>
            <button type="button" class="button_first_chunk" data-shortcut="1"/>
            <button type="button" class="button_prev_file" data-shortcut="["/>
            <button type="button" class="button_next_file" data-shortcut="]"/>
            <button type="button" class="button_to_index" data-shortcut="u"/>
            <button type="button" class="button_show_hide_help" data-shortcut="?"/>
        </aside>
    </div>
</header>
<main id="source">
    <p class="run"><span class="n"><a id="t1" href="#t1">1</a></span><span class="t"><span class="key">import</span> <span class="nam">einops</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t2" href="#t2">2</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t3" href="#t3">3</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t4" href="#t4">4</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">HookedTransformerConfig</span> <span class="key">import</span> <span class="nam">HookedTransformerConfig</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t5" href="#t5">5</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t6" href="#t6">6</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t7" href="#t7">7</a></span><span class="t"><span class="key">def</span> <span class="nam">convert_neox_weights</span><span class="op">(</span><span class="nam">neox</span><span class="op">,</span> <span class="nam">cfg</span><span class="op">:</span> <span class="nam">HookedTransformerConfig</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t8" href="#t8">8</a></span><span class="t">    <span class="nam">state_dict</span> <span class="op">=</span> <span class="op">{</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t9" href="#t9">9</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t10" href="#t10">10</a></span><span class="t">    <span class="nam">state_dict</span><span class="op">[</span><span class="str">"embed.W_E"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">embed_in</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t11" href="#t11">11</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t12" href="#t12">12</a></span><span class="t">    <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t13" href="#t13">13</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.w"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">input_layernorm</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t14" href="#t14">14</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln1.b"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">input_layernorm</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t15" href="#t15">15</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t16" href="#t16">16</a></span><span class="t">        <span class="com"># For some inexplicable reason, NeoX both uses the concatenated QKV</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t17" href="#t17">17</a></span><span class="t">        <span class="com"># matmul of GPT-2 (afaict this has a neglible performance impact) AND</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t18" href="#t18">18</a></span><span class="t">        <span class="com"># has the flattened axis in the DIFFERENT order of (head_index qkv</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t19" href="#t19">19</a></span><span class="t">        <span class="com"># d_head) - this took me an hour to debug...</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t20" href="#t20">20</a></span><span class="t">        <span class="nam">W</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">attention</span><span class="op">.</span><span class="nam">query_key_value</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t21" href="#t21">21</a></span><span class="t">        <span class="nam">W</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span><span class="nam">W</span><span class="op">,</span> <span class="str">"(i qkv h) m->qkv i m h"</span><span class="op">,</span> <span class="nam">i</span><span class="op">=</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span> <span class="nam">qkv</span><span class="op">=</span><span class="num">3</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t22" href="#t22">22</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t23" href="#t23">23</a></span><span class="t">        <span class="com"># Fold in layer norm weights</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t24" href="#t24">24</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W</span><span class="op">[</span><span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t25" href="#t25">25</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_K"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W</span><span class="op">[</span><span class="num">1</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t26" href="#t26">26</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W</span><span class="op">[</span><span class="num">2</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t27" href="#t27">27</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t28" href="#t28">28</a></span><span class="t">        <span class="nam">qkv_bias</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">attention</span><span class="op">.</span><span class="nam">query_key_value</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t29" href="#t29">29</a></span><span class="t">        <span class="nam">qkv_bias</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t30" href="#t30">30</a></span><span class="t">            <span class="nam">qkv_bias</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t31" href="#t31">31</a></span><span class="t">            <span class="str">"(index qkv head)->qkv index head"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t32" href="#t32">32</a></span><span class="t">            <span class="nam">qkv</span><span class="op">=</span><span class="num">3</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t33" href="#t33">33</a></span><span class="t">            <span class="nam">index</span><span class="op">=</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t34" href="#t34">34</a></span><span class="t">            <span class="nam">head</span><span class="op">=</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_head</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t35" href="#t35">35</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t36" href="#t36">36</a></span><span class="t">        <span class="com"># Fold in layer norm biases</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t37" href="#t37">37</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_Q"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">qkv_bias</span><span class="op">[</span><span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t38" href="#t38">38</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_K"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">qkv_bias</span><span class="op">[</span><span class="num">1</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t39" href="#t39">39</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_V"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">qkv_bias</span><span class="op">[</span><span class="num">2</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t40" href="#t40">40</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t41" href="#t41">41</a></span><span class="t">        <span class="nam">W_O</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">attention</span><span class="op">.</span><span class="nam">dense</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t42" href="#t42">42</a></span><span class="t">        <span class="nam">W_O</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span><span class="nam">W_O</span><span class="op">,</span> <span class="str">"m (i h)->i h m"</span><span class="op">,</span> <span class="nam">i</span><span class="op">=</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t43" href="#t43">43</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.W_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">W_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t44" href="#t44">44</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.attn.b_O"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">attention</span><span class="op">.</span><span class="nam">dense</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t45" href="#t45">45</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t46" href="#t46">46</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.w"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">post_attention_layernorm</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t47" href="#t47">47</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.ln2.b"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">post_attention_layernorm</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t48" href="#t48">48</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t49" href="#t49">49</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_in"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">dense_h_to_4h</span><span class="op">.</span><span class="nam">weight</span><span class="op">.</span><span class="nam">T</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t50" href="#t50">50</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_in"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">dense_h_to_4h</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t51" href="#t51">51</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t52" href="#t52">52</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.W_out"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">dense_4h_to_h</span><span class="op">.</span><span class="nam">weight</span><span class="op">.</span><span class="nam">T</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t53" href="#t53">53</a></span><span class="t">        <span class="nam">state_dict</span><span class="op">[</span><span class="str">f"blocks.{l}.mlp.b_out"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">layers</span><span class="op">[</span><span class="nam">l</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">dense_4h_to_h</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t54" href="#t54">54</a></span><span class="t">    <span class="nam">state_dict</span><span class="op">[</span><span class="str">"ln_final.w"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">final_layer_norm</span><span class="op">.</span><span class="nam">weight</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t55" href="#t55">55</a></span><span class="t">    <span class="nam">state_dict</span><span class="op">[</span><span class="str">"ln_final.b"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">gpt_neox</span><span class="op">.</span><span class="nam">final_layer_norm</span><span class="op">.</span><span class="nam">bias</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t56" href="#t56">56</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t57" href="#t57">57</a></span><span class="t">    <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.W_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">neox</span><span class="op">.</span><span class="nam">embed_out</span><span class="op">.</span><span class="nam">weight</span><span class="op">.</span><span class="nam">T</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t58" href="#t58">58</a></span><span class="t">    <span class="nam">state_dict</span><span class="op">[</span><span class="str">"unembed.b_U"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_vocab</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t59" href="#t59">59</a></span><span class="t">    <span class="key">return</span> <span class="nam">state_dict</span>&nbsp;</span><span class="r"></span></p>
</main>
<footer>
    <div class="content">
        <p>
            <a id="prevFileLink" class="nav" href="d_c1ea89878f9b2ac7_neo_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_c1ea89878f9b2ac7_opt_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2024-10-16 00:05 +0000
        </p>
    </div>
</footer>
</body>
</html>
