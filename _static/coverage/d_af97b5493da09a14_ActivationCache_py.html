<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Coverage for transformer_lens/ActivationCache.py: 95%</title>
    <link rel="icon" sizes="32x32" href="favicon_32.png">
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="coverage_html.js" defer></script>
</head>
<body class="pyfile">
<header>
    <div class="content">
        <h1>
            <span class="text">Coverage for </span><b>transformer_lens/ActivationCache.py</b>:
            <span class="pc_cov">95%</span>
        </h1>
        <aside id="help_panel_wrapper">
            <input id="help_panel_state" type="checkbox">
            <label for="help_panel_state">
                <img id="keyboard_icon" src="keybd_closed.png" alt="Show/hide keyboard shortcuts" />
            </label>
            <div id="help_panel">
                <p class="legend">Shortcuts on this page</p>
                <div class="keyhelp">
                    <p>
                        <kbd>r</kbd>
                        <kbd>m</kbd>
                        <kbd>x</kbd>
                        <kbd>p</kbd>
                        &nbsp; toggle line displays
                    </p>
                    <p>
                        <kbd>j</kbd>
                        <kbd>k</kbd>
                        &nbsp; next/prev highlighted chunk
                    </p>
                    <p>
                        <kbd>0</kbd> &nbsp; (zero) top of page
                    </p>
                    <p>
                        <kbd>1</kbd> &nbsp; (one) first highlighted chunk
                    </p>
                    <p>
                        <kbd>[</kbd>
                        <kbd>]</kbd>
                        &nbsp; prev/next file
                    </p>
                    <p>
                        <kbd>u</kbd> &nbsp; up to the index
                    </p>
                    <p>
                        <kbd>?</kbd> &nbsp; show/hide this help
                    </p>
                </div>
            </div>
        </aside>
        <h2>
            <span class="text">289 statements &nbsp;</span>
            <button type="button" class="run button_toggle_run" value="run" data-shortcut="r" title="Toggle lines run">282<span class="text"> run</span></button>
            <button type="button" class="mis show_mis button_toggle_mis" value="mis" data-shortcut="m" title="Toggle lines missing">7<span class="text"> missing</span></button>
            <button type="button" class="exc show_exc button_toggle_exc" value="exc" data-shortcut="x" title="Toggle lines excluded">0<span class="text"> excluded</span></button>
            <button type="button" class="par run show_par button_toggle_par" value="par" data-shortcut="p" title="Toggle lines partially run">10<span class="text"> partial</span></button>
        </h2>
        <p class="text">
            <a id="prevFileLink" class="nav" href="index.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_af97b5493da09a14_BertNextSentencePrediction_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2025-05-05 16:22 +0000
        </p>
        <aside class="hidden">
            <button type="button" class="button_next_chunk" data-shortcut="j"/>
            <button type="button" class="button_prev_chunk" data-shortcut="k"/>
            <button type="button" class="button_top_of_page" data-shortcut="0"/>
            <button type="button" class="button_first_chunk" data-shortcut="1"/>
            <button type="button" class="button_prev_file" data-shortcut="["/>
            <button type="button" class="button_next_file" data-shortcut="]"/>
            <button type="button" class="button_to_index" data-shortcut="u"/>
            <button type="button" class="button_show_hide_help" data-shortcut="?"/>
        </aside>
    </div>
</header>
<main id="source">
    <p class="pln"><span class="n"><a id="t1" href="#t1">1</a></span><span class="t"><span class="str">"""Activation Cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2" href="#t2">2</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t3" href="#t3">3</a></span><span class="t"><span class="str">The :class:`ActivationCache` is at the core of Transformer Lens. It is a wrapper that stores all</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t4" href="#t4">4</a></span><span class="t"><span class="str">important activations from a forward pass of the model, and provides a variety of helper functions</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t5" href="#t5">5</a></span><span class="t"><span class="str">to investigate them.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t6" href="#t6">6</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t7" href="#t7">7</a></span><span class="t"><span class="str">Getting Started:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t8" href="#t8">8</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t9" href="#t9">9</a></span><span class="t"><span class="str">When reading these docs for the first time, we recommend reading the main :class:`ActivationCache`</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t10" href="#t10">10</a></span><span class="t"><span class="str">class first, including the examples, and then skimming the available methods. You can then refer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t11" href="#t11">11</a></span><span class="t"><span class="str">back to these docs depending on what you need to do.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t12" href="#t12">12</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t13" href="#t13">13</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t14" href="#t14">14</a></span><span class="t"><span class="key">from</span> <span class="nam">__future__</span> <span class="key">import</span> <span class="nam">annotations</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t15" href="#t15">15</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t16" href="#t16">16</a></span><span class="t"><span class="key">import</span> <span class="nam">logging</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t17" href="#t17">17</a></span><span class="t"><span class="key">import</span> <span class="nam">warnings</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t18" href="#t18">18</a></span><span class="t"><span class="key">from</span> <span class="nam">typing</span> <span class="key">import</span> <span class="nam">Any</span><span class="op">,</span> <span class="nam">Dict</span><span class="op">,</span> <span class="nam">Iterator</span><span class="op">,</span> <span class="nam">List</span><span class="op">,</span> <span class="nam">Optional</span><span class="op">,</span> <span class="nam">Tuple</span><span class="op">,</span> <span class="nam">Union</span><span class="op">,</span> <span class="nam">cast</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t19" href="#t19">19</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t20" href="#t20">20</a></span><span class="t"><span class="key">import</span> <span class="nam">einops</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t21" href="#t21">21</a></span><span class="t"><span class="key">import</span> <span class="nam">numpy</span> <span class="key">as</span> <span class="nam">np</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t22" href="#t22">22</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t23" href="#t23">23</a></span><span class="t"><span class="key">from</span> <span class="nam">jaxtyping</span> <span class="key">import</span> <span class="nam">Float</span><span class="op">,</span> <span class="nam">Int</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t24" href="#t24">24</a></span><span class="t"><span class="key">from</span> <span class="nam">typing_extensions</span> <span class="key">import</span> <span class="nam">Literal</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t25" href="#t25">25</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t26" href="#t26">26</a></span><span class="t"><span class="key">import</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utils</span> <span class="key">as</span> <span class="nam">utils</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t27" href="#t27">27</a></span><span class="t"><span class="key">from</span> <span class="nam">transformer_lens</span><span class="op">.</span><span class="nam">utils</span> <span class="key">import</span> <span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t28" href="#t28">28</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t29" href="#t29">29</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t30" href="#t30">30</a></span><span class="t"><span class="key">class</span> <span class="nam">ActivationCache</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t31" href="#t31">31</a></span><span class="t">    <span class="str">"""Activation Cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t32" href="#t32">32</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t33" href="#t33">33</a></span><span class="t"><span class="str">    A wrapper that stores all important activations from a forward pass of the model, and provides a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t34" href="#t34">34</a></span><span class="t"><span class="str">    variety of helper functions to investigate them.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t35" href="#t35">35</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t36" href="#t36">36</a></span><span class="t"><span class="str">    The :class:`ActivationCache` is at the core of Transformer Lens. It is a wrapper that stores all</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t37" href="#t37">37</a></span><span class="t"><span class="str">    important activations from a forward pass of the model, and provides a variety of helper</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t38" href="#t38">38</a></span><span class="t"><span class="str">    functions to investigate them. The common way to access it is to run the model with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t39" href="#t39">39</a></span><span class="t"><span class="str">    :meth:`transformer_lens.HookedTransformer.run_with_cache`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t40" href="#t40">40</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t41" href="#t41">41</a></span><span class="t"><span class="str">    Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t42" href="#t42">42</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t43" href="#t43">43</a></span><span class="t"><span class="str">    When investigating a particular behaviour of a model, a very common first step is to try and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t44" href="#t44">44</a></span><span class="t"><span class="str">    understand which components of the model are most responsible for that behaviour. For example,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t45" href="#t45">45</a></span><span class="t"><span class="str">    if you're investigating the prompt "Why did the chicken cross the" -> " road", you might want to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t46" href="#t46">46</a></span><span class="t"><span class="str">    understand if there is a specific sublayer (mlp or multi-head attention) that is responsible for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t47" href="#t47">47</a></span><span class="t"><span class="str">    the model predicting "road". This kind of analysis commonly falls under the category of "logit</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t48" href="#t48">48</a></span><span class="t"><span class="str">    attribution" or "direct logit attribution" (DLA).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t49" href="#t49">49</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t50" href="#t50">50</a></span><span class="t"><span class="str">    >>> from transformer_lens import HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t51" href="#t51">51</a></span><span class="t"><span class="str">    >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t52" href="#t52">52</a></span><span class="t"><span class="str">    Loaded pretrained model tiny-stories-1M into HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t53" href="#t53">53</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t54" href="#t54">54</a></span><span class="t"><span class="str">    >>> _logits, cache = model.run_with_cache("Why did the chicken cross the")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t55" href="#t55">55</a></span><span class="t"><span class="str">    >>> residual_stream, labels = cache.decompose_resid(return_labels=True, mode="attn")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t56" href="#t56">56</a></span><span class="t"><span class="str">    >>> print(labels[0:3])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t57" href="#t57">57</a></span><span class="t"><span class="str">    ['embed', 'pos_embed', '0_attn_out']</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t58" href="#t58">58</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t59" href="#t59">59</a></span><span class="t"><span class="str">    >>> answer = " road" # Note the proceeding space to match the model's tokenization</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t60" href="#t60">60</a></span><span class="t"><span class="str">    >>> logit_attrs = cache.logit_attrs(residual_stream, answer)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t61" href="#t61">61</a></span><span class="t"><span class="str">    >>> print(logit_attrs.shape) # Attention layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t62" href="#t62">62</a></span><span class="t"><span class="str">    torch.Size([10, 1, 7])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t63" href="#t63">63</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t64" href="#t64">64</a></span><span class="t"><span class="str">    >>> most_important_component_idx = torch.argmax(logit_attrs)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t65" href="#t65">65</a></span><span class="t"><span class="str">    >>> print(labels[most_important_component_idx])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t66" href="#t66">66</a></span><span class="t"><span class="str">    3_attn_out</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t67" href="#t67">67</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t68" href="#t68">68</a></span><span class="t"><span class="str">    You can also dig in with more granularity, using :meth:`get_full_resid_decomposition` to get the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t69" href="#t69">69</a></span><span class="t"><span class="str">    residual stream by individual component (mlp neurons and individual attention heads). This</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t70" href="#t70">70</a></span><span class="t"><span class="str">    creates a larger residual stack, but the approach of using :meth"`logit_attrs` remains the same.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t71" href="#t71">71</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t72" href="#t72">72</a></span><span class="t"><span class="str">    Equally you might want to find out if the model struggles to construct such excellent jokes</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t73" href="#t73">73</a></span><span class="t"><span class="str">    until the very last layers, or if it is trivial and the first few layers are enough. This kind</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t74" href="#t74">74</a></span><span class="t"><span class="str">    of analysis is called "logit lens", and you can find out more about how to do that with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t75" href="#t75">75</a></span><span class="t"><span class="str">    :meth:`ActivationCache.accumulated_resid`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t76" href="#t76">76</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t77" href="#t77">77</a></span><span class="t"><span class="str">    Warning:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t78" href="#t78">78</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t79" href="#t79">79</a></span><span class="t"><span class="str">    :class:`ActivationCache` is designed to be used with</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t80" href="#t80">80</a></span><span class="t"><span class="str">    :class:`transformer_lens.HookedTransformer`, and will not work with other models. It's also</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t81" href="#t81">81</a></span><span class="t"><span class="str">    designed to be used with all activations of :class:`transformer_lens.HookedTransformer` being</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t82" href="#t82">82</a></span><span class="t"><span class="str">    cached, and some internal methods will break without that.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t83" href="#t83">83</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t84" href="#t84">84</a></span><span class="t"><span class="str">    The biggest footgun and source of bugs in this code will be keeping track of indexes,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t85" href="#t85">85</a></span><span class="t"><span class="str">    dimensions, and the numbers of each. There are several kinds of activations:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t86" href="#t86">86</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t87" href="#t87">87</a></span><span class="t"><span class="str">    * Internal attn head vectors: q, k, v, z. Shape [batch, pos, head_index, d_head].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t88" href="#t88">88</a></span><span class="t"><span class="str">    * Internal attn pattern style results: pattern (post softmax), attn_scores (pre-softmax). Shape</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t89" href="#t89">89</a></span><span class="t"><span class="str">      [batch, head_index, query_pos, key_pos].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t90" href="#t90">90</a></span><span class="t"><span class="str">    * Attn head results: result. Shape [batch, pos, head_index, d_model].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t91" href="#t91">91</a></span><span class="t"><span class="str">    * Internal MLP vectors: pre, post, mid (only used for solu_ln - the part between activation +</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t92" href="#t92">92</a></span><span class="t"><span class="str">      layernorm). Shape [batch, pos, d_mlp].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t93" href="#t93">93</a></span><span class="t"><span class="str">    * Residual stream vectors: resid_pre, resid_mid, resid_post, attn_out, mlp_out, embed,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t94" href="#t94">94</a></span><span class="t"><span class="str">      pos_embed, normalized (output of each LN or LNPre). Shape [batch, pos, d_model].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t95" href="#t95">95</a></span><span class="t"><span class="str">    * LayerNorm Scale: scale. Shape [batch, pos, 1].</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t96" href="#t96">96</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t97" href="#t97">97</a></span><span class="t"><span class="str">    Sometimes the batch dimension will be missing because we applied `remove_batch_dim` (used when</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t98" href="#t98">98</a></span><span class="t"><span class="str">    batch_size=1), and as such all library functions *should* be robust to that.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t99" href="#t99">99</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t100" href="#t100">100</a></span><span class="t"><span class="str">    Type annotations are in the following form:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t101" href="#t101">101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t102" href="#t102">102</a></span><span class="t"><span class="str">    * layers_covered is the number of layers queried in functions that stack the residual stream.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t103" href="#t103">103</a></span><span class="t"><span class="str">    * batch_and_pos_dims is the set of dimensions from batch and pos - by default this is ["batch",</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t104" href="#t104">104</a></span><span class="t"><span class="str">      "pos"], but is only ["pos"] if we've removed the batch dimension and is [()] if we've removed</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t105" href="#t105">105</a></span><span class="t"><span class="str">      batch dimension and are applying a pos slice which indexes a specific position.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t106" href="#t106">106</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t107" href="#t107">107</a></span><span class="t"><span class="str">    Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t108" href="#t108">108</a></span><span class="t"><span class="str">        cache_dict:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t109" href="#t109">109</a></span><span class="t"><span class="str">            A dictionary of cached activations from a model run.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t110" href="#t110">110</a></span><span class="t"><span class="str">        model:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t111" href="#t111">111</a></span><span class="t"><span class="str">            The model that the activations are from.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t112" href="#t112">112</a></span><span class="t"><span class="str">        has_batch_dim:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t113" href="#t113">113</a></span><span class="t"><span class="str">            Whether the activations have a batch dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t114" href="#t114">114</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t115" href="#t115">115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t116" href="#t116">116</a></span><span class="t">    <span class="key">def</span> <span class="nam">__init__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">cache_dict</span><span class="op">:</span> <span class="nam">Dict</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">,</span> <span class="nam">model</span><span class="op">,</span> <span class="nam">has_batch_dim</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t117" href="#t117">117</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span> <span class="op">=</span> <span class="nam">cache_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t118" href="#t118">118</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">model</span> <span class="op">=</span> <span class="nam">model</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t119" href="#t119">119</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span> <span class="op">=</span> <span class="nam">has_batch_dim</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t120" href="#t120">120</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">has_embed</span> <span class="op">=</span> <span class="str">"hook_embed"</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t121" href="#t121">121</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">has_pos_embed</span> <span class="op">=</span> <span class="str">"hook_pos_embed"</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t122" href="#t122">122</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t123" href="#t123">123</a></span><span class="t">    <span class="key">def</span> <span class="nam">remove_batch_dim</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">ActivationCache</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t124" href="#t124">124</a></span><span class="t">        <span class="str">"""Remove the Batch Dimension (if a single batch item).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t125" href="#t125">125</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t126" href="#t126">126</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t127" href="#t127">127</a></span><span class="t"><span class="str">            The ActivationCache with the batch dimension removed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t128" href="#t128">128</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t129" href="#t129">129</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t130" href="#t130">130</a></span><span class="t">            <span class="key">for</span> <span class="nam">key</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t131" href="#t131">131</a></span><span class="t">                <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t132" href="#t132">132</a></span><span class="t">                    <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">0</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t133" href="#t133">133</a></span><span class="t">                <span class="op">)</span><span class="op">,</span> <span class="str">f"Cannot remove batch dimension from cache with batch size > 1, \</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t134" href="#t134">134</a></span><span class="t"><span class="str">                    for key {key} with shape {self.cache_dict[key].shape}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t135" href="#t135">135</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span><span class="op">[</span><span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t136" href="#t136">136</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span> <span class="op">=</span> <span class="key">False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t137" href="#t137">137</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t138" href="#t138">138</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">"Tried removing batch dimension after already having removed it."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t139" href="#t139">139</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t140" href="#t140">140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t141" href="#t141">141</a></span><span class="t">    <span class="key">def</span> <span class="nam">__repr__</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">str</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t142" href="#t142">142</a></span><span class="t">        <span class="str">"""Representation of the ActivationCache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t143" href="#t143">143</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t144" href="#t144">144</a></span><span class="t"><span class="str">        Special method that returns a string representation of an object. It's normally used to give</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t145" href="#t145">145</a></span><span class="t"><span class="str">        a string that can be used to recreate the object, but here we just return a string that</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t146" href="#t146">146</a></span><span class="t"><span class="str">        describes the object.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t147" href="#t147">147</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t148" href="#t148">148</a></span><span class="t">        <span class="key">return</span> <span class="str">f"ActivationCache with keys {list(self.cache_dict.keys())}"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t149" href="#t149">149</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t150" href="#t150">150</a></span><span class="t">    <span class="key">def</span> <span class="nam">__getitem__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">key</span><span class="op">)</span> <span class="op">-></span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t151" href="#t151">151</a></span><span class="t">        <span class="str">"""Retrieve Cached Activations by Key or Shorthand.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t152" href="#t152">152</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t153" href="#t153">153</a></span><span class="t"><span class="str">        Enables direct access to cached activations via dictionary-style indexing using keys or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t154" href="#t154">154</a></span><span class="t"><span class="str">        shorthand naming conventions. It also supports tuples for advanced indexing, with the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t155" href="#t155">155</a></span><span class="t"><span class="str">        dimension order as (get_act_name, layer_index, layer_type).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t156" href="#t156">156</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t157" href="#t157">157</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t158" href="#t158">158</a></span><span class="t"><span class="str">            key:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t159" href="#t159">159</a></span><span class="t"><span class="str">                The key or shorthand name for the activation to retrieve.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t160" href="#t160">160</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t161" href="#t161">161</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t162" href="#t162">162</a></span><span class="t"><span class="str">            The cached activation tensor corresponding to the given key.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t163" href="#t163">163</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t164" href="#t164">164</a></span><span class="t">        <span class="key">if</span> <span class="nam">key</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t165" href="#t165">165</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">key</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t166" href="#t166">166</a></span><span class="t">        <span class="key">elif</span> <span class="nam">type</span><span class="op">(</span><span class="nam">key</span><span class="op">)</span> <span class="op">==</span> <span class="nam">str</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t167" href="#t167">167</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">utils</span><span class="op">.</span><span class="nam">get_act_name</span><span class="op">(</span><span class="nam">key</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t168" href="#t168">168</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t169" href="#t169">169</a></span><span class="t">            <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">key</span><span class="op">)</span> <span class="op">></span> <span class="num">1</span> <span class="key">and</span> <span class="nam">key</span><span class="op">[</span><span class="num">1</span><span class="op">]</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t170" href="#t170">170</a></span><span class="t">                <span class="key">if</span> <span class="nam">key</span><span class="op">[</span><span class="num">1</span><span class="op">]</span> <span class="op">&lt;</span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t171" href="#t171">171</a></span><span class="t">                    <span class="com"># Supports negative indexing on the layer dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t172" href="#t172">172</a></span><span class="t">                    <span class="nam">key</span> <span class="op">=</span> <span class="op">(</span><span class="nam">key</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span> <span class="op">+</span> <span class="nam">key</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="op">*</span><span class="nam">key</span><span class="op">[</span><span class="num">2</span><span class="op">:</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t173" href="#t173">173</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="nam">utils</span><span class="op">.</span><span class="nam">get_act_name</span><span class="op">(</span><span class="op">*</span><span class="nam">key</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t174" href="#t174">174</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t175" href="#t175">175</a></span><span class="t">    <span class="key">def</span> <span class="nam">__len__</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">int</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t176" href="#t176">176</a></span><span class="t">        <span class="str">"""Length of the ActivationCache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t177" href="#t177">177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t178" href="#t178">178</a></span><span class="t"><span class="str">        Special method that returns the length of an object (in this case the number of different</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t179" href="#t179">179</a></span><span class="t"><span class="str">        activations in the cache).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t180" href="#t180">180</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t181" href="#t181">181</a></span><span class="t">        <span class="key">return</span> <span class="nam">len</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t182" href="#t182">182</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t183" href="#t183">183</a></span><span class="t">    <span class="key">def</span> <span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">device</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">str</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">device</span><span class="op">]</span><span class="op">,</span> <span class="nam">move_model</span><span class="op">=</span><span class="key">False</span><span class="op">)</span> <span class="op">-></span> <span class="nam">ActivationCache</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t184" href="#t184">184</a></span><span class="t">        <span class="str">"""Move the Cache to a Device.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t185" href="#t185">185</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t186" href="#t186">186</a></span><span class="t"><span class="str">        Mostly useful for moving the cache to the CPU after model computation finishes to save GPU</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t187" href="#t187">187</a></span><span class="t"><span class="str">        memory. Note however that operations will be much slower on the CPU. Note also that some</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t188" href="#t188">188</a></span><span class="t"><span class="str">        methods will break unless the model is also moved to the same device, eg</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t189" href="#t189">189</a></span><span class="t"><span class="str">        `compute_head_results`.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t190" href="#t190">190</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t191" href="#t191">191</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t192" href="#t192">192</a></span><span class="t"><span class="str">            device:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t193" href="#t193">193</a></span><span class="t"><span class="str">                The device to move the cache to (e.g. `torch.device.cpu`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t194" href="#t194">194</a></span><span class="t"><span class="str">            move_model:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t195" href="#t195">195</a></span><span class="t"><span class="str">                Whether to also move the model to the same device. @deprecated</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t196" href="#t196">196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t197" href="#t197">197</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t198" href="#t198">198</a></span><span class="t">        <span class="com"># Move model is deprecated as we plan on de-coupling the classes</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t199" href="#t199">199</a></span><span class="t">        <span class="key">if</span> <span class="nam">move_model</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t200" href="#t200">200</a></span><span class="t">            <span class="nam">warnings</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t201" href="#t201">201</a></span><span class="t">                <span class="str">"The 'move_model' parameter is deprecated."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t202" href="#t202">202</a></span><span class="t">                <span class="nam">DeprecationWarning</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t203" href="#t203">203</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t204" href="#t204">204</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t205" href="#t205">205</a></span><span class="t">        <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span> <span class="op">=</span> <span class="op">{</span><span class="nam">key</span><span class="op">:</span> <span class="nam">value</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">device</span><span class="op">)</span> <span class="key">for</span> <span class="nam">key</span><span class="op">,</span> <span class="nam">value</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">items</span><span class="op">(</span><span class="op">)</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t206" href="#t206">206</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t207" href="#t207">207</a></span><span class="t">        <span class="key">if</span> <span class="nam">move_model</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t208" href="#t208">208</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t209" href="#t209">209</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t210" href="#t210">210</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t211" href="#t211">211</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t212" href="#t212">212</a></span><span class="t">    <span class="key">def</span> <span class="nam">toggle_autodiff</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">mode</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t213" href="#t213">213</a></span><span class="t">        <span class="str">"""Toggle Autodiff Globally.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t214" href="#t214">214</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t215" href="#t215">215</a></span><span class="t"><span class="str">        Applies `torch.set_grad_enabled(mode)` to the global state (not just TransformerLens).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t216" href="#t216">216</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t217" href="#t217">217</a></span><span class="t"><span class="str">        Warning:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t218" href="#t218">218</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t219" href="#t219">219</a></span><span class="t"><span class="str">        This is pretty dangerous, since autodiff is global state - this turns off torch's</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t220" href="#t220">220</a></span><span class="t"><span class="str">        ability to take gradients completely and it's easy to get a bunch of errors if you don't</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t221" href="#t221">221</a></span><span class="t"><span class="str">        realise what you're doing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t222" href="#t222">222</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t223" href="#t223">223</a></span><span class="t"><span class="str">        But autodiff consumes a LOT of GPU memory (since every intermediate activation is cached</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t224" href="#t224">224</a></span><span class="t"><span class="str">        until all downstream activations are deleted - this means that computing the loss and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t225" href="#t225">225</a></span><span class="t"><span class="str">        storing it in a list will keep every activation sticking around!). So often when you're</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t226" href="#t226">226</a></span><span class="t"><span class="str">        analysing a model's activations, and don't need to do any training, autodiff is more trouble</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t227" href="#t227">227</a></span><span class="t"><span class="str">        than its worth.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t228" href="#t228">228</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t229" href="#t229">229</a></span><span class="t"><span class="str">        If you don't want to mess with global state, using torch.inference_mode as a context manager</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t230" href="#t230">230</a></span><span class="t"><span class="str">        or decorator achieves similar effects:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t231" href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t232" href="#t232">232</a></span><span class="t"><span class="str">        >>> with torch.inference_mode():</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t233" href="#t233">233</a></span><span class="t"><span class="str">        ...     y = torch.Tensor([1., 2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t234" href="#t234">234</a></span><span class="t"><span class="str">        >>> y.requires_grad</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t235" href="#t235">235</a></span><span class="t"><span class="str">        False</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t236" href="#t236">236</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t237" href="#t237">237</a></span><span class="t">        <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">"Changed the global state, set autodiff to %s"</span><span class="op">,</span> <span class="nam">mode</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t238" href="#t238">238</a></span><span class="t">        <span class="nam">torch</span><span class="op">.</span><span class="nam">set_grad_enabled</span><span class="op">(</span><span class="nam">mode</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t239" href="#t239">239</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t240" href="#t240">240</a></span><span class="t">    <span class="key">def</span> <span class="nam">keys</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t241" href="#t241">241</a></span><span class="t">        <span class="str">"""Keys of the ActivationCache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t242" href="#t242">242</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t243" href="#t243">243</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t244" href="#t244">244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t245" href="#t245">245</a></span><span class="t"><span class="str">            >>> from transformer_lens import HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t246" href="#t246">246</a></span><span class="t"><span class="str">            >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t247" href="#t247">247</a></span><span class="t"><span class="str">            Loaded pretrained model tiny-stories-1M into HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t248" href="#t248">248</a></span><span class="t"><span class="str">            >>> _logits, cache = model.run_with_cache("Some prompt")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t249" href="#t249">249</a></span><span class="t"><span class="str">            >>> list(cache.keys())[0:3]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t250" href="#t250">250</a></span><span class="t"><span class="str">            ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre']</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t251" href="#t251">251</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t252" href="#t252">252</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t253" href="#t253">253</a></span><span class="t"><span class="str">            List of all keys.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t254" href="#t254">254</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t255" href="#t255">255</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">keys</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t256" href="#t256">256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t257" href="#t257">257</a></span><span class="t">    <span class="key">def</span> <span class="nam">values</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t258" href="#t258">258</a></span><span class="t">        <span class="str">"""Values of the ActivationCache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t259" href="#t259">259</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t260" href="#t260">260</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t261" href="#t261">261</a></span><span class="t"><span class="str">            List of all values.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t262" href="#t262">262</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t263" href="#t263">263</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">values</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t264" href="#t264">264</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t265" href="#t265">265</a></span><span class="t">    <span class="key">def</span> <span class="nam">items</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t266" href="#t266">266</a></span><span class="t">        <span class="str">"""Items of the ActivationCache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t267" href="#t267">267</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t268" href="#t268">268</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t269" href="#t269">269</a></span><span class="t"><span class="str">            List of all items ((key, value) tuples).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t270" href="#t270">270</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t271" href="#t271">271</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">items</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t272" href="#t272">272</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t273" href="#t273">273</a></span><span class="t">    <span class="key">def</span> <span class="nam">__iter__</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span> <span class="op">-></span> <span class="nam">Iterator</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t274" href="#t274">274</a></span><span class="t">        <span class="str">"""ActivationCache Iterator.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t275" href="#t275">275</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t276" href="#t276">276</a></span><span class="t"><span class="str">        Special method that returns an iterator over the ActivationCache. Allows looping over the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t277" href="#t277">277</a></span><span class="t"><span class="str">        cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t278" href="#t278">278</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t279" href="#t279">279</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t280" href="#t280">280</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t281" href="#t281">281</a></span><span class="t"><span class="str">            >>> from transformer_lens import HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t282" href="#t282">282</a></span><span class="t"><span class="str">            >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t283" href="#t283">283</a></span><span class="t"><span class="str">            Loaded pretrained model tiny-stories-1M into HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t284" href="#t284">284</a></span><span class="t"><span class="str">            >>> _logits, cache = model.run_with_cache("Some prompt")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t285" href="#t285">285</a></span><span class="t"><span class="str">            >>> cache_interesting_names = []</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t286" href="#t286">286</a></span><span class="t"><span class="str">            >>> for key in cache:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t287" href="#t287">287</a></span><span class="t"><span class="str">            ...     if not key.startswith("blocks.") or key.startswith("blocks.0"):</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t288" href="#t288">288</a></span><span class="t"><span class="str">            ...         cache_interesting_names.append(key)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t289" href="#t289">289</a></span><span class="t"><span class="str">            >>> print(cache_interesting_names[0:3])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t290" href="#t290">290</a></span><span class="t"><span class="str">            ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre']</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t291" href="#t291">291</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t292" href="#t292">292</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t293" href="#t293">293</a></span><span class="t"><span class="str">            Iterator over the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t294" href="#t294">294</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t295" href="#t295">295</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">__iter__</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t296" href="#t296">296</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t297" href="#t297">297</a></span><span class="t">    <span class="key">def</span> <span class="nam">apply_slice_to_batch_dim</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">batch_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span><span class="op">)</span> <span class="op">-></span> <span class="nam">ActivationCache</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t298" href="#t298">298</a></span><span class="t">        <span class="str">"""Apply a Slice to the Batch Dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t299" href="#t299">299</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t300" href="#t300">300</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t301" href="#t301">301</a></span><span class="t"><span class="str">            batch_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t302" href="#t302">302</a></span><span class="t"><span class="str">                The slice to apply to the batch dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t303" href="#t303">303</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t304" href="#t304">304</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t305" href="#t305">305</a></span><span class="t"><span class="str">            The ActivationCache with the batch dimension sliced.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t306" href="#t306">306</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t307" href="#t307">307</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t308" href="#t308">308</a></span><span class="t">            <span class="nam">batch_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t309" href="#t309">309</a></span><span class="t">        <span class="nam">batch_slice</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">batch_slice</span><span class="op">)</span>  <span class="com"># mypy can't seem to infer this</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t310" href="#t310">310</a></span><span class="t">        <span class="key">assert</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t311" href="#t311">311</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span> <span class="key">or</span> <span class="nam">batch_slice</span><span class="op">.</span><span class="nam">mode</span> <span class="op">==</span> <span class="str">"empty"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t312" href="#t312">312</a></span><span class="t">        <span class="op">)</span><span class="op">,</span> <span class="str">"Cannot index into a cache without a batch dim"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t313" href="#t313">313</a></span><span class="t">        <span class="nam">still_has_batch_dim</span> <span class="op">=</span> <span class="op">(</span><span class="nam">batch_slice</span><span class="op">.</span><span class="nam">mode</span> <span class="op">!=</span> <span class="str">"int"</span><span class="op">)</span> <span class="key">and</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t314" href="#t314">314</a></span><span class="t">        <span class="nam">new_cache_dict</span> <span class="op">=</span> <span class="op">{</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t315" href="#t315">315</a></span><span class="t">            <span class="nam">name</span><span class="op">:</span> <span class="nam">batch_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">param</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span> <span class="key">for</span> <span class="nam">name</span><span class="op">,</span> <span class="nam">param</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">.</span><span class="nam">items</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t316" href="#t316">316</a></span><span class="t">        <span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t317" href="#t317">317</a></span><span class="t">        <span class="key">return</span> <span class="nam">ActivationCache</span><span class="op">(</span><span class="nam">new_cache_dict</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">,</span> <span class="nam">has_batch_dim</span><span class="op">=</span><span class="nam">still_has_batch_dim</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t318" href="#t318">318</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t319" href="#t319">319</a></span><span class="t">    <span class="key">def</span> <span class="nam">accumulated_resid</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t320" href="#t320">320</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t321" href="#t321">321</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t322" href="#t322">322</a></span><span class="t">        <span class="nam">incl_mid</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t323" href="#t323">323</a></span><span class="t">        <span class="nam">apply_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t324" href="#t324">324</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t325" href="#t325">325</a></span><span class="t">        <span class="nam">mlp_input</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t326" href="#t326">326</a></span><span class="t">        <span class="nam">return_labels</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t327" href="#t327">327</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t328" href="#t328">328</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"layers_covered *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t329" href="#t329">329</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"layers_covered *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t330" href="#t330">330</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t331" href="#t331">331</a></span><span class="t">        <span class="str">"""Accumulated Residual Stream.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t332" href="#t332">332</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t333" href="#t333">333</a></span><span class="t"><span class="str">        Returns the accumulated residual stream at each layer/sub-layer. This is useful for `Logit</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t334" href="#t334">334</a></span><span class="t"><span class="str">        Lens &lt;https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens>`</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t335" href="#t335">335</a></span><span class="t"><span class="str">        style analysis, where it can be thought of as what the model "believes" at each point in the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t336" href="#t336">336</a></span><span class="t"><span class="str">        residual stream.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t337" href="#t337">337</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t338" href="#t338">338</a></span><span class="t"><span class="str">        To project this into the vocabulary space, remember that there is a final layer norm in most</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t339" href="#t339">339</a></span><span class="t"><span class="str">        decoder-only transformers. Therefore, you need to first apply the final layer norm (which</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t340" href="#t340">340</a></span><span class="t"><span class="str">        can be done with `apply_ln`), and then multiply by the unembedding matrix (:math:`W_U`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t341" href="#t341">341</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t342" href="#t342">342</a></span><span class="t"><span class="str">        If you instead want to look at contributions to the residual stream from each component</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t343" href="#t343">343</a></span><span class="t"><span class="str">        (e.g. for direct logit attribution), see :meth:`decompose_resid` instead, or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t344" href="#t344">344</a></span><span class="t"><span class="str">        :meth:`get_full_resid_decomposition` if you want contributions broken down further into each</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t345" href="#t345">345</a></span><span class="t"><span class="str">        MLP neuron.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t346" href="#t346">346</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t347" href="#t347">347</a></span><span class="t"><span class="str">        Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t348" href="#t348">348</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t349" href="#t349">349</a></span><span class="t"><span class="str">        Logit Lens analysis can be done as follows:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t350" href="#t350">350</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t351" href="#t351">351</a></span><span class="t"><span class="str">        >>> from transformer_lens import HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t352" href="#t352">352</a></span><span class="t"><span class="str">        >>> from einops import einsum</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t353" href="#t353">353</a></span><span class="t"><span class="str">        >>> import torch</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t354" href="#t354">354</a></span><span class="t"><span class="str">        >>> import pandas as pd</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t355" href="#t355">355</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t356" href="#t356">356</a></span><span class="t"><span class="str">        >>> model = HookedTransformer.from_pretrained("tiny-stories-1M", device="cpu")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t357" href="#t357">357</a></span><span class="t"><span class="str">        Loaded pretrained model tiny-stories-1M into HookedTransformer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t358" href="#t358">358</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t359" href="#t359">359</a></span><span class="t"><span class="str">        >>> prompt = "Why did the chicken cross the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t360" href="#t360">360</a></span><span class="t"><span class="str">        >>> answer = " road"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t361" href="#t361">361</a></span><span class="t"><span class="str">        >>> logits, cache = model.run_with_cache("Why did the chicken cross the")</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t362" href="#t362">362</a></span><span class="t"><span class="str">        >>> answer_token = model.to_single_token(answer)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t363" href="#t363">363</a></span><span class="t"><span class="str">        >>> print(answer_token)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t364" href="#t364">364</a></span><span class="t"><span class="str">        2975</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t365" href="#t365">365</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t366" href="#t366">366</a></span><span class="t"><span class="str">        >>> accum_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t367" href="#t367">367</a></span><span class="t"><span class="str">        >>> last_token_accum = accum_resid[:, 0, -1, :]  # layer, batch, pos, d_model</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t368" href="#t368">368</a></span><span class="t"><span class="str">        >>> print(last_token_accum.shape)  # layer, d_model</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t369" href="#t369">369</a></span><span class="t"><span class="str">        torch.Size([9, 64])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t370" href="#t370">370</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t371" href="#t371">371</a></span><span class="t"><span class="str">        >>> W_U = model.W_U</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t372" href="#t372">372</a></span><span class="t"><span class="str">        >>> print(W_U.shape)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t373" href="#t373">373</a></span><span class="t"><span class="str">        torch.Size([64, 50257])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t374" href="#t374">374</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t375" href="#t375">375</a></span><span class="t"><span class="str">        >>> layers_unembedded = einsum(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t376" href="#t376">376</a></span><span class="t"><span class="str">        ...         last_token_accum,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t377" href="#t377">377</a></span><span class="t"><span class="str">        ...         W_U,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t378" href="#t378">378</a></span><span class="t"><span class="str">        ...         "layer d_model, d_model d_vocab -> layer d_vocab"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t379" href="#t379">379</a></span><span class="t"><span class="str">        ...     )</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t380" href="#t380">380</a></span><span class="t"><span class="str">        >>> print(layers_unembedded.shape)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t381" href="#t381">381</a></span><span class="t"><span class="str">        torch.Size([9, 50257])</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t382" href="#t382">382</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t383" href="#t383">383</a></span><span class="t"><span class="str">        >>> # Get the rank of the correct answer by layer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t384" href="#t384">384</a></span><span class="t"><span class="str">        >>> sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t385" href="#t385">385</a></span><span class="t"><span class="str">        >>> rank_answer = (sorted_indices == 2975).nonzero(as_tuple=True)[1]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t386" href="#t386">386</a></span><span class="t"><span class="str">        >>> print(pd.Series(rank_answer, index=labels))</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t387" href="#t387">387</a></span><span class="t"><span class="str">        0_pre         4442</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t388" href="#t388">388</a></span><span class="t"><span class="str">        1_pre          382</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t389" href="#t389">389</a></span><span class="t"><span class="str">        2_pre          982</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t390" href="#t390">390</a></span><span class="t"><span class="str">        3_pre         1160</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t391" href="#t391">391</a></span><span class="t"><span class="str">        4_pre          408</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t392" href="#t392">392</a></span><span class="t"><span class="str">        5_pre          145</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t393" href="#t393">393</a></span><span class="t"><span class="str">        6_pre           78</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t394" href="#t394">394</a></span><span class="t"><span class="str">        7_pre          387</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t395" href="#t395">395</a></span><span class="t"><span class="str">        final_post       6</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t396" href="#t396">396</a></span><span class="t"><span class="str">        dtype: int64</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t397" href="#t397">397</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t398" href="#t398">398</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t399" href="#t399">399</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t400" href="#t400">400</a></span><span class="t"><span class="str">                The layer to take components up to - by default includes resid_pre for that layer</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t401" href="#t401">401</a></span><span class="t"><span class="str">                and excludes resid_mid and resid_post for that layer. If set as `n_layers`, `-1` or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t402" href="#t402">402</a></span><span class="t"><span class="str">                `None` it will return all residual streams, including the final one (i.e.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t403" href="#t403">403</a></span><span class="t"><span class="str">                immediately pre logits). The indices are taken such that this gives the accumulated</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t404" href="#t404">404</a></span><span class="t"><span class="str">                streams up to the input to layer l.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t405" href="#t405">405</a></span><span class="t"><span class="str">            incl_mid:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t406" href="#t406">406</a></span><span class="t"><span class="str">                Whether to return `resid_mid` for all previous layers.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t407" href="#t407">407</a></span><span class="t"><span class="str">            apply_ln:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t408" href="#t408">408</a></span><span class="t"><span class="str">                Whether to apply LayerNorm to the stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t409" href="#t409">409</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t410" href="#t410">410</a></span><span class="t"><span class="str">                A slice object to apply to the pos dimension. Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t411" href="#t411">411</a></span><span class="t"><span class="str">            mlp_input:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t412" href="#t412">412</a></span><span class="t"><span class="str">                Whether to include resid_mid for the current layer. This essentially gives the MLP</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t413" href="#t413">413</a></span><span class="t"><span class="str">                input rather than the attention input.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t414" href="#t414">414</a></span><span class="t"><span class="str">            return_labels:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t415" href="#t415">415</a></span><span class="t"><span class="str">                Whether to return a list of labels for the residual stream components. Useful for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t416" href="#t416">416</a></span><span class="t"><span class="str">                labelling graphs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t417" href="#t417">417</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t418" href="#t418">418</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t419" href="#t419">419</a></span><span class="t"><span class="str">            A tensor of the accumulated residual streams. If `return_labels` is True, also returns a</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t420" href="#t420">420</a></span><span class="t"><span class="str">            list of labels for the components (as a tuple in the form `(components, labels)`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t421" href="#t421">421</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t422" href="#t422">422</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t423" href="#t423">423</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t424" href="#t424">424</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t425" href="#t425">425</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t426" href="#t426">426</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t427" href="#t427">427</a></span><span class="t">        <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">layer</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t428" href="#t428">428</a></span><span class="t">        <span class="nam">labels</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t429" href="#t429">429</a></span><span class="t">        <span class="nam">components_list</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t430" href="#t430">430</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span> <span class="op">+</span> <span class="num">1</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t431" href="#t431">431</a></span><span class="t">            <span class="key">if</span> <span class="nam">l</span> <span class="op">==</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t432" href="#t432">432</a></span><span class="t">                <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_post"</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span> <span class="op">-</span> <span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t433" href="#t433">433</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"final_post"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t434" href="#t434">434</a></span><span class="t">                <span class="key">continue</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t435" href="#t435">435</a></span><span class="t">            <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_pre"</span><span class="op">,</span> <span class="nam">l</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t436" href="#t436">436</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">f"{l}_pre"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t437" href="#t437">437</a></span><span class="t">            <span class="key">if</span> <span class="op">(</span><span class="nam">incl_mid</span> <span class="key">and</span> <span class="nam">l</span> <span class="op">&lt;</span> <span class="nam">layer</span><span class="op">)</span> <span class="key">or</span> <span class="op">(</span><span class="nam">mlp_input</span> <span class="key">and</span> <span class="nam">l</span> <span class="op">==</span> <span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t438" href="#t438">438</a></span><span class="t">                <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_mid"</span><span class="op">,</span> <span class="nam">l</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t439" href="#t439">439</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">f"{l}_mid"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t440" href="#t440">440</a></span><span class="t">        <span class="nam">components_list</span> <span class="op">=</span> <span class="op">[</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">c</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span> <span class="key">for</span> <span class="nam">c</span> <span class="key">in</span> <span class="nam">components_list</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t441" href="#t441">441</a></span><span class="t">        <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="nam">components_list</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t442" href="#t442">442</a></span><span class="t">        <span class="key">if</span> <span class="nam">apply_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t443" href="#t443">443</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t444" href="#t444">444</a></span><span class="t">                <span class="nam">components</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">mlp_input</span><span class="op">=</span><span class="nam">mlp_input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t445" href="#t445">445</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t446" href="#t446">446</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_labels</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t447" href="#t447">447</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span><span class="op">,</span> <span class="nam">labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t448" href="#t448">448</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t449" href="#t449">449</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t450" href="#t450">450</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t451" href="#t451">451</a></span><span class="t">    <span class="key">def</span> <span class="nam">logit_attrs</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t452" href="#t452">452</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t453" href="#t453">453</a></span><span class="t">        <span class="nam">residual_stack</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t454" href="#t454">454</a></span><span class="t">        <span class="nam">tokens</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t455" href="#t455">455</a></span><span class="t">            <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t456" href="#t456">456</a></span><span class="t">            <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t457" href="#t457">457</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">""</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t458" href="#t458">458</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t459" href="#t459">459</a></span><span class="t">            <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch position"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t460" href="#t460">460</a></span><span class="t">        <span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t461" href="#t461">461</a></span><span class="t">        <span class="nam">incorrect_tokens</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t462" href="#t462">462</a></span><span class="t">            <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t463" href="#t463">463</a></span><span class="t">                <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t464" href="#t464">464</a></span><span class="t">                <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t465" href="#t465">465</a></span><span class="t">                <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">""</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t466" href="#t466">466</a></span><span class="t">                <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t467" href="#t467">467</a></span><span class="t">                <span class="nam">Int</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"batch position"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t468" href="#t468">468</a></span><span class="t">            <span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t469" href="#t469">469</a></span><span class="t">        <span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t470" href="#t470">470</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t471" href="#t471">471</a></span><span class="t">        <span class="nam">batch_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t472" href="#t472">472</a></span><span class="t">        <span class="nam">has_batch_dim</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t473" href="#t473">473</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims_out"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t474" href="#t474">474</a></span><span class="t">        <span class="str">"""Logit Attributions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t475" href="#t475">475</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t476" href="#t476">476</a></span><span class="t"><span class="str">        Takes a residual stack (typically the residual stream decomposed by components), and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t477" href="#t477">477</a></span><span class="t"><span class="str">        calculates how much each item in the stack "contributes" to specific tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t478" href="#t478">478</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t479" href="#t479">479</a></span><span class="t"><span class="str">        It does this by:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t480" href="#t480">480</a></span><span class="t"><span class="str">            1. Getting the residual directions of the tokens (i.e. reversing the unembed)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t481" href="#t481">481</a></span><span class="t"><span class="str">            2. Taking the dot product of each item in the residual stack, with the token residual</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t482" href="#t482">482</a></span><span class="t"><span class="str">               directions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t483" href="#t483">483</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t484" href="#t484">484</a></span><span class="t"><span class="str">        Note that if incorrect tokens are provided, it instead takes the difference between the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t485" href="#t485">485</a></span><span class="t"><span class="str">        correct and incorrect tokens (to calculate the residual directions). This is useful as</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t486" href="#t486">486</a></span><span class="t"><span class="str">        sometimes we want to know e.g. which components are most responsible for selecting the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t487" href="#t487">487</a></span><span class="t"><span class="str">        correct token rather than an incorrect one. For example in the `Interpretability in the Wild</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t488" href="#t488">488</a></span><span class="t"><span class="str">        paper &lt;https://arxiv.org/abs/2211.00593>` prompts such as "John and Mary went to the shops,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t489" href="#t489">489</a></span><span class="t"><span class="str">        John gave a bag to" were investigated, and it was therefore useful to calculate attribution</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t490" href="#t490">490</a></span><span class="t"><span class="str">        for the :math:`\\text{Mary} - \\text{John}` residual direction.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t491" href="#t491">491</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t492" href="#t492">492</a></span><span class="t"><span class="str">        Warning:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t493" href="#t493">493</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t494" href="#t494">494</a></span><span class="t"><span class="str">        Choosing the correct `tokens` and `incorrect_tokens` is both important and difficult. When</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t495" href="#t495">495</a></span><span class="t"><span class="str">        investigating specific components it's also useful to look at it's impact on all tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t496" href="#t496">496</a></span><span class="t"><span class="str">        (i.e. :math:`\\text{final_ln}(\\text{residual_stack_item}) W_U`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t497" href="#t497">497</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t498" href="#t498">498</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t499" href="#t499">499</a></span><span class="t"><span class="str">            residual_stack:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t500" href="#t500">500</a></span><span class="t"><span class="str">                Stack of components of residual stream to get logit attributions for.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t501" href="#t501">501</a></span><span class="t"><span class="str">            tokens:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t502" href="#t502">502</a></span><span class="t"><span class="str">                Tokens to compute logit attributions on.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t503" href="#t503">503</a></span><span class="t"><span class="str">            incorrect_tokens:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t504" href="#t504">504</a></span><span class="t"><span class="str">                If provided, compute attributions on logit difference between tokens and</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t505" href="#t505">505</a></span><span class="t"><span class="str">                incorrect_tokens. Must have the same shape as tokens.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t506" href="#t506">506</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t507" href="#t507">507</a></span><span class="t"><span class="str">                The slice to apply layer norm scaling on. Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t508" href="#t508">508</a></span><span class="t"><span class="str">            batch_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t509" href="#t509">509</a></span><span class="t"><span class="str">                The slice to take on the batch dimension during layer norm scaling. Defaults to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t510" href="#t510">510</a></span><span class="t"><span class="str">                None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t511" href="#t511">511</a></span><span class="t"><span class="str">            has_batch_dim:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t512" href="#t512">512</a></span><span class="t"><span class="str">                Whether residual_stack has a batch dimension. Defaults to True.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t513" href="#t513">513</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t514" href="#t514">514</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t515" href="#t515">515</a></span><span class="t"><span class="str">            A tensor of the logit attributions or logit difference attributions if incorrect_tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t516" href="#t516">516</a></span><span class="t"><span class="str">            was provided.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t517" href="#t517">517</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t518" href="#t518">518</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t519" href="#t519">519</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t520" href="#t520">520</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t521" href="#t521">521</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t522" href="#t522">522</a></span><span class="t">            <span class="nam">batch_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t523" href="#t523">523</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t524" href="#t524">524</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t525" href="#t525">525</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">to_single_token</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t526" href="#t526">526</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t527" href="#t527">527</a></span><span class="t">        <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tokens</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t528" href="#t528">528</a></span><span class="t">            <span class="nam">tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t529" href="#t529">529</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t530" href="#t530">530</a></span><span class="t">        <span class="nam">logit_directions</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">tokens_to_residual_directions</span><span class="op">(</span><span class="nam">tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t531" href="#t531">531</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t532" href="#t532">532</a></span><span class="t">        <span class="key">if</span> <span class="nam">incorrect_tokens</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t533" href="#t533">533</a></span><span class="t">            <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">incorrect_tokens</span><span class="op">,</span> <span class="nam">str</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t534" href="#t534">534</a></span><span class="t">                <span class="nam">incorrect_tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">to_single_token</span><span class="op">(</span><span class="nam">incorrect_tokens</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t535" href="#t535">535</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t536" href="#t536">536</a></span><span class="t">            <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">incorrect_tokens</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t537" href="#t537">537</a></span><span class="t">                <span class="nam">incorrect_tokens</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">(</span><span class="nam">incorrect_tokens</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t538" href="#t538">538</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t539" href="#t539">539</a></span><span class="t">            <span class="key">if</span> <span class="nam">tokens</span><span class="op">.</span><span class="nam">shape</span> <span class="op">!=</span> <span class="nam">incorrect_tokens</span><span class="op">.</span><span class="nam">shape</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t540" href="#t540">540</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t541" href="#t541">541</a></span><span class="t">                    <span class="str">f"tokens and incorrect_tokens must have the same shape! \</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t542" href="#t542">542</a></span><span class="t"><span class="str">                        (tokens.shape={tokens.shape}, \</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t543" href="#t543">543</a></span><span class="t"><span class="str">                        incorrect_tokens.shape={incorrect_tokens.shape})"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t544" href="#t544">544</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t545" href="#t545">545</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t546" href="#t546">546</a></span><span class="t">            <span class="com"># If incorrect_tokens was provided, take the logit difference</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t547" href="#t547">547</a></span><span class="t">            <span class="nam">logit_directions</span> <span class="op">=</span> <span class="nam">logit_directions</span> <span class="op">-</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">tokens_to_residual_directions</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t548" href="#t548">548</a></span><span class="t">                <span class="nam">incorrect_tokens</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t549" href="#t549">549</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t550" href="#t550">550</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t551" href="#t551">551</a></span><span class="t">        <span class="nam">scaled_residual_stack</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t552" href="#t552">552</a></span><span class="t">            <span class="nam">residual_stack</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t553" href="#t553">553</a></span><span class="t">            <span class="nam">layer</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t554" href="#t554">554</a></span><span class="t">            <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t555" href="#t555">555</a></span><span class="t">            <span class="nam">batch_slice</span><span class="op">=</span><span class="nam">batch_slice</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t556" href="#t556">556</a></span><span class="t">            <span class="nam">has_batch_dim</span><span class="op">=</span><span class="nam">has_batch_dim</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t557" href="#t557">557</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t558" href="#t558">558</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t559" href="#t559">559</a></span><span class="t">        <span class="com"># Element-wise multiplication and sum over the d_model dimension</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t560" href="#t560">560</a></span><span class="t">        <span class="nam">logit_attrs</span> <span class="op">=</span> <span class="op">(</span><span class="nam">scaled_residual_stack</span> <span class="op">*</span> <span class="nam">logit_directions</span><span class="op">)</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t561" href="#t561">561</a></span><span class="t">        <span class="key">return</span> <span class="nam">logit_attrs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t562" href="#t562">562</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t563" href="#t563">563</a></span><span class="t">    <span class="key">def</span> <span class="nam">decompose_resid</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t564" href="#t564">564</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t565" href="#t565">565</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t566" href="#t566">566</a></span><span class="t">        <span class="nam">mlp_input</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t567" href="#t567">567</a></span><span class="t">        <span class="nam">mode</span><span class="op">:</span> <span class="nam">Literal</span><span class="op">[</span><span class="str">"all"</span><span class="op">,</span> <span class="str">"mlp"</span><span class="op">,</span> <span class="str">"attn"</span><span class="op">]</span> <span class="op">=</span> <span class="str">"all"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t568" href="#t568">568</a></span><span class="t">        <span class="nam">apply_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t569" href="#t569">569</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t570" href="#t570">570</a></span><span class="t">        <span class="nam">incl_embeds</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t571" href="#t571">571</a></span><span class="t">        <span class="nam">return_labels</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t572" href="#t572">572</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t573" href="#t573">573</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"layers_covered *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t574" href="#t574">574</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"layers_covered *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t575" href="#t575">575</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t576" href="#t576">576</a></span><span class="t">        <span class="str">"""Decompose the Residual Stream.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t577" href="#t577">577</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t578" href="#t578">578</a></span><span class="t"><span class="str">        Decomposes the residual stream input to layer L into a stack of the output of previous</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t579" href="#t579">579</a></span><span class="t"><span class="str">        layers. The sum of these is the input to layer L (plus embedding and pos embedding). This is</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t580" href="#t580">580</a></span><span class="t"><span class="str">        useful for attributing model behaviour to different components of the residual stream</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t581" href="#t581">581</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t582" href="#t582">582</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t583" href="#t583">583</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t584" href="#t584">584</a></span><span class="t"><span class="str">                The layer to take components up to - by default includes</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t585" href="#t585">585</a></span><span class="t"><span class="str">                resid_pre for that layer and excludes resid_mid and resid_post for that layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t586" href="#t586">586</a></span><span class="t"><span class="str">                layer==n_layers means to return all layer outputs incl in the final layer, layer==0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t587" href="#t587">587</a></span><span class="t"><span class="str">                means just embed and pos_embed. The indices are taken such that this gives the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t588" href="#t588">588</a></span><span class="t"><span class="str">                accumulated streams up to the input to layer l</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t589" href="#t589">589</a></span><span class="t"><span class="str">            mlp_input:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t590" href="#t590">590</a></span><span class="t"><span class="str">                Whether to include attn_out for the current</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t591" href="#t591">591</a></span><span class="t"><span class="str">                layer - essentially decomposing the residual stream that's input to the MLP input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t592" href="#t592">592</a></span><span class="t"><span class="str">                rather than the Attn input.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t593" href="#t593">593</a></span><span class="t"><span class="str">            mode:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t594" href="#t594">594</a></span><span class="t"><span class="str">                Values are "all", "mlp" or "attn". "all" returns all</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t595" href="#t595">595</a></span><span class="t"><span class="str">                components, "mlp" returns only the MLP components, and "attn" returns only the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t596" href="#t596">596</a></span><span class="t"><span class="str">                attention components. Defaults to "all".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t597" href="#t597">597</a></span><span class="t"><span class="str">            apply_ln:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t598" href="#t598">598</a></span><span class="t"><span class="str">                Whether to apply LayerNorm to the stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t599" href="#t599">599</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t600" href="#t600">600</a></span><span class="t"><span class="str">                A slice object to apply to the pos dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t601" href="#t601">601</a></span><span class="t"><span class="str">                Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t602" href="#t602">602</a></span><span class="t"><span class="str">            incl_embeds:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t603" href="#t603">603</a></span><span class="t"><span class="str">                Whether to include embed &amp; pos_embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t604" href="#t604">604</a></span><span class="t"><span class="str">            return_labels:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t605" href="#t605">605</a></span><span class="t"><span class="str">                Whether to return a list of labels for the residual stream components.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t606" href="#t606">606</a></span><span class="t"><span class="str">                Useful for labelling graphs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t607" href="#t607">607</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t608" href="#t608">608</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t609" href="#t609">609</a></span><span class="t"><span class="str">            A tensor of the accumulated residual streams. If `return_labels` is True, also returns</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t610" href="#t610">610</a></span><span class="t"><span class="str">            a list of labels for the components (as a tuple in the form `(components, labels)`).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t611" href="#t611">611</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t612" href="#t612">612</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t613" href="#t613">613</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t614" href="#t614">614</a></span><span class="t">        <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">)</span>  <span class="com"># mypy can't seem to infer this</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t615" href="#t615">615</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t616" href="#t616">616</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t617" href="#t617">617</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t618" href="#t618">618</a></span><span class="t">        <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">layer</span><span class="op">,</span> <span class="nam">int</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t619" href="#t619">619</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t620" href="#t620">620</a></span><span class="t">        <span class="nam">incl_attn</span> <span class="op">=</span> <span class="nam">mode</span> <span class="op">!=</span> <span class="str">"mlp"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t621" href="#t621">621</a></span><span class="t">        <span class="nam">incl_mlp</span> <span class="op">=</span> <span class="nam">mode</span> <span class="op">!=</span> <span class="str">"attn"</span> <span class="key">and</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_only</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t622" href="#t622">622</a></span><span class="t">        <span class="nam">components_list</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t623" href="#t623">623</a></span><span class="t">        <span class="nam">labels</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t624" href="#t624">624</a></span><span class="t">        <span class="key">if</span> <span class="nam">incl_embeds</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t625" href="#t625">625</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_embed</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">625&#x202F;&#x219B;&#x202F;628</span><span class="annotate long">line 625 didn't jump to line 628, because the condition on line 625 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t626" href="#t626">626</a></span><span class="t">                <span class="nam">components_list</span> <span class="op">=</span> <span class="op">[</span><span class="nam">self</span><span class="op">[</span><span class="str">"hook_embed"</span><span class="op">]</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t627" href="#t627">627</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"embed"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t628" href="#t628">628</a></span><span class="t">            <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_pos_embed</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">628&#x202F;&#x219B;&#x202F;632</span><span class="annotate long">line 628 didn't jump to line 632, because the condition on line 628 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t629" href="#t629">629</a></span><span class="t">                <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="str">"hook_pos_embed"</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t630" href="#t630">630</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"pos_embed"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t631" href="#t631">631</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t632" href="#t632">632</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t633" href="#t633">633</a></span><span class="t">            <span class="key">if</span> <span class="nam">incl_attn</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t634" href="#t634">634</a></span><span class="t">                <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"attn_out"</span><span class="op">,</span> <span class="nam">l</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t635" href="#t635">635</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">f"{l}_attn_out"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t636" href="#t636">636</a></span><span class="t">            <span class="key">if</span> <span class="nam">incl_mlp</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t637" href="#t637">637</a></span><span class="t">                <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"mlp_out"</span><span class="op">,</span> <span class="nam">l</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t638" href="#t638">638</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">f"{l}_mlp_out"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t639" href="#t639">639</a></span><span class="t">        <span class="key">if</span> <span class="nam">mlp_input</span> <span class="key">and</span> <span class="nam">incl_attn</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t640" href="#t640">640</a></span><span class="t">            <span class="nam">components_list</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"attn_out"</span><span class="op">,</span> <span class="nam">layer</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t641" href="#t641">641</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">f"{layer}_attn_out"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t642" href="#t642">642</a></span><span class="t">        <span class="nam">components_list</span> <span class="op">=</span> <span class="op">[</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">c</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span> <span class="key">for</span> <span class="nam">c</span> <span class="key">in</span> <span class="nam">components_list</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t643" href="#t643">643</a></span><span class="t">        <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="nam">components_list</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t644" href="#t644">644</a></span><span class="t">        <span class="key">if</span> <span class="nam">apply_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t645" href="#t645">645</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t646" href="#t646">646</a></span><span class="t">                <span class="nam">components</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">mlp_input</span><span class="op">=</span><span class="nam">mlp_input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t647" href="#t647">647</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t648" href="#t648">648</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_labels</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t649" href="#t649">649</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span><span class="op">,</span> <span class="nam">labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t650" href="#t650">650</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t651" href="#t651">651</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t652" href="#t652">652</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t653" href="#t653">653</a></span><span class="t">    <span class="key">def</span> <span class="nam">compute_head_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t654" href="#t654">654</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t655" href="#t655">655</a></span><span class="t">    <span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t656" href="#t656">656</a></span><span class="t">        <span class="str">"""Compute Head Results.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t657" href="#t657">657</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t658" href="#t658">658</a></span><span class="t"><span class="str">        Computes and caches the results for each attention head, ie the amount contributed to the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t659" href="#t659">659</a></span><span class="t"><span class="str">        residual stream from that head. attn_out for a layer is the sum of head results plus b_O.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t660" href="#t660">660</a></span><span class="t"><span class="str">        Intended use is to enable use_attn_results when running and caching the model, but this can</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t661" href="#t661">661</a></span><span class="t"><span class="str">        be useful if you forget.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t662" href="#t662">662</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t663" href="#t663">663</a></span><span class="t">        <span class="key">if</span> <span class="str">"blocks.0.attn.hook_result"</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t664" href="#t664">664</a></span><span class="t">            <span class="nam">logging</span><span class="op">.</span><span class="nam">warning</span><span class="op">(</span><span class="str">"Tried to compute head results when they were already cached"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t665" href="#t665">665</a></span><span class="t">            <span class="key">return</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t666" href="#t666">666</a></span><span class="t">        <span class="key">for</span> <span class="nam">layer</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t667" href="#t667">667</a></span><span class="t">            <span class="com"># Note that we haven't enabled set item on this object so we need to edit the underlying</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t668" href="#t668">668</a></span><span class="t">            <span class="com"># cache_dict directly.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t669" href="#t669">669</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t670" href="#t670">670</a></span><span class="t">            <span class="com"># Add singleton dimension to match W_O's shape for broadcasting</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t671" href="#t671">671</a></span><span class="t">            <span class="nam">z</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t672" href="#t672">672</a></span><span class="t">                <span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"z"</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="str">"attn"</span><span class="op">)</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t673" href="#t673">673</a></span><span class="t">                <span class="str">"... head_index d_head -> ... head_index d_head 1"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t674" href="#t674">674</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t675" href="#t675">675</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t676" href="#t676">676</a></span><span class="t">            <span class="com"># Element-wise multiplication of z and W_O (with shape [head_index, d_head, d_model])</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t677" href="#t677">677</a></span><span class="t">            <span class="nam">result</span> <span class="op">=</span> <span class="nam">z</span> <span class="op">*</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">blocks</span><span class="op">[</span><span class="nam">layer</span><span class="op">]</span><span class="op">.</span><span class="nam">attn</span><span class="op">.</span><span class="nam">W_O</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t678" href="#t678">678</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t679" href="#t679">679</a></span><span class="t">            <span class="com"># Sum over d_head to get the contribution of each head to the residual stream</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t680" href="#t680">680</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">[</span><span class="str">f"blocks.{layer}.attn.hook_result"</span><span class="op">]</span> <span class="op">=</span> <span class="nam">result</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t681" href="#t681">681</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t682" href="#t682">682</a></span><span class="t">    <span class="key">def</span> <span class="nam">stack_head_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t683" href="#t683">683</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t684" href="#t684">684</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t685" href="#t685">685</a></span><span class="t">        <span class="nam">return_labels</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t686" href="#t686">686</a></span><span class="t">        <span class="nam">incl_remainder</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t687" href="#t687">687</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t688" href="#t688">688</a></span><span class="t">        <span class="nam">apply_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t689" href="#t689">689</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t690" href="#t690">690</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t691" href="#t691">691</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t692" href="#t692">692</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t693" href="#t693">693</a></span><span class="t">        <span class="str">"""Stack Head Results.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t694" href="#t694">694</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t695" href="#t695">695</a></span><span class="t"><span class="str">        Returns a stack of all head results (ie residual stream contribution) up to layer L. A good</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t696" href="#t696">696</a></span><span class="t"><span class="str">        way to decompose the outputs of attention layers into attribution by specific heads. Note</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t697" href="#t697">697</a></span><span class="t"><span class="str">        that the num_components axis has length layer x n_heads ((layer head_index) in einops</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t698" href="#t698">698</a></span><span class="t"><span class="str">        notation).</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t699" href="#t699">699</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t700" href="#t700">700</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t701" href="#t701">701</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t702" href="#t702">702</a></span><span class="t"><span class="str">                Layer index - heads at all layers strictly before this are included. layer must be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t703" href="#t703">703</a></span><span class="t"><span class="str">                in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t704" href="#t704">704</a></span><span class="t"><span class="str">            return_labels:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t705" href="#t705">705</a></span><span class="t"><span class="str">                Whether to also return a list of labels of the form "L0H0" for the heads.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t706" href="#t706">706</a></span><span class="t"><span class="str">            incl_remainder:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t707" href="#t707">707</a></span><span class="t"><span class="str">                Whether to return a final term which is "the rest of the residual stream".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t708" href="#t708">708</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t709" href="#t709">709</a></span><span class="t"><span class="str">                A slice object to apply to the pos dimension. Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t710" href="#t710">710</a></span><span class="t"><span class="str">            apply_ln:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t711" href="#t711">711</a></span><span class="t"><span class="str">                Whether to apply LayerNorm to the stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t712" href="#t712">712</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t713" href="#t713">713</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t714" href="#t714">714</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t715" href="#t715">715</a></span><span class="t">        <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">)</span>  <span class="com"># mypy can't seem to infer this</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t716" href="#t716">716</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t717" href="#t717">717</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t718" href="#t718">718</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t719" href="#t719">719</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t720" href="#t720">720</a></span><span class="t">        <span class="key">if</span> <span class="str">"blocks.0.attn.hook_result"</span> <span class="key">not</span> <span class="key">in</span> <span class="nam">self</span><span class="op">.</span><span class="nam">cache_dict</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t721" href="#t721">721</a></span><span class="t">            <span class="nam">print</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t722" href="#t722">722</a></span><span class="t">                <span class="str">"Tried to stack head results when they weren't cached. Computing head results now"</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t723" href="#t723">723</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t724" href="#t724">724</a></span><span class="t">            <span class="nam">self</span><span class="op">.</span><span class="nam">compute_head_results</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t725" href="#t725">725</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t726" href="#t726">726</a></span><span class="t">        <span class="nam">components</span><span class="op">:</span> <span class="nam">Any</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t727" href="#t727">727</a></span><span class="t">        <span class="nam">labels</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t728" href="#t728">728</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t729" href="#t729">729</a></span><span class="t">            <span class="com"># Note that this has shape batch x pos x head_index x d_model</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t730" href="#t730">730</a></span><span class="t">            <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"result"</span><span class="op">,</span> <span class="nam">l</span><span class="op">,</span> <span class="str">"attn"</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">3</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t731" href="#t731">731</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">extend</span><span class="op">(</span><span class="op">[</span><span class="str">f"L{l}H{h}"</span> <span class="key">for</span> <span class="nam">h</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_heads</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t732" href="#t732">732</a></span><span class="t">        <span class="key">if</span> <span class="nam">components</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t733" href="#t733">733</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t734" href="#t734">734</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t735" href="#t735">735</a></span><span class="t">                <span class="nam">components</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t736" href="#t736">736</a></span><span class="t">                <span class="str">"... concat_head_index d_model -> concat_head_index ... d_model"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t737" href="#t737">737</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t738" href="#t738">738</a></span><span class="t">            <span class="key">if</span> <span class="nam">incl_remainder</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t739" href="#t739">739</a></span><span class="t">                <span class="nam">remainder</span> <span class="op">=</span> <span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t740" href="#t740">740</a></span><span class="t">                    <span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_post"</span><span class="op">,</span> <span class="nam">layer</span> <span class="op">-</span> <span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t741" href="#t741">741</a></span><span class="t">                <span class="op">)</span> <span class="op">-</span> <span class="nam">components</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t742" href="#t742">742</a></span><span class="t">                <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="op">[</span><span class="nam">components</span><span class="op">,</span> <span class="nam">remainder</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t743" href="#t743">743</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"remainder"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t744" href="#t744">744</a></span><span class="t">        <span class="key">elif</span> <span class="nam">incl_remainder</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t745" href="#t745">745</a></span><span class="t">            <span class="com"># There are no components, so the remainder is the entire thing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t746" href="#t746">746</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t747" href="#t747">747</a></span><span class="t">                <span class="op">[</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_post"</span><span class="op">,</span> <span class="nam">layer</span> <span class="op">-</span> <span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t748" href="#t748">748</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t749" href="#t749">749</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"remainder"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t750" href="#t750">750</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t751" href="#t751">751</a></span><span class="t">            <span class="com"># If this is called with layer 0, we return an empty tensor of the right shape to be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t752" href="#t752">752</a></span><span class="t">            <span class="com"># stacked correctly. This uses the shape of hook_embed, which is pretty janky since it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t753" href="#t753">753</a></span><span class="t">            <span class="com"># assumes embed is in the cache. But it's hard to explicitly code the shape, since it</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t754" href="#t754">754</a></span><span class="t">            <span class="com"># depends on the pos slice, whether we have a batch dim, etc. And it's pretty messy!</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t755" href="#t755">755</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t756" href="#t756">756</a></span><span class="t">                <span class="num">0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t757" href="#t757">757</a></span><span class="t">                <span class="op">*</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="str">"hook_embed"</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">.</span><span class="nam">shape</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t758" href="#t758">758</a></span><span class="t">                <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t759" href="#t759">759</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t760" href="#t760">760</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t761" href="#t761">761</a></span><span class="t">        <span class="key">if</span> <span class="nam">apply_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t762" href="#t762">762</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t763" href="#t763">763</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t764" href="#t764">764</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_labels</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t765" href="#t765">765</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span><span class="op">,</span> <span class="nam">labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t766" href="#t766">766</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t767" href="#t767">767</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t768" href="#t768">768</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t769" href="#t769">769</a></span><span class="t">    <span class="key">def</span> <span class="nam">stack_activation</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t770" href="#t770">770</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t771" href="#t771">771</a></span><span class="t">        <span class="nam">activation_name</span><span class="op">:</span> <span class="nam">str</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t772" href="#t772">772</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">int</span> <span class="op">=</span> <span class="op">-</span><span class="num">1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t773" href="#t773">773</a></span><span class="t">        <span class="nam">sublayer_type</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t774" href="#t774">774</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"layers_covered ..."</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t775" href="#t775">775</a></span><span class="t">        <span class="str">"""Stack Activations.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t776" href="#t776">776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t777" href="#t777">777</a></span><span class="t"><span class="str">        Flexible way to stack activations with a given name.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t778" href="#t778">778</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t779" href="#t779">779</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t780" href="#t780">780</a></span><span class="t"><span class="str">            activation_name:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t781" href="#t781">781</a></span><span class="t"><span class="str">                The name of the activation to be stacked</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t782" href="#t782">782</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t783" href="#t783">783</a></span><span class="t"><span class="str">                'Layer index - heads' at all layers strictly before this are included. layer must be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t784" href="#t784">784</a></span><span class="t"><span class="str">                in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t785" href="#t785">785</a></span><span class="t"><span class="str">            sublayer_type:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t786" href="#t786">786</a></span><span class="t"><span class="str">                The sub layer type of the activation, passed to utils.get_act_name. Can normally be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t787" href="#t787">787</a></span><span class="t"><span class="str">                inferred.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t788" href="#t788">788</a></span><span class="t"><span class="str">            incl_remainder:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t789" href="#t789">789</a></span><span class="t"><span class="str">                Whether to return a final term which is "the rest of the residual stream".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t790" href="#t790">790</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t791" href="#t791">791</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t792" href="#t792">792</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t793" href="#t793">793</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t794" href="#t794">794</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t795" href="#t795">795</a></span><span class="t">        <span class="nam">components</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t796" href="#t796">796</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t797" href="#t797">797</a></span><span class="t">            <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="nam">activation_name</span><span class="op">,</span> <span class="nam">l</span><span class="op">,</span> <span class="nam">sublayer_type</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t798" href="#t798">798</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t799" href="#t799">799</a></span><span class="t">        <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t800" href="#t800">800</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t801" href="#t801">801</a></span><span class="t">    <span class="key">def</span> <span class="nam">get_neuron_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t802" href="#t802">802</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t803" href="#t803">803</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t804" href="#t804">804</a></span><span class="t">        <span class="nam">neuron_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t805" href="#t805">805</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t806" href="#t806">806</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"*batch_and_pos_dims num_neurons d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t807" href="#t807">807</a></span><span class="t">        <span class="str">"""Get Neuron Results.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t808" href="#t808">808</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t809" href="#t809">809</a></span><span class="t"><span class="str">        Get the results of for neurons in a specific layer (i.e, how much each neuron contributes to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t810" href="#t810">810</a></span><span class="t"><span class="str">        the residual stream). Does it for the subset of neurons specified by neuron_slice, defaults</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t811" href="#t811">811</a></span><span class="t"><span class="str">        to all of them. Does *not* cache these because it's expensive in space and cheap to compute.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t812" href="#t812">812</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t813" href="#t813">813</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t814" href="#t814">814</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t815" href="#t815">815</a></span><span class="t"><span class="str">                Layer index.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t816" href="#t816">816</a></span><span class="t"><span class="str">            neuron_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t817" href="#t817">817</a></span><span class="t"><span class="str">                Slice of the neuron.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t818" href="#t818">818</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t819" href="#t819">819</a></span><span class="t"><span class="str">                Slice of the positions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t820" href="#t820">820</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t821" href="#t821">821</a></span><span class="t"><span class="str">        Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t822" href="#t822">822</a></span><span class="t"><span class="str">            Tensor of the results.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t823" href="#t823">823</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t824" href="#t824">824</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">neuron_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t825" href="#t825">825</a></span><span class="t">            <span class="nam">neuron_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">neuron_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t826" href="#t826">826</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t827" href="#t827">827</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t828" href="#t828">828</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t829" href="#t829">829</a></span><span class="t">        <span class="nam">neuron_acts</span> <span class="op">=</span> <span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"post"</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="str">"mlp"</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t830" href="#t830">830</a></span><span class="t">        <span class="nam">W_out</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">blocks</span><span class="op">[</span><span class="nam">layer</span><span class="op">]</span><span class="op">.</span><span class="nam">mlp</span><span class="op">.</span><span class="nam">W_out</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t831" href="#t831">831</a></span><span class="t">        <span class="key">if</span> <span class="nam">pos_slice</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">831&#x202F;&#x219B;&#x202F;835</span><span class="annotate long">line 831 didn't jump to line 835, because the condition on line 831 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t832" href="#t832">832</a></span><span class="t">            <span class="com"># Note - order is important, as Slice.apply *may* collapse a dimension, so this ensures</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t833" href="#t833">833</a></span><span class="t">            <span class="com"># that position dimension is -2 when we apply position slice</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t834" href="#t834">834</a></span><span class="t">            <span class="nam">neuron_acts</span> <span class="op">=</span> <span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">neuron_acts</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t835" href="#t835">835</a></span><span class="t">        <span class="key">if</span> <span class="nam">neuron_slice</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">835&#x202F;&#x219B;&#x202F;838</span><span class="annotate long">line 835 didn't jump to line 838, because the condition on line 835 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t836" href="#t836">836</a></span><span class="t">            <span class="nam">neuron_acts</span> <span class="op">=</span> <span class="nam">neuron_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">neuron_acts</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t837" href="#t837">837</a></span><span class="t">            <span class="nam">W_out</span> <span class="op">=</span> <span class="nam">neuron_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">W_out</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t838" href="#t838">838</a></span><span class="t">        <span class="key">return</span> <span class="nam">neuron_acts</span><span class="op">[</span><span class="op">...</span><span class="op">,</span> <span class="key">None</span><span class="op">]</span> <span class="op">*</span> <span class="nam">W_out</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t839" href="#t839">839</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t840" href="#t840">840</a></span><span class="t">    <span class="key">def</span> <span class="nam">stack_neuron_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t841" href="#t841">841</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t842" href="#t842">842</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">int</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t843" href="#t843">843</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t844" href="#t844">844</a></span><span class="t">        <span class="nam">neuron_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t845" href="#t845">845</a></span><span class="t">        <span class="nam">return_labels</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t846" href="#t846">846</a></span><span class="t">        <span class="nam">incl_remainder</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t847" href="#t847">847</a></span><span class="t">        <span class="nam">apply_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t848" href="#t848">848</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t849" href="#t849">849</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t850" href="#t850">850</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t851" href="#t851">851</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t852" href="#t852">852</a></span><span class="t">        <span class="str">"""Stack Neuron Results</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t853" href="#t853">853</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t854" href="#t854">854</a></span><span class="t"><span class="str">        Returns a stack of all neuron results (ie residual stream contribution) up to layer L - ie</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t855" href="#t855">855</a></span><span class="t"><span class="str">        the amount each individual neuron contributes to the residual stream. Also returns a list of</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t856" href="#t856">856</a></span><span class="t"><span class="str">        labels of the form "L0N0" for the neurons. A good way to decompose the outputs of MLP layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t857" href="#t857">857</a></span><span class="t"><span class="str">        into attribution by specific neurons.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t858" href="#t858">858</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t859" href="#t859">859</a></span><span class="t"><span class="str">        Note that doing this for all neurons is SUPER expensive on GPU memory and only works for</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t860" href="#t860">860</a></span><span class="t"><span class="str">        small models or short inputs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t861" href="#t861">861</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t862" href="#t862">862</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t863" href="#t863">863</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t864" href="#t864">864</a></span><span class="t"><span class="str">                Layer index - heads at all layers strictly before this are included. layer must be</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t865" href="#t865">865</a></span><span class="t"><span class="str">                in [1, n_layers]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t866" href="#t866">866</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t867" href="#t867">867</a></span><span class="t"><span class="str">                Slice of the positions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t868" href="#t868">868</a></span><span class="t"><span class="str">            neuron_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t869" href="#t869">869</a></span><span class="t"><span class="str">                Slice of the neurons.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t870" href="#t870">870</a></span><span class="t"><span class="str">            return_labels:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t871" href="#t871">871</a></span><span class="t"><span class="str">                Whether to also return a list of labels of the form "L0H0" for the heads.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t872" href="#t872">872</a></span><span class="t"><span class="str">            incl_remainder:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t873" href="#t873">873</a></span><span class="t"><span class="str">                Whether to return a final term which is "the rest of the residual stream".</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t874" href="#t874">874</a></span><span class="t"><span class="str">            apply_ln:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t875" href="#t875">875</a></span><span class="t"><span class="str">                Whether to apply LayerNorm to the stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t876" href="#t876">876</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t877" href="#t877">877</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t878" href="#t878">878</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t879" href="#t879">879</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t880" href="#t880">880</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t881" href="#t881">881</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t882" href="#t882">882</a></span><span class="t">        <span class="nam">components</span><span class="op">:</span> <span class="nam">Any</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>  <span class="com"># TODO: fix typing properly</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t883" href="#t883">883</a></span><span class="t">        <span class="nam">labels</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t884" href="#t884">884</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t885" href="#t885">885</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">neuron_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t886" href="#t886">886</a></span><span class="t">            <span class="nam">neuron_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">neuron_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t887" href="#t887">887</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t888" href="#t888">888</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t889" href="#t889">889</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t890" href="#t890">890</a></span><span class="t">        <span class="nam">neuron_labels</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span> <span class="op">|</span> <span class="nam">np</span><span class="op">.</span><span class="nam">ndarray</span> <span class="op">=</span> <span class="nam">neuron_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t891" href="#t891">891</a></span><span class="t">            <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">d_mlp</span><span class="op">)</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t892" href="#t892">892</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t893" href="#t893">893</a></span><span class="t">        <span class="key">if</span> <span class="nam">type</span><span class="op">(</span><span class="nam">neuron_labels</span><span class="op">)</span> <span class="op">==</span> <span class="nam">int</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">893&#x202F;&#x219B;&#x202F;894</span><span class="annotate long">line 893 didn't jump to line 894, because the condition on line 893 was never true</span></span></p>
    <p class="mis show_mis"><span class="n"><a id="t894" href="#t894">894</a></span><span class="t">            <span class="nam">neuron_labels</span> <span class="op">=</span> <span class="nam">np</span><span class="op">.</span><span class="nam">array</span><span class="op">(</span><span class="op">[</span><span class="nam">neuron_labels</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t895" href="#t895">895</a></span><span class="t">        <span class="key">for</span> <span class="nam">l</span> <span class="key">in</span> <span class="nam">range</span><span class="op">(</span><span class="nam">layer</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t896" href="#t896">896</a></span><span class="t">            <span class="com"># Note that this has shape batch x pos x head_index x d_model</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t897" href="#t897">897</a></span><span class="t">            <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t898" href="#t898">898</a></span><span class="t">                <span class="nam">self</span><span class="op">.</span><span class="nam">get_neuron_results</span><span class="op">(</span><span class="nam">l</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">neuron_slice</span><span class="op">=</span><span class="nam">neuron_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t899" href="#t899">899</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t900" href="#t900">900</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">extend</span><span class="op">(</span><span class="op">[</span><span class="str">f"L{l}N{h}"</span> <span class="key">for</span> <span class="nam">h</span> <span class="key">in</span> <span class="nam">neuron_labels</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t901" href="#t901">901</a></span><span class="t">        <span class="key">if</span> <span class="nam">components</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t902" href="#t902">902</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t903" href="#t903">903</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">einops</span><span class="op">.</span><span class="nam">rearrange</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t904" href="#t904">904</a></span><span class="t">                <span class="nam">components</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t905" href="#t905">905</a></span><span class="t">                <span class="str">"... concat_neuron_index d_model -> concat_neuron_index ... d_model"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t906" href="#t906">906</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t907" href="#t907">907</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t908" href="#t908">908</a></span><span class="t">            <span class="key">if</span> <span class="nam">incl_remainder</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t909" href="#t909">909</a></span><span class="t">                <span class="nam">remainder</span> <span class="op">=</span> <span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t910" href="#t910">910</a></span><span class="t">                    <span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_post"</span><span class="op">,</span> <span class="nam">layer</span> <span class="op">-</span> <span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t911" href="#t911">911</a></span><span class="t">                <span class="op">)</span> <span class="op">-</span> <span class="nam">components</span><span class="op">.</span><span class="nam">sum</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t912" href="#t912">912</a></span><span class="t">                <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="op">[</span><span class="nam">components</span><span class="op">,</span> <span class="nam">remainder</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t913" href="#t913">913</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"remainder"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t914" href="#t914">914</a></span><span class="t">        <span class="key">elif</span> <span class="nam">incl_remainder</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t915" href="#t915">915</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t916" href="#t916">916</a></span><span class="t">                <span class="op">[</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="op">(</span><span class="str">"resid_post"</span><span class="op">,</span> <span class="nam">layer</span> <span class="op">-</span> <span class="num">1</span><span class="op">)</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t917" href="#t917">917</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t918" href="#t918">918</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"remainder"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t919" href="#t919">919</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t920" href="#t920">920</a></span><span class="t">            <span class="com"># Returning empty, give it the right shape to stack properly</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t921" href="#t921">921</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t922" href="#t922">922</a></span><span class="t">                <span class="num">0</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t923" href="#t923">923</a></span><span class="t">                <span class="op">*</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="str">"hook_embed"</span><span class="op">]</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">.</span><span class="nam">shape</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t924" href="#t924">924</a></span><span class="t">                <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t925" href="#t925">925</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t926" href="#t926">926</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t927" href="#t927">927</a></span><span class="t">        <span class="key">if</span> <span class="nam">apply_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t928" href="#t928">928</a></span><span class="t">            <span class="nam">components</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t929" href="#t929">929</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t930" href="#t930">930</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_labels</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t931" href="#t931">931</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span><span class="op">,</span> <span class="nam">labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t932" href="#t932">932</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t933" href="#t933">933</a></span><span class="t">            <span class="key">return</span> <span class="nam">components</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t934" href="#t934">934</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t935" href="#t935">935</a></span><span class="t">    <span class="key">def</span> <span class="nam">apply_ln_to_stack</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t936" href="#t936">936</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t937" href="#t937">937</a></span><span class="t">        <span class="nam">residual_stack</span><span class="op">:</span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t938" href="#t938">938</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t939" href="#t939">939</a></span><span class="t">        <span class="nam">mlp_input</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t940" href="#t940">940</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t941" href="#t941">941</a></span><span class="t">        <span class="nam">batch_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t942" href="#t942">942</a></span><span class="t">        <span class="nam">has_batch_dim</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t943" href="#t943">943</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims_out d_model"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t944" href="#t944">944</a></span><span class="t">        <span class="str">"""Apply Layer Norm to a Stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t945" href="#t945">945</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t946" href="#t946">946</a></span><span class="t"><span class="str">        Takes a stack of components of the residual stream (eg outputs of decompose_resid or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t947" href="#t947">947</a></span><span class="t"><span class="str">        accumulated_resid), treats them as the input to a specific layer, and applies the layer norm</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t948" href="#t948">948</a></span><span class="t"><span class="str">        scaling of that layer to them, using the cached scale factors - simulating what that</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t949" href="#t949">949</a></span><span class="t"><span class="str">        component of the residual stream contributes to that layer's input.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t950" href="#t950">950</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t951" href="#t951">951</a></span><span class="t"><span class="str">        The layernorm scale is global across the entire residual stream for each layer, batch</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t952" href="#t952">952</a></span><span class="t"><span class="str">        element and position, which is why we need to use the cached scale factors rather than just</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t953" href="#t953">953</a></span><span class="t"><span class="str">        applying a new LayerNorm.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t954" href="#t954">954</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t955" href="#t955">955</a></span><span class="t"><span class="str">        If the model does not use LayerNorm or RMSNorm, it returns the residual stack unchanged.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t956" href="#t956">956</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t957" href="#t957">957</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t958" href="#t958">958</a></span><span class="t"><span class="str">            residual_stack:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t959" href="#t959">959</a></span><span class="t"><span class="str">                A tensor, whose final dimension is d_model. The other trailing dimensions are</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t960" href="#t960">960</a></span><span class="t"><span class="str">                assumed to be the same as the stored hook_scale - which may or may not include batch</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t961" href="#t961">961</a></span><span class="t"><span class="str">                or position dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t962" href="#t962">962</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t963" href="#t963">963</a></span><span class="t"><span class="str">                The layer we're taking the input to. In [0, n_layers], n_layers means the unembed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t964" href="#t964">964</a></span><span class="t"><span class="str">                None maps to the n_layers case, ie the unembed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t965" href="#t965">965</a></span><span class="t"><span class="str">            mlp_input:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t966" href="#t966">966</a></span><span class="t"><span class="str">                Whether the input is to the MLP or attn (ie ln2 vs ln1). Defaults to False, ie ln1.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t967" href="#t967">967</a></span><span class="t"><span class="str">                If layer==n_layers, must be False, and we use ln_final</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t968" href="#t968">968</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t969" href="#t969">969</a></span><span class="t"><span class="str">                The slice to take of positions, if residual_stack is not over the full context, None</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t970" href="#t970">970</a></span><span class="t"><span class="str">                means do nothing. It is assumed that pos_slice has already been applied to</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t971" href="#t971">971</a></span><span class="t"><span class="str">                residual_stack, and this is only applied to the scale. See utils.Slice for details.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t972" href="#t972">972</a></span><span class="t"><span class="str">                Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t973" href="#t973">973</a></span><span class="t"><span class="str">            batch_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t974" href="#t974">974</a></span><span class="t"><span class="str">                The slice to take on the batch dimension. Defaults to None, do nothing.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t975" href="#t975">975</a></span><span class="t"><span class="str">            has_batch_dim:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t976" href="#t976">976</a></span><span class="t"><span class="str">                Whether residual_stack has a batch dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t977" href="#t977">977</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t978" href="#t978">978</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t979" href="#t979">979</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">not</span> <span class="key">in</span> <span class="op">[</span><span class="str">"LN"</span><span class="op">,</span> <span class="str">"LNPre"</span><span class="op">,</span> <span class="str">"RMS"</span><span class="op">,</span> <span class="str">"RMSPre"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">979&#x202F;&#x219B;&#x202F;981</span><span class="annotate long">line 979 didn't jump to line 981, because the condition on line 979 was never true</span></span></p>
    <p class="pln"><span class="n"><a id="t980" href="#t980">980</a></span><span class="t">            <span class="com"># The model does not use LayerNorm, so we don't need to do anything.</span>&nbsp;</span><span class="r"></span></p>
    <p class="mis show_mis"><span class="n"><a id="t981" href="#t981">981</a></span><span class="t">            <span class="key">return</span> <span class="nam">residual_stack</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t982" href="#t982">982</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t983" href="#t983">983</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t984" href="#t984">984</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t985" href="#t985">985</a></span><span class="t">            <span class="nam">batch_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">batch_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t986" href="#t986">986</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t987" href="#t987">987</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t988" href="#t988">988</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t989" href="#t989">989</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t990" href="#t990">990</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t991" href="#t991">991</a></span><span class="t">        <span class="key">if</span> <span class="nam">has_batch_dim</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t992" href="#t992">992</a></span><span class="t">            <span class="com"># Apply batch slice to the stack</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t993" href="#t993">993</a></span><span class="t">            <span class="nam">residual_stack</span> <span class="op">=</span> <span class="nam">batch_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">residual_stack</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">1</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t994" href="#t994">994</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t995" href="#t995">995</a></span><span class="t">        <span class="com"># Center the stack onlny if the model uses LayerNorm</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t996" href="#t996">996</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">normalization_type</span> <span class="key">in</span> <span class="op">[</span><span class="str">"LN"</span><span class="op">,</span> <span class="str">"LNPre"</span><span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">996&#x202F;&#x219B;&#x202F;999</span><span class="annotate long">line 996 didn't jump to line 999, because the condition on line 996 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t997" href="#t997">997</a></span><span class="t">            <span class="nam">residual_stack</span> <span class="op">=</span> <span class="nam">residual_stack</span> <span class="op">-</span> <span class="nam">residual_stack</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">1</span><span class="op">,</span> <span class="nam">keepdim</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t998" href="#t998">998</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t999" href="#t999">999</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="op">==</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span> <span class="key">or</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1000" href="#t1000">1000</a></span><span class="t">            <span class="nam">scale</span> <span class="op">=</span> <span class="nam">self</span><span class="op">[</span><span class="str">"ln_final.hook_scale"</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1001" href="#t1001">1001</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1002" href="#t1002">1002</a></span><span class="t">            <span class="nam">hook_name</span> <span class="op">=</span> <span class="str">f"blocks.{layer}.ln{2 if mlp_input else 1}.hook_scale"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1003" href="#t1003">1003</a></span><span class="t">            <span class="nam">scale</span> <span class="op">=</span> <span class="nam">self</span><span class="op">[</span><span class="nam">hook_name</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1004" href="#t1004">1004</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1005" href="#t1005">1005</a></span><span class="t">        <span class="com"># The shape of scale is [batch, position, 1] or [position, 1] - final dimension is a dummy</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1006" href="#t1006">1006</a></span><span class="t">        <span class="com"># thing to get broadcoasting to work nicely.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1007" href="#t1007">1007</a></span><span class="t">        <span class="nam">scale</span> <span class="op">=</span> <span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">scale</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="op">-</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1008" href="#t1008">1008</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1009" href="#t1009">1009</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_batch_dim</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1009&#x202F;&#x219B;&#x202F;1013</span><span class="annotate long">line 1009 didn't jump to line 1013, because the condition on line 1009 was never false</span></span></p>
    <p class="pln"><span class="n"><a id="t1010" href="#t1010">1010</a></span><span class="t">            <span class="com"># Apply batch slice to the scale</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1011" href="#t1011">1011</a></span><span class="t">            <span class="nam">scale</span> <span class="op">=</span> <span class="nam">batch_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">scale</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1012" href="#t1012">1012</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1013" href="#t1013">1013</a></span><span class="t">        <span class="key">return</span> <span class="nam">residual_stack</span> <span class="op">/</span> <span class="nam">scale</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1014" href="#t1014">1014</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1015" href="#t1015">1015</a></span><span class="t">    <span class="key">def</span> <span class="nam">get_full_resid_decomposition</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1016" href="#t1016">1016</a></span><span class="t">        <span class="nam">self</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1017" href="#t1017">1017</a></span><span class="t">        <span class="nam">layer</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">int</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1018" href="#t1018">1018</a></span><span class="t">        <span class="nam">mlp_input</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1019" href="#t1019">1019</a></span><span class="t">        <span class="nam">expand_neurons</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1020" href="#t1020">1020</a></span><span class="t">        <span class="nam">apply_ln</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1021" href="#t1021">1021</a></span><span class="t">        <span class="nam">pos_slice</span><span class="op">:</span> <span class="nam">Union</span><span class="op">[</span><span class="nam">Slice</span><span class="op">,</span> <span class="nam">SliceInput</span><span class="op">]</span> <span class="op">=</span> <span class="key">None</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1022" href="#t1022">1022</a></span><span class="t">        <span class="nam">return_labels</span><span class="op">:</span> <span class="nam">bool</span> <span class="op">=</span> <span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1023" href="#t1023">1023</a></span><span class="t">    <span class="op">)</span> <span class="op">-></span> <span class="nam">Union</span><span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1024" href="#t1024">1024</a></span><span class="t">        <span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1025" href="#t1025">1025</a></span><span class="t">        <span class="nam">Tuple</span><span class="op">[</span><span class="nam">Float</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">"num_components *batch_and_pos_dims d_model"</span><span class="op">]</span><span class="op">,</span> <span class="nam">List</span><span class="op">[</span><span class="nam">str</span><span class="op">]</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1026" href="#t1026">1026</a></span><span class="t">    <span class="op">]</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1027" href="#t1027">1027</a></span><span class="t">        <span class="str">"""Get the full Residual Decomposition.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1028" href="#t1028">1028</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1029" href="#t1029">1029</a></span><span class="t"><span class="str">        Returns the full decomposition of the residual stream into embed, pos_embed, each head</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1030" href="#t1030">1030</a></span><span class="t"><span class="str">        result, each neuron result, and the accumulated biases. We break down the residual stream</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1031" href="#t1031">1031</a></span><span class="t"><span class="str">        that is input into some layer.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1032" href="#t1032">1032</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1033" href="#t1033">1033</a></span><span class="t"><span class="str">        Args:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1034" href="#t1034">1034</a></span><span class="t"><span class="str">            layer:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1035" href="#t1035">1035</a></span><span class="t"><span class="str">                The layer we're inputting into. layer is in [0, n_layers], if layer==n_layers (or</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1036" href="#t1036">1036</a></span><span class="t"><span class="str">                None) we're inputting into the unembed (the entire stream), if layer==0 then it's</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1037" href="#t1037">1037</a></span><span class="t"><span class="str">                just embed and pos_embed</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1038" href="#t1038">1038</a></span><span class="t"><span class="str">            mlp_input:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1039" href="#t1039">1039</a></span><span class="t"><span class="str">                Are we inputting to the MLP in that layer or the attn? Must be False for final</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1040" href="#t1040">1040</a></span><span class="t"><span class="str">                layer, since that's the unembed.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1041" href="#t1041">1041</a></span><span class="t"><span class="str">            expand_neurons:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1042" href="#t1042">1042</a></span><span class="t"><span class="str">                Whether to expand the MLP outputs to give every neuron's result or just return the</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1043" href="#t1043">1043</a></span><span class="t"><span class="str">                MLP layer outputs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1044" href="#t1044">1044</a></span><span class="t"><span class="str">            apply_ln:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1045" href="#t1045">1045</a></span><span class="t"><span class="str">                Whether to apply LayerNorm to the stack.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1046" href="#t1046">1046</a></span><span class="t"><span class="str">            pos_slice:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1047" href="#t1047">1047</a></span><span class="t"><span class="str">                Slice of the positions to take.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1048" href="#t1048">1048</a></span><span class="t"><span class="str">            return_labels:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1049" href="#t1049">1049</a></span><span class="t"><span class="str">                Whether to return the labels.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1050" href="#t1050">1050</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1051" href="#t1051">1051</a></span><span class="t">        <span class="key">if</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">layer</span> <span class="op">==</span> <span class="op">-</span><span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1052" href="#t1052">1052</a></span><span class="t">            <span class="com"># Default to the residual stream immediately pre unembed</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1053" href="#t1053">1053</a></span><span class="t">            <span class="nam">layer</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">n_layers</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1054" href="#t1054">1054</a></span><span class="t">        <span class="key">assert</span> <span class="nam">layer</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span>  <span class="com"># keep mypy happy</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1055" href="#t1055">1055</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1056" href="#t1056">1056</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">Slice</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1057" href="#t1057">1057</a></span><span class="t">            <span class="nam">pos_slice</span> <span class="op">=</span> <span class="nam">Slice</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1058" href="#t1058">1058</a></span><span class="t">        <span class="nam">head_stack</span><span class="op">,</span> <span class="nam">head_labels</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">stack_head_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1059" href="#t1059">1059</a></span><span class="t">            <span class="nam">layer</span> <span class="op">+</span> <span class="op">(</span><span class="num">1</span> <span class="key">if</span> <span class="nam">mlp_input</span> <span class="key">else</span> <span class="num">0</span><span class="op">)</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">return_labels</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1060" href="#t1060">1060</a></span><span class="t">        <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1061" href="#t1061">1061</a></span><span class="t">        <span class="nam">labels</span> <span class="op">=</span> <span class="nam">head_labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1062" href="#t1062">1062</a></span><span class="t">        <span class="nam">components</span> <span class="op">=</span> <span class="op">[</span><span class="nam">head_stack</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1063" href="#t1063">1063</a></span><span class="t">        <span class="key">if</span> <span class="key">not</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">cfg</span><span class="op">.</span><span class="nam">attn_only</span> <span class="key">and</span> <span class="nam">layer</span> <span class="op">></span> <span class="num">0</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1064" href="#t1064">1064</a></span><span class="t">            <span class="key">if</span> <span class="nam">expand_neurons</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1065" href="#t1065">1065</a></span><span class="t">                <span class="nam">neuron_stack</span><span class="op">,</span> <span class="nam">neuron_labels</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">stack_neuron_results</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1066" href="#t1066">1066</a></span><span class="t">                    <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">return_labels</span><span class="op">=</span><span class="key">True</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1067" href="#t1067">1067</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1068" href="#t1068">1068</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">extend</span><span class="op">(</span><span class="nam">neuron_labels</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1069" href="#t1069">1069</a></span><span class="t">                <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">neuron_stack</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1070" href="#t1070">1070</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1071" href="#t1071">1071</a></span><span class="t">                <span class="com"># Get the stack of just the MLP outputs</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1072" href="#t1072">1072</a></span><span class="t">                <span class="com"># mlp_input included for completeness, but it doesn't actually matter, since it's</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1073" href="#t1073">1073</a></span><span class="t">                <span class="com"># just for MLP outputs</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1074" href="#t1074">1074</a></span><span class="t">                <span class="nam">mlp_stack</span><span class="op">,</span> <span class="nam">mlp_labels</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">decompose_resid</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1075" href="#t1075">1075</a></span><span class="t">                    <span class="nam">layer</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1076" href="#t1076">1076</a></span><span class="t">                    <span class="nam">mlp_input</span><span class="op">=</span><span class="nam">mlp_input</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1077" href="#t1077">1077</a></span><span class="t">                    <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1078" href="#t1078">1078</a></span><span class="t">                    <span class="nam">incl_embeds</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1079" href="#t1079">1079</a></span><span class="t">                    <span class="nam">mode</span><span class="op">=</span><span class="str">"mlp"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1080" href="#t1080">1080</a></span><span class="t">                    <span class="nam">return_labels</span><span class="op">=</span><span class="key">True</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1081" href="#t1081">1081</a></span><span class="t">                <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1082" href="#t1082">1082</a></span><span class="t">                <span class="nam">labels</span><span class="op">.</span><span class="nam">extend</span><span class="op">(</span><span class="nam">mlp_labels</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1083" href="#t1083">1083</a></span><span class="t">                <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">mlp_stack</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1084" href="#t1084">1084</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1085" href="#t1085">1085</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_embed</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1085&#x202F;&#x219B;&#x202F;1088</span><span class="annotate long">line 1085 didn't jump to line 1088, because the condition on line 1085 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1086" href="#t1086">1086</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"embed"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1087" href="#t1087">1087</a></span><span class="t">            <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="str">"embed"</span><span class="op">]</span><span class="op">,</span> <span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="par run show_par"><span class="n"><a id="t1088" href="#t1088">1088</a></span><span class="t">        <span class="key">if</span> <span class="nam">self</span><span class="op">.</span><span class="nam">has_pos_embed</span><span class="op">:</span>&nbsp;</span><span class="r"><span class="annotate short">1088&#x202F;&#x219B;&#x202F;1092</span><span class="annotate long">line 1088 didn't jump to line 1092, because the condition on line 1088 was never false</span></span></p>
    <p class="run"><span class="n"><a id="t1089" href="#t1089">1089</a></span><span class="t">            <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"pos_embed"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1090" href="#t1090">1090</a></span><span class="t">            <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">pos_slice</span><span class="op">.</span><span class="nam">apply</span><span class="op">(</span><span class="nam">self</span><span class="op">[</span><span class="str">"pos_embed"</span><span class="op">]</span><span class="op">,</span> <span class="op">-</span><span class="num">2</span><span class="op">)</span><span class="op">[</span><span class="key">None</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1091" href="#t1091">1091</a></span><span class="t">        <span class="com"># If we didn't expand the neurons, the MLP biases are already included in the MLP outputs.</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1092" href="#t1092">1092</a></span><span class="t">        <span class="nam">bias</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">model</span><span class="op">.</span><span class="nam">accumulated_bias</span><span class="op">(</span><span class="nam">layer</span><span class="op">,</span> <span class="nam">mlp_input</span><span class="op">,</span> <span class="nam">include_mlp_biases</span><span class="op">=</span><span class="nam">expand_neurons</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1093" href="#t1093">1093</a></span><span class="t">        <span class="nam">bias</span> <span class="op">=</span> <span class="nam">bias</span><span class="op">.</span><span class="nam">expand</span><span class="op">(</span><span class="op">(</span><span class="num">1</span><span class="op">,</span><span class="op">)</span> <span class="op">+</span> <span class="nam">head_stack</span><span class="op">.</span><span class="nam">shape</span><span class="op">[</span><span class="num">1</span><span class="op">:</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1094" href="#t1094">1094</a></span><span class="t">        <span class="nam">labels</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="str">"bias"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1095" href="#t1095">1095</a></span><span class="t">        <span class="nam">components</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">bias</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1096" href="#t1096">1096</a></span><span class="t">        <span class="nam">residual_stack</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">(</span><span class="nam">components</span><span class="op">,</span> <span class="nam">dim</span><span class="op">=</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1097" href="#t1097">1097</a></span><span class="t">        <span class="key">if</span> <span class="nam">apply_ln</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1098" href="#t1098">1098</a></span><span class="t">            <span class="nam">residual_stack</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">apply_ln_to_stack</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1099" href="#t1099">1099</a></span><span class="t">                <span class="nam">residual_stack</span><span class="op">,</span> <span class="nam">layer</span><span class="op">,</span> <span class="nam">pos_slice</span><span class="op">=</span><span class="nam">pos_slice</span><span class="op">,</span> <span class="nam">mlp_input</span><span class="op">=</span><span class="nam">mlp_input</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1100" href="#t1100">1100</a></span><span class="t">            <span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1101" href="#t1101">1101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1102" href="#t1102">1102</a></span><span class="t">        <span class="key">if</span> <span class="nam">return_labels</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1103" href="#t1103">1103</a></span><span class="t">            <span class="key">return</span> <span class="nam">residual_stack</span><span class="op">,</span> <span class="nam">labels</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t1104" href="#t1104">1104</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t1105" href="#t1105">1105</a></span><span class="t">            <span class="key">return</span> <span class="nam">residual_stack</span>&nbsp;</span><span class="r"></span></p>
</main>
<footer>
    <div class="content">
        <p>
            <a id="prevFileLink" class="nav" href="index.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="d_af97b5493da09a14_BertNextSentencePrediction_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.4.4">coverage.py v7.4.4</a>,
            created at 2025-05-05 16:22 +0000
        </p>
    </div>
</footer>
</body>
</html>
